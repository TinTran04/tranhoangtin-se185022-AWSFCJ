[
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " How iFood Built a Platform to Run Hundreds of Machine Learning Models with Amazon SageMaker Inference By Daniel Vieira, Debora Fanin, Gopi Mudiyala, and Saurabh Trikande – April 8, 2025, in the Advanced section (300), Amazon SageMaker Data \u0026amp; AI Governance, Customer Solutions.\nIntroduction Headquartered in São Paulo, Brazil, iFood is a privately held national company and a leader in food tech in Latin America, processing millions of orders every month. iFood stands out for its strategy of integrating advanced technology into its operations. With the support of AWS, iFood has developed a powerful machine learning (ML) inference infrastructure using services like Amazon SageMaker to create and deploy ML models efficiently. This collaboration has allowed iFood to not only optimize internal processes but also provide innovative solutions for its delivery partners and restaurants.\niFood\u0026rsquo;s ML platform includes a set of tools, processes, and workflows developed with the following main objectives:\nAccelerating the development and training of AI/ML models, making them more reliable and easier to reproduce. Ensuring that the deployment of these models into production is reliable, scalable, and traceable. Enabling transparent, accessible, and standardized testing, monitoring, and evaluation of models in production. Figure 1. Overview — iFood and the application of AI/ML in their product system.\nGoals and Approach To achieve these goals, iFood leverages SageMaker to streamline model training and deployment. By integrating SageMaker features into iFood\u0026rsquo;s infrastructure, key steps are automated — from creating training datasets, training models, deploying them into production, to continuously monitoring their performance.\nThis article outlines how iFood uses SageMaker to enhance the entire ML lifecycle — from training to inference — and describes architectural changes and capabilities developed by the team.\nAI Inference at iFood iFood leverages its AI/ML platform to enhance customer experience across various touchpoints. Some typical use cases include:\nPersonalized Recommendations — Models analyze order history, preferences, and context to suggest appropriate restaurants and dishes, increasing customer satisfaction and order volume. Smart Order Tracking — The system predicts delivery times in real-time by combining traffic data, restaurant preparation times, and the location of the delivery driver, proactively notifying customers. Automated Customer Service — AI chatbots handle thousands of common requests daily, providing fast responses and retrieving relevant data to support personalization. Grocery Shopping Support — The app integrates language models that help customers create shopping lists from voice or text requests. Thanks to these initiatives, iFood can forecast demand, optimize processes, and deliver consistent user experiences.\nSolution Overview (Legacy Architecture) The diagram below illustrates iFood\u0026rsquo;s legacy architecture, where Data Science and Engineering teams had separate workflows — leading to challenges when deploying real-time ML models into production.\nFigure 2. Legacy Architecture — Describing data flow and the barriers between teams.\nPreviously, data scientists developed models in notebooks, fine-tuned, and published artifacts. Engineers then had to integrate these artifacts into the production system, causing delays and integration errors. To address this, iFood developed an internal ML platform to unify the process from development to deployment, creating a seamless experience for both teams.\nUpdated Architecture and ML Go! One of the core capabilities of iFood\u0026rsquo;s ML platform is providing the infrastructure to serve predictions. The internal platform (called ML Go!) is responsible for managing the deployment process, overseeing SageMaker Endpoints and Jobs. ML Go! supports both offline (batch) and real-time (online) predictions, and manages the lifecycle of models (registry, versioning, monitoring).\nFigure 3. Updated Architecture — Including pipelines, model registry, and inference components.\nThe platform provides:\nAutomated ML pipelines (SageMaker Pipelines) for model training and retraining. ML Go! CI/CD to push artifacts, build Docker images, and trigger pipelines. SageMaker Model Registry for versioning and model management. Monitoring mechanisms to detect drift and performance degradation. Final Architecture: Inference Components \u0026amp; ML Go! Gateway A significant improvement is the abstraction concept connecting with SageMaker (Endpoints \u0026amp; Jobs) called ML Go! Gateway, along with separating \u0026ldquo;inference components\u0026rdquo; in the endpoint — helping to divide concerns, accelerate delivery, and manage resources more efficiently. Endpoints now manage multiple inference components, and ML Go! CI/CD only focuses on model version promotion, without deep intervention in infrastructure.\nFigure 4. Final Architecture — Inference components, ML Go! Gateway, and integration with service accounts.\nIn this new structure:\nEndpoints can contain multiple inference components, allowing load distribution by function or load. ML Go! Dispatcher/Gateway forwards requests to the appropriate endpoint or job. CI/CD handles artifacts (Docker images, configs), and SageMaker Pipeline orchestrates training → evaluation → registry → deployment. Using SageMaker Inference Model Serving Containers Standardizing the environment through containers is a crucial element of modern ML platforms. SageMaker provides built-in containers for TensorFlow, PyTorch, XGBoost, and more, as well as the ability to use custom containers.\niFood focuses on using custom containers to:\nStandardize ML code (not directly using notebooks in production). Package dependencies, libraries, and inference logic in an image (e.g., BruceML scaffolding). Easily recreate training and serving environments, monitor results, and debug. BruceML helps standardize the way training and serving code is written, creating a scaffold compatible with SageMaker (autotuning, deployment hooks, monitoring).\nAutomating Deployment and Retraining (ML Pipelines \u0026amp; CI/CD) iFood uses SageMaker Pipelines to build CI/CD for ML: pipelines are responsible for orchestrating the entire data flow — from preprocessing, training, evaluation, to promotion in the Model Registry and deployment. ML Go! CI/CD integrates with the organization’s CI/CD system to:\nPush artifacts (code + container image). Trigger training and evaluation pipelines. Automatically register models into the Model Registry. Deploy or promote models to the appropriate endpoint (online / batch). Depending on SLA:\nBatch inference: Uses SageMaker Transform jobs for large-scale predictions. Real-time inference: Deploys models to SageMaker Endpoint with the appropriate container/instance configuration. SageMaker Pipelines helps automate and coordinate complex workflows, reducing errors and shortening development cycles.\nRunning Inference at Different SLA Formats iFood uses multiple inference methods to meet different requirements:\nReal-time endpoints for low-latency tasks (user-facing). Batch transform jobs for large-scale data processing, periodic recommendations. Asynchronous inference (SageMaker Asynchronous Inference) for time-consuming inference tasks. Multi-model endpoints (GPU) to host multiple models on the same GPU endpoint, optimizing resource use. The improvements in collaboration between iFood and the SageMaker Inference team include:\nOptimizing cost and performance for inference (reducing ~50% of cost for some workloads, lowering ~20% average latency when using inference components). Improving autoscaling to handle spikes more effectively (shortening scaling time, enhancing detection of scale events). Easier deployment of LLM / Foundation Models (FM). Scale-to-zero feature for endpoints helps save costs when there is no traffic. Multi-model GPU endpoints reduce infrastructure costs in multi-model scenarios. Model Optimization and Packaging Some technical points iFood focuses on:\nStandardizing containers for both training and serving. Automating the build/publish of images to the registry (ECR). Packaging LLM / FM for faster deployment. Supporting autoscaling and scale-to-zero for dev/test environments and low-traffic workloads. Achieved Benefits \u0026amp; Impact The benefits iFood has gained:\nReduced time to get models into production (faster time-to-market). Increased pipeline \u0026amp; artifact reuse across teams. Lower operational costs through GPU/multi-model optimization and scale-to-zero. Improved stability and model management at scale. Conclusion By leveraging SageMaker capabilities, iFood has transformed its approach to ML/AI: building a centralized ML platform (ML Go!), automating data flows, standardizing containers, and collaborating with the SageMaker Inference team to optimize efficiency, cost, and scalability. This has helped iFood:\nBridge the gap between Data Science and Engineering. Deploy hundreds of ML models reliably. Create a reference platform for organizations wishing to apply inference at scale. “At iFood, we are leading the way in applying AI and machine learning technologies to transform\u0026hellip; The lessons learned have helped us create our internal platform, which can serve as a blueprint for other organizations\u0026hellip;”\n– Daniel Vieira, Director of ML Platforms at iFood.\nAbout the Authors Daniel Vieira — Director of Machine Learning Engineering at iFood. He has a background in computer science (BSc \u0026amp; MSc, UFMG) and over a decade of experience in software engineering and ML platforms. He enjoys music, philosophy, and coffee.\nDebora Fanin — Senior Customer Solutions Architect at AWS (Brazil). She specializes in managing enterprise customer transformation and designing effective cloud adoption strategies.\nSaurabh Trikande — Senior Product Manager, Amazon Bedrock \u0026amp; SageMaker Inference. Focuses on democratizing AI and inference solutions at scale.\nGopi Mudiyala — Senior Technical Account Manager at AWS. Supports clients in the financial services industry and is passionate about machine learning.\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.1-week1/",
	"title": "Worklog Week 1",
	"tags": [],
	"description": "",
	"content": " Week 1 Goals: Get familiar with the team and learning environment at First Cloud Journey (FCJ). Understand the overview of AWS services and basic service groups. Learn how to create an AWS Free Tier account and activate $200 credit. Get familiar with the AWS Management Console to use basic services. Install and set up AWS CLI to interact with AWS resources via the command line. Practice some basic labs such as creating an account, setting up MFA, budgets, and exploring EC2. Tasks for this week Day Task Start Date End Date Resource 2 - Create AWS Free Tier account - Follow 5 steps to receive $200 credit: + Use the platform model in Amazon Bedrock playground + Create Amazon RDS database + Build a web application using AWS Lambda + Set up cost budget with AWS Budgets + Launch an EC2 instance with Amazon EC2 08/09/2025 08/09/2025 AWS Free Tier 3 - Watch and learn AWS introductory modules (YouTube): + Module 01-01 - What is Cloud Computing? + Module 01-02 - What makes AWS unique? + Module 01-03 - How to get started with the cloud journey + Module 01-04 - AWS Global Infrastructure + Module 01-05 - AWS Services Management Tools + Module 01-06 - Cost Optimization and Working with AWS + Module 01-07 - Hands-on and further research 09/09/2025 09/09/2025 Study Materials 4 - Practice (YouTube): + Module 01-Lab01-01 - Create AWS Account + Module 01-Lab01-02 - Set up Virtual MFA device + Module 01-Lab01-03 - Create an Admin group and users + Module 01-Lab01-04 - Support Account Authentication + Module 01-Lab07-01 - Create Budget from Template + Module 01-Lab07-02 - Guide to Creating Cost Budgets + Module 01-Lab07-03 - Create Usage Budget in AWS + Module 01-Lab07-04 - Create Reserved Instance Budget (RI) + Module 01-Lab07-05 - Create Savings Plans Budget + Module 01-Lab07-06 - Clean Up Budgets + Module 01-Lab09-01 - AWS Support Plans + Module 01-Lab09-02 - Types of Support Requests + Module 01-Lab09-03 - Change Support Plan + Module 01-Lab09-04 - Manage Support Requests 10/09/2025 10/09/2025 Study Materials 5 - Learn EC2 basics (hands-on): + What is EC2? + AMI (Amazon Machine Image) + EBS (Elastic Block Store) + Elastic IP + Methods to SSH into EC2 + Create and manage EC2 instances 11/09/2025 11/09/2025 Study Materials Achievements of Week 1: Successfully created AWS Free Tier account and activated $200 credit. Learned to log in and use AWS Management Console, search and access basic services. Successfully installed and configured AWS CLI on the personal computer, including: Setting Access Key, Secret Key, and default Region. Became familiar with theoretical modules and hands-on labs on YouTube, including: Overview of cloud computing and AWS AWS global infrastructure Managing account and cost budgets Introduction to EC2 and creating an instance Started to understand how to create, launch, and connect to an EC2 instance. Learned how to set up a budget with AWS Budgets to monitor usage costs. Completed basic security configurations (setting up MFA, IAM admin users). "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Reflection Report “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” Event Information Event name: AWS Cloud Day Vietnam – AI Edition 2025 (Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders) Date: 18 September 2025 Location: 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City Role: Attendee Purpose of the Event AWS Cloud Day Vietnam – AI Edition 2025 is positioned as an important gathering of the technology and business community in Vietnam. Its main goal is to accelerate digital transformation by leveraging the combined power of Cloud Computing and Artificial Intelligence (AI).\nThe objectives can be summarized into the following pillars and meanings:\nConnecting the Vietnamese technology community:\nCreating a space for companies, IT experts, developers, and industry leaders to meet, exchange, and learn together about Cloud \u0026amp; AI.\nHelping businesses understand and practically apply Cloud \u0026amp; AI:\nFocusing on topics such as Gen AI, data analytics, modernizing workloads, and industry cloud tailored for specific sectors.\nShowcasing success stories:\nPresenting case studies such as Xanh SM, Honda Vietnam, Masterise Group, Techcombank, TymeX, F88… to demonstrate how Cloud \u0026amp; AI can transform business operations.\nPromoting digital economy growth and innovation:\nContributing to the realization of the national digital transformation strategy, making Cloud \u0026amp; AI a foundation for economic growth.\nBuilding a platform for long-term networking and collaboration:\nCreating bridges between technology – enterprises – industry leaders, aiming at sustainable cooperation in the digital era.\nFour key strategic pillars emphasized Democratizing Generative AI (GenAI) for businesses\nMoving GenAI beyond initial “hype” into practical applications. Demonstrating how businesses can transform generic AI models into context-aware solutions tied to a clear data strategy. Bridging the gap between Business and IT\nEspecially in the Financial Services (FSI) domain. Cloud is not just infrastructure, but a driver of business value, enabling models such as Ecosystem Banking and Embedded Finance. Accelerating industry-specific modernization\nDesigning customized roadmaps for each sector: Retail, Energy, Telecommunications, Real Estate, Public Sector, etc. Emphasizing that modernization has no “one-size-fits-all formula”; it must be based on the characteristics of each system and each business strategy. Strengthening security and resilience\nPromoting the “security by design” mindset – security embedded from the design phase. Integrating security throughout the entire application lifecycle, from development to production operations. Speakers The event brought together 24 speakers from government agencies, embassies, financial institutions, technology companies, and AWS partners. Some key speakers:\nH.E. Pham Duc Long – Vice Minister, Ministry of Science and Technology, Vietnam H.E. Marc E. Knapper – United States Ambassador to Vietnam Jaime Valles – Vice President, Managing Director for Asia Pacific \u0026amp; Japan, AWS Jeff Johnson – Managing Director ASEAN, AWS Dr Jens Lottner – CEO, Techcombank Dieter Botha – CEO, TymeX Trang Phung – CEO, U2U Network Vu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Along with many other leaders and experts from F88, Masterise Group, VTV Digital, Honda Vietnam, Mobifone, Katalon, Renova Cloud, TechX Corp,… Key Content 1. Strategic convergence: Policy and leadership Government endorsement:\nThe opening remarks from the Vice Minister of the Ministry of Science and Technology and the U.S. Ambassador showed strong support for Vietnam’s digital infrastructure and Cloud–AI ecosystem.\nLeadership Panel:\nLeaders such as Jeff Johnson (AWS), Vu Van (ELSA), Nguyen Hoa Binh (Nexttech)… emphasized the role of people and an innovation culture in driving digital transformation, not just technology alone.\n2. Track 1: Financial Services (FSI) – New banking models Innovation in Banking \u0026amp; Insurance:\nTechcombank, Bao Viet Holdings and other financial organizations shared their journey towards Ecosystem Banking and Embedded Finance.\nXGenAI implementation:\nTechX presented the XGenAI platform built on AWS, illustrating how GenAI is used to personalize customer experiences, optimize care, and recommend financial services.\n3. Track 2: Multi-industry modernization Honda Vietnam:\nShared a detailed roadmap for migrating the SAP system to AWS, going beyond “lift-and-shift” towards changing the operating model, optimizing costs, and increasing flexibility.\nVTV Digital \u0026amp; Mobifone:\nPresented their digital transformation journeys “From Vision to Value” in digital media and telecommunications.\nMasterise Group:\nHighlighted the migration of hundreds of VMware workloads to AWS, showing the scale and complexity of modernizing real estate infrastructure.\n4. Tracks 3 \u0026amp; 4: Data, AI and DevOps Data Strategy:\nExperts from Onebyzero and Techcom Securities emphasized that “data is the key differentiator” for GenAI; AI output quality directly depends on input data quality.\nThe DevOps revolution:\nKatalon and Renova Cloud discussed integrating GenAI into the DevOps lifecycle: auto code generation, automated test creation, log analysis, and CI/CD optimization.\n5. Negative impacts of legacy application architectures Slow product release cycles → Lost revenue and missed market opportunities. Inefficient operations → Lost productivity and higher operational costs. Non-compliance with security standards → High security risks and reputational damage. These limitations are the driving force behind modernizing application architectures.\n6. Transition to Microservices Architecture The event emphasized the shift from monolithic systems to Microservices Architecture:\nSystems are modularized, with each function as an independent service. Services communicate via events (event-driven). Three key architectural pillars were highlighted:\nQueue Management: Handling asynchronous tasks, offloading the main system. Caching Strategy: Optimizing performance and reducing query latency. Message Handling: Enabling flexible communication between services, supporting scalability and fault tolerance. 7. Domain-Driven Design (DDD) Four-step approach:\nIdentify domain events → build timelines → identify actors → define bounded contexts.\n“Bookstore” case study:\nUsed to demonstrate how to apply DDD in practice, from domain modeling to service decomposition.\nContext mapping – 7 integration patterns:\nHelping manage relationships between bounded contexts (Partnership, Shared Kernel, Anti-corruption Layer, etc.).\n8. Event-Driven Architecture Three main integration patterns:\nPublish/Subscribe Point-to-Point Streaming Benefits:\nLoose coupling between services Easy scalability Higher resilience in case of localized failures Sync vs Async comparison:\nSpeakers clearly explained the trade-offs: synchronous calls are simpler but prone to bottlenecks; asynchronous patterns are more complex but more flexible and scalable.\n9. Compute Evolution \u0026amp; Serverless Shared Responsibility Model:\nExplained the roles of AWS vs customers across compute models: EC2 → ECS → Fargate → Lambda.\nBenefits of Serverless:\nNo server management Automatic scaling Pay-for-value based on actual usage Functions vs Containers:\nSuggested criteria for choosing between them (runtime duration, statefulness, frequency, deployment complexity, etc.).\n10. Amazon Q Developer SDLC automation:\nSupporting developers across the entire lifecycle from planning → coding → testing → maintenance.\nCode transformation:\nHelping upgrade Java, modernize .NET, and migrate from mainframe/legacy stacks to modern architectures.\nAWS Transform Agents:\nProviding dedicated agents for VMware, Mainframe, .NET… to reduce time and risk during application modernization.\nLessons Learned 1. Design \u0026amp; Strategic Thinking Business-first approach:\nTechnology solutions must start from business problems, rather than following technology trends blindly.\nUbiquitous Language:\nA shared language between business and tech teams helps reduce misunderstandings and accelerates requirements analysis and design.\nResilience as a baseline:\nModern systems need to be designed with resilience from the beginning, instead of patching after incidents occur.\n2. Technical Architecture Event storming:\nAn effective method to model business processes into domain events, leading naturally to DDD and event-driven architecture.\nEvent-driven communication:\nReplacing part of synchronous communication with event-driven mechanisms makes the system more flexible, more scalable, and less tightly coupled.\nIntegration patterns:\nUnderstanding clearly when to use synchronous APIs, when to use pub/sub, and when to use streaming (Kafka/Kinesis).\nCompute spectrum:\nKnowing how to balance between VMs, containers, and serverless to choose the most suitable runtime for each workload.\n3. Modernization Strategy Phased approach:\nInstead of “tearing everything down and rebuilding”, a phased roadmap is needed, prioritizing systems with the highest impact.\n7Rs framework:\nThere are multiple paths to modernization (Rehost, Replatform, Refactor, Rearchitect, Repurchase, Retire, Retain).\nEach application must be assessed individually.\n“Migrate to Operate”:\nThe goal is not just to move to the cloud, but to operate more intelligently after migration, continuously optimizing cost, performance, and security.\nROI measurement:\nIt is important to measure clearly: cost reduction, increased agility, shorter time-to-market, and improved customer experience.\nApplication to My Work Applying DDD to current projects:\nOrganizing event storming sessions with the business team to model domains and split bounded contexts.\nRefactoring Microservices:\nUsing bounded contexts to define service boundaries, avoiding microservices being “over-fragmented” or overlapping responsibilities.\nImplementing event-driven patterns:\nGradually replacing certain synchronous APIs with asynchronous messaging, pub/sub, or streaming.\nServerless Adoption:\nExperimenting with some use cases on AWS Lambda or Fargate to evaluate real-world benefits.\nAssessing data readiness:\nBefore starting any GenAI project, checking data quality, data governance, and the overall data strategy.\nExperimenting with GenAI in DevOps:\nUsing tools like Amazon Q Developer to auto-generate code, tests, and shorten the development cycle.\nImplementing “Security by design”:\nApplying best practices in IAM, encryption, logging, and monitoring right from the system design stage.\nEvent Experience Attending AWS Cloud Day Vietnam – AI Edition 2025 provided me with insights that are both strategic and hands-on:\nLearning from high-profile experts Presentations from banking leaders, cloud experts, and solution architects helped me clearly see the connection between business strategy and technical infrastructure. Real-world case studies (Techcombank, Honda, Masterise, TymeX…) gave me a concrete picture of the transformation journeys in different industries. Practical technical experience Sessions on event storming, DDD, microservices, and event-driven architecture helped me connect theoretical knowledge with realistic deployment scenarios. I was particularly impressed by the “GenAI-powered App-DB Modernization” workshop, which simulated the journey of modernizing a monolithic application into a cloud-native architecture. Using modern tools My first deep dive into Amazon Q Developer in a real SDLC context showed me the potential of using AI to increase developer productivity and reduce repetitive work. Integrating GenAI into DevOps gave me a new perspective on the future of software development. Networking and discussions The event was a great opportunity to talk with experts and professionals working in finance, technology, and cloud consulting. Through short side conversations, I realized that digital transformation is not only about technology, but also about changing mindsets, ways of working, and organizational culture. Key takeaways Data is the key differentiator: GenAI cannot be truly effective if data is not properly governed. Modernization is a continuous journey: There is no fixed “final destination”; it is an ongoing process of optimization and adaptation. Security is everyone’s responsibility: From developers to operators, from leadership to staff, everyone must be conscious about security. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Reflection Report “Exploring Agentic AI – Amazon QuickSuite” Event Information **Event name: GenAI-powered App-DB Modernization workshop **Date: 7 November 2025 Location: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City Role: Attendee Purpose of the Event Clarify the concept of Agentic AI: the shift from passive Generative AI to Agentic AI (AI agents) capable of autonomous action. Introduce and demonstrate Amazon QuickSuite live for the first time in Vietnam. Encourage businesses to adopt Agentic AI through the AWS LIFT financial support program (credits of up to USD 80,000). Provide a practical hands-on environment in which participants can directly build and experience AI models under the guidance of AWS experts and partners. Speakers Vivien Nguyen – Territory Manager, AWS Tung Cao – Solution Architect, AWS Cloud Kinetics team – AWS strategic implementation partner in Vietnam Key Content Paradigm shift: From Generative AI to Agentic AI Generative AI\nFocuses on generating content (text, images, source code, etc.) based on user prompts. Fundamentally reactive: only responds when queried. Agentic AI (AI agents)\nFocuses on autonomy and the ability to act. Can perceive its environment, plan, reason through multiple steps, and execute tasks automatically without constant human intervention. The goal is to build systems where “AI does the work for us”, not just “chats with us”. The workshop helped me clearly understand that Agentic AI is the evolution from “AI that answers” to “AI that acts”, which is very suitable for complex and repetitive tasks in enterprises.\nIntroduction to Amazon QuickSuite First live demo in Vietnam:\nThe workshop marked the first live demonstration of Amazon QuickSuite for users in Vietnam. Unified ecosystem:\nTightly integrates: Amazon QuickSight – data visualization and analytics. QuickSuite Q – natural language interaction for asking questions and generating insights. Together, they enable “Analyst Agents” that can read data, answer business questions, and propose actions. Speed and agility (“Quick”):\nAllows businesses to move from idea → prototype → real-world testing in a short time. Suitable for teams that want to experiment quickly and iterate continuously. Data-centric by design:\nQuickSuite is built for large data volumes, ensuring that AI agents have enough “fuel” to make accurate decisions grounded in real business context. Partner ecosystem \u0026amp; strategic support AWS provides the underlying infrastructure platform and AI/Analytics services. Cloud Kinetics plays the role of: Architecture advisor, ensuring that solutions fit the specific needs of each business. Supporting the “last mile” of implementation – turning abstract technology into concrete solutions that run in production. This “two-layer support model” (AWS + partner) helps businesses reduce risk, especially when starting out with new technologies such as Agentic AI.\nAWS LIFT program – Financial support for innovation Credits of up to USD 80,000 for eligible customers, particularly new customers and SMBs. Purpose: Lower the cost barrier when experimenting with high-performance compute systems and AI-related R\u0026amp;D projects. Allow companies to focus on ideas and execution, instead of worrying too much about initial infrastructure costs. What I Learned Design Mindset Focus on system autonomy:\nWhen designing Agentic AI, the goal is to build agents that proactively work on behalf of humans, for example: Automatically generating periodic reports. Monitoring metrics and sending alerts. Suggesting operational adjustments (such as in the supply chain). Tying AI to concrete operational bottlenecks:\nAI should not be built just because it is a “trend”, but should start from processes that are: Repetitive. Composed of many manual steps. Error-prone when done entirely by humans. Technical Architecture Ecosystem-based approach:\nAn effective agent must be connected to multiple tools: data sources, BI, APIs, workflows, etc. In this context, QuickSuite is the “connective tissue” between: Data (data sources, QuickSight). Action logic (questions, rules, automated task workflows). AWS infrastructure readiness:\nThe first step is to set up the AWS account, region, access permissions, and required services. This lays the foundation to easily enable/disable and experiment with new services such as QuickSuite and Agentic workflows later on. Modernization \u0026amp; Deployment Strategy Early adopter advantage:\nBusinesses/teams that learn and master QuickSuite early will gain advantages in productivity, decision-making speed, and the ability to test new business models. Smart cost management:\nCombining AWS LIFT credits with a small, clearly scoped PoC approach. Helps accelerate time-to-market while still minimizing financial risk. Application to My Work Deepening my knowledge of QuickSuite:\nExploring how to integrate QuickSight + QuickSuite Q into existing data analytics workflows. Aiming to build “Analyst Agents” that can automatically answer questions about KPIs, trends, and reporting. Leveraging the AWS LIFT program:\nReviewing eligibility and applying to receive AWS credits for upcoming AI/Agentic AI R\u0026amp;D or PoC projects. Identifying internal use cases suitable for Agentic AI:\nReviewing workflows that involve many repetitive steps (reporting, monitoring, request classification, etc.) to see whether they can be transformed into AI agent workflows. Collaborating with partners such as Cloud Kinetics:\nInstead of building everything 100% in-house, partnering with experts to design a robust architecture from the start, and avoid fundamental mistakes for complex systems. Event Experience Attending the “Exploring Agentic AI – Amazon QuickSuite” workshop at Bitexco Financial Tower was a professional and multi-dimensional experience: combining theory, hands-on practice, and community connection.\nLearning from experienced speakers I had the opportunity to hear directly from AWS and Cloud Kinetics about the vision for Agentic AI in enterprises. The content was well balanced between concepts (what Agentic AI is and why it matters) and real-world implementation (how to use QuickSuite and where to start). Hands-on technical experience The ~90-minute hands-on session allowed me to: Work directly with QuickSight + QuickSuite Q. Understand more concretely how an “analyst agent” can read data and answer business questions. The “hands-on guidance” from AWS experts helped me quickly overcome initial difficulties when working with a new tool. Networking and ecosystem Networking time enabled me to talk with: Professionals working in cloud, data, and AI. The Cloud Kinetics team, which helped me better understand the role of partners in turning the AWS platform into concrete solutions for each industry. Key takeaways Agentic AI is the future of enterprise operations:\nThe shift from “chatting with AI” to “AI actually doing the work” will unlock many new optimization models. Speed is a critical factor:\nTools like QuickSuite are designed for rapid deployment, so early adopters will gain a clear advantage in agility. Capital enables innovation:\nThe AWS LIFT program shows that the key question is no longer “Can we afford it?”, but rather “Do we have the courage and readiness to seize the opportunity?”. In summary, the workshop helped me gain a much clearer understanding of Agentic AI, how AWS brings it to life through Amazon QuickSuite, and the resources (tools, partners, funding) that AWS provides so that businesses can start their AI adoption journey in a practical and sustainable way.\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Trần Hoàng Tín\nPhone Number: 0936091757\nEmail: tinthse185022@fpt.edu.vn\nUniversity: FPT University Ho Chi Minh\nMajor: Information Technology\nClass: SE185022\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Week 1 Worklog\nWeek 2: Week 2 Worklog\nWeek 3: Week 3 Worklog\nWeek 4: Week 4 Worklog\nWeek 5: Week 5 Worklog\nWeek 6: Week 6 Worklog\nWeek 7: Week 7 Worklog\nWeek 8: Week 8 Worklog\nWeek 9: Week 9 Worklog\nWeek 10: Week 10 Worklog\nWeek 11: Week 11 Worklog\nWeek 12: Week 12 Worklog\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " How Salesforce Business Technology Uses AWS Direct Connect SiteLink for Reliable Global Connectivity By Alexandra Huides and Corey Harris Jr – May 9, 2025, in the AWS Direct Connect SiteLink section.\nIntroduction Salesforce Business Technology has used AWS Direct Connect SiteLink to build a global hybrid network architecture, ensuring flexible, high-performance, and reliable connectivity.\nThis solution helped Salesforce scale its infrastructure, reduce operational costs, and accelerate innovation in its journey toward cloud modernization with AWS.\nThis article is brought to you in collaboration with Georgi Stoev and Ravi Patel – senior technical experts at Salesforce.\nOverview Salesforce is an AWS strategic partner and the world leader in customer relationship management (CRM).\nThe Business Technology team is responsible for building and operating enterprise applications that support areas such as finance, data centers, security, data warehousing, and Salesforce\u0026rsquo;s virtual machines.\nWith a global presence, Salesforce needed a network architecture that was:\nFlexible and highly scalable. Minimized latency and downtime. Ensured high security and reliability. However, traditional internet-based network solutions couldn\u0026rsquo;t meet these stringent requirements.\nThis is why AWS Direct Connect SiteLink was chosen — providing a private, dedicated connection, bypassing the public internet to significantly improve security and latency.\nPrerequisites Before deployment, the Salesforce technical team understood the following AWS network components:\nAmazon Virtual Private Cloud (VPC) AWS Transit Gateway AWS Direct Connect These services form the foundation for the global hybrid network architecture, allowing for private, low-latency connectivity between multiple Direct Connect locations without going through intermediate AWS regions.\nAWS Direct Connect SiteLink AWS Direct Connect provides a private network connection between on-premises infrastructure and AWS, optimizing performance, latency, and reliability.\nSiteLink is an extension feature of Direct Connect that allows direct connections between on-premises networks via the global AWS backbone, enabling:\nData transmission through the shortest path, bypassing AWS regions. Leveraging the global AWS network to transfer data quickly and securely. Pay-per-use pricing, without needing to establish new connections. How it works:\nEstablish an on-premises connection to AWS at one of over 100 Direct Connect locations worldwide. Create a Virtual Interface (VIF) on that connection and enable SiteLink. When VIFs are attached to the same Direct Connect Gateway (DXGW), data is transmitted directly between locations using the AWS backbone. Salesforce Business Technology\u0026rsquo;s Global Footprint Salesforce Business Technology manages 7 strategic locations globally:\n3 in the United States 3 in the Asia-Pacific region 1 in Europe The network is built on private MPLS backbone links combined with AWS Regions, supporting complex data flows between data centers and cloud environments.\nHowever, challenges arose including:\nStatic and hard-to-scale infrastructure. High operational costs and heavy reliance on multiple providers. Routing complexity and outages in some regions. Figure 1. A sample global private data center connection using dedicated circuits.\nSolution: SiteLink To solve these problems, Salesforce Business Technology modernized its network infrastructure by deploying SiteLink.\nThe main objectives were:\nBuild a flexible, scalable network as needed. Reduce operational costs and complexity. Enhance resilience and security. The team:\nDeployed SiteLink on existing Direct Connect connections. Created new dedicated VIFs for production and development environments. Maintained a global segmentation to meet on-premises data storage requirements. Figure 2. Sample global SiteLink deployment for Production and Development.\nAchieved Benefits The SiteLink solution delivered several outstanding benefits for Salesforce:\nBenefit Description Simplified network management Eliminated the complexity of MPLS Layer 3 VPN routing while maintaining traffic isolation. Improved performance Increased stability, reducing global average latency by 15%. Cost optimization Utilized existing connections with pay-per-use billing. Enhanced security Applied MACSec Layer 2 encryption on all Direct Connect connections. Additionally, SiteLink helps:\nReduce single points of failure (SPOF) and improve network reliability. Optimize data path routing between data centers. Comprehensive monitoring via CloudWatch Network Monitor. “With SiteLink, Salesforce Business Technology has streamlined network operations and ensured maximum resilience for global connectivity. We can set up connections between 7 data centers in just a few minutes and expand into new markets in a few days.”\n— Ravi Patel, Senior Technical Director at Salesforce.\nConclusion Adopting AWS Direct Connect SiteLink has helped Salesforce:\nUnify its global network architecture. Modernize infrastructure, reduce costs, and improve performance. Be ready for scaling and rapid innovation. To learn more about AWS Direct Connect SiteLink, you can refer to the official documentation or ask questions on AWS re:Post.\nAbout the Authors Alexandra Huides\nSenior Network Solutions Architect at AWS.\nSpecializes in large-scale network architecture, helping clients adopt IPv6 and build flexible environments. Outside of work, she enjoys kayaking, traveling, and reading books.\nCorey Harris Jr.\nSenior Solutions Architect at AWS.\nA network and serverless expert, helping customers optimize their AWS systems. Outside of work, he enjoys gaming, traveling, and spending time with family.\nGeorgi Stoev\nSenior Technical Architect at Salesforce.\nWith over 20 years of experience in networking, AI, and security, he is passionate about technology, beekeeping, and nature exploration.\nRavi Patel\nSenior Technical Director at Salesforce.\nWith over 15 years of experience in building flexible and high-performance networks, he enjoys surfing, mountaineering, and adventuring around the world.\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.2-week2/",
	"title": "Worklog Week 2",
	"tags": [],
	"description": "",
	"content": " Week 2 Objectives: Dive deeper into AWS services, focusing on Compute, Networking, and Storage. Get familiar with AWS EC2, S3, Lambda, and Elastic Beanstalk. Practice creating and managing EC2 instances, working with storage, and deploying a web application. Understand AWS IAM for managing identities and access permissions. Tasks to be carried out this week: Day Tasks Start date Completion date References 2 - Learn the basics of AWS EC2 + Types of EC2 instances + EC2 Security Groups + Managing the lifecycle of EC2 instances 15/09/2025 15/09/2025 Study materials 3 - Learn about AWS S3 + Create an S3 Bucket + Manage objects in S3 + Control access (ACL, Bucket policy) 16/09/2025 16/09/2025 Study materials 4 - Learn about AWS Lambda + Introduction to serverless architecture and AWS Lambda + Create a simple Lambda function + Trigger Lambda from other AWS services 17/09/2025 17/09/2025 Study materials 5 - Learn about AWS Elastic Beanstalk + Overview of Elastic Beanstalk + Deploy a sample application using Elastic Beanstalk 18/09/2025 18/09/2025 Study materials 6 - Learn about IAM (Identity and Access Management) + Create and manage IAM Users, Groups, and Roles + Attach policies and manage access 19/09/2025 19/09/2025 Study materials 7 - Summary practice: + Labs for EC2, S3, Lambda, Elastic Beanstalk + Review steps for creating, configuring, and deploying 20/09/2025 20/09/2025 Study materials Week 2 Achievements: Created and managed EC2 instances (launch, stop/delete, view details). Understood how to configure and use AWS S3 to store and manage objects (upload, view, delete, set permissions). Created and tested a simple AWS Lambda function, and learned how to trigger Lambda from other services. Successfully deployed a sample application using AWS Elastic Beanstalk. Gained a basic understanding of AWS IAM, including how to: Create IAM Users, Groups, and Roles. Attach IAM Policies to control access following the “least privilege” principle. Completed several combined hands-on labs for EC2, S3, Lambda, and Elastic Beanstalk, reinforcing the theoretical knowledge learned. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Team Super Beast Warrior(SBW) Batch-based Clickstream Analytics Platform 1. Executive Summary This project aims to design and implement a Batch-based Clickstream Analytics Platform for a computer and accessories e-commerce website (The website’s frontend integrates a lightweight JavaScript SDK that sends user activity data (clicks, views, searches) to the backend API) using AWS Cloud Services. The system collects user interaction data (such as clicks, searches, and page visits) from the website and stores it in Amazon S3 as raw logs. Every hour, Amazon EventBridge triggers AWS Lambda functions to process and transform the data before loading it into a data warehouse hosted on Amazon EC2.\nThe processed data is visualized through R Shiny dashboards, providing store owners with business insights such as customer behavior patterns, product popularity, and website engagement trends.\nThis architecture focuses on batch analytics, ETL pipelines, and business intelligence while ensuring security, scalability, and cost efficiency by leveraging AWS managed services.\n2. Problem Statement What’s the Problem? E-commerce websites generate a large volume of clickstream data—including product views, cart actions, and search activities—that contain valuable business insights.\nHowever, small and medium-sized stores often lack the infrastructure and expertise to collect, process, and analyze this data effectively.\nAs a result, they face difficulties in:\nUnderstanding customer purchasing behavior Identifying top-performing products Optimizing marketing campaigns and website performance Making data-driven inventory and pricing decisions The Solution This project introduces an AWS-based batch clickstream analytics system that automatically collects user interaction data from the website every hour, processes it through serverless functions, and stores it in a central data warehouse on Amazon EC2.\nThe results are visualized using R Shiny dashboards, enabling store owners to gain actionable insights into customer behavior and improve overall business performance.\nBenefits and Return on Investment Data-driven decision making: Discover customer preferences, popular products, and shopping trends. Scalable and modular design: Easily extendable to handle more users or additional data sources. Cost-efficient batch processing: Reduces continuous compute costs by operating on a scheduled, hourly basis. Business insight enablement: Empowers store owners to optimize sales strategies and improve revenue using evidence-based analytics. 3. Solution Architecture AWS Services Used Amazon Cognito: Handles user authentication and authorization for both administrators and website customers, ensuring secure access to the e-commerce platform. Amazon S3: Acts as a centralized data storage layer — hosting the static website front-end and storing raw clickstream logs collected from user interactions. It also temporarily holds batch files before they are processed and transferred to the data warehouse. Amazon CloudFront: Distributes static website content globally with low latency, improving user experience and caching resources close to customers. Amazon API Gateway: Serves as the main entry point for incoming API calls from the website, enabling secure data submission (such as clickstream or browsing activity) into AWS. AWS Lambda: Executes serverless functions to preprocess and organize clickstream data uploaded to S3. It also handles scheduled data transformation jobs triggered by EventBridge before loading them into the data warehouse. Amazon EventBridge: Schedules and orchestrates batch workflows — for example, triggering Lambda functions every hour to process and move clickstream data from S3 into the EC2 data warehouse. Amazon EC2 (Data Warehouse): Acts as the data warehouse environment, running PostgreSQL or another relational database for batch analytics, trend analysis, and business reporting. Both instances are deployed inside a VPC private subnet for network isolation and security R Shiny (on EC2): Hosts interactive dashboards that visualize batch-processed insights, helping the business explore customer behavior, popular products, and sales opportunities. AWS IAM: Manages access permissions and policies to ensure that only authorized users and AWS components can interact with data and services. Amazon CloudWatch: Collects and monitors metrics, logs, and scheduled job statuses from Lambda and EC2 to maintain system reliability and performance visibility. Amazon SNS: Sends notifications or alerts when batch jobs complete, fail, or encounter errors, ensuring timely operational awareness. 4. Technical Implementation End-to-end data flow Auth (Cognito). Browser authenticates with Amazon Cognito (Hosted UI or JS SDK). ID token (JWT) is stored in memory; SDK attaches Authorization: Bearer \u0026lt;JWT\u0026gt; for API calls. Static web (CloudFront + S3). SPA/assets hosted on S3; CloudFront in front with OAC, gzip/brotli, HTTP/2, WAF managed rules. The page loads a tiny analytics SDK that collects events and sends to API Gateway (below). Event ingest (API Gateway). POST /v1/events (HTTP API). CORS locked to site origin; JWT authorizer validates Cognito token (or API key for anon flows). Requests forwarded to Lambda. Security \u0026amp; Ops. IAM least-privilege for every component. CloudWatch logs/metrics/alarms on API 5xx, Lambda errors, throttles, Shiny health. SNS notifies on alarms \u0026amp; DLQ growth. Processing \u0026amp; storage (Lambda → S3 batch buffer → EventBridge → Lambda(ETL) → PostgreSQL on EC2 (data warehouse) → Shiny). Ingest Lambda validates/enriches events then append-writes NDJSON objects into S3 (partitioned by date/hour). EventBridge (cron) triggers an ETL Lambda (batch) on a fixed cadence (e.g., every 60 minutes). ETL Lambda reads a slice of S3 partitions, deduplicates, transforms, and upserts into PostgreSQL on EC2 (VPC access). R Shiny Server (on EC2) reads curated tables and renders dashboards for admins. Data Contracts \u0026amp; Governance Event JSON (ingest)\n{ \u0026#34;event_id\u0026#34;: \u0026#34;uuid-v4\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2025-10-18T12:34:56.789Z\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;view|click|search|add_to_cart|checkout|purchase\u0026#34;, \u0026#34;session_id\u0026#34;: \u0026#34;uuid-v4\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;cognito-sub-or-null\u0026#34;, \u0026#34;anonymous_id\u0026#34;: \u0026#34;stable-anon-id\u0026#34;, \u0026#34;page_url\u0026#34;: \u0026#34;https://site/p/123\u0026#34;, \u0026#34;referrer\u0026#34;: \u0026#34;https://google.com\u0026#34;, \u0026#34;device\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mobile|desktop|tablet\u0026#34; }, \u0026#34;geo\u0026#34;: { \u0026#34;country\u0026#34;: \u0026#34;VN\u0026#34;, \u0026#34;city\u0026#34;: null }, \u0026#34;ecom\u0026#34;: { \u0026#34;product_id\u0026#34;: \u0026#34;sku-123\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Shoes\u0026#34;, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;price\u0026#34;: 79.99, \u0026#34;qty\u0026#34;: 1, }, \u0026#34;props\u0026#34;: { \u0026#34;search_query\u0026#34;: \u0026#34;running shoes\u0026#34; }, } PII: never send name/email/phone; any optional identifier is hashed in Lambda. Behaviour: generate anonymous_id once, maintain session_id (roll after 30 min idle), send via navigator.sendBeacon with fetch retry fallback; optional offline buffer via IndexedDB. S3 raw layout \u0026amp; retention\nBucket: s3://clickstream-raw/ Object format: NDJSON, optionally GZIP. Partitioning: year=YYYY/month=MM/day=DD/hour=HH/ → events-\u0026lt;uuid\u0026gt;.ndjson.gz Optional manifest per batch: processed watermark, object list, record counts, hash. Lifecycle: raw → (30 days Standard/IA) → (365+ days Glacier/Flex). Idempotency: maintain a compact staging table in PostgreSQL (or a small S3 key-value manifest) to track last processed object/batch and prevent double-load. Frontend SDK (Static site on S3 + CloudFront) Instrumentation\nTiny JS snippet loaded site-wide (defer). Generates anonymous_id once and keeps session_id in localStorage; session rolls after 30 minutes of inactivity. Sends events via navigator.sendBeacon; fallback to fetch with retry \u0026amp; jitter. Auth context\nIf user signs in with Cognito, include user_id = idToken.sub to enable logged-in funnels. Offline durability\nOptional Service Worker queue: when offline, buffer events in IndexedDB and flush on reconnect. Ingestion API (API Gateway → Lambda) API Gateway (HTTP API)\nRoute: POST /v1/events. JWT authorizer (Cognito user pool). For anonymous pre-login events, use an API key usage-plan with strict rate limits. WAF: AWS Managed Core + Bot Control; block non-site origins via strict CORS. Lambda (Node.js or Python)\nValidate against JSON Schema (ajv/pydantic). Idempotency: recent event_id cache in memory (short TTL) + batch-level dedupe during ETL. Enrichment: derive date/hour, parse UA, infer country from CloudFront-Viewer-Country if present. Persist: PutObject to S3 path .../year=YYYY/month=MM/day=DD/hour=HH/.... Failure path: publish to SQS DLQ; alarm via SNS if DLQ depth \u0026gt; 0. Batch Buffer (S3) Purpose: cheap, durable buffer for batch analytics. Write pattern: small per-request objects or micro-batches (e.g., 1–5 MB each) with GZIP. Optional compactor merges into ≥64MB files for efficient reads. Read pattern: ETL Lambda scans only new partitions/objects since the last watermark. Schema-on-read: ETL applies schema, handles late-arriving data by reprocessing a small sliding window (e.g., last 2 hours) to correct sessions. EC2 “data warehouse” node Purpose: run ETL + host the curated analytical store that Shiny queries. Two choices:\nPostgres on EC2 (recommended if team prefers SQL/window functions) Instance: t3.small/t4g.small; gp3 50–100GB. Schema: fact_events, fact_sessions, dim_date, dim_product. Security: within VPC private subnet; access via ALB/SSM Session Manager; automated daily snapshots to S3 ETL (Lambda, batch via EventBridge cron): Trigger: rate(5 minutes) / cron(\u0026hellip;) depending on cost \u0026amp; freshness. Steps: list new S3 objects → read → validate/dedupe → transform (flatten nested JSON, cast types, add ingest_date, session_window_start/end) → upsert into Postgres using COPY to temp tables + merge, hoặc batched INSERT \u0026hellip; ON CONFLICT. Networking: Lambda attached to VPC private subnets to reach EC2 Postgres security group. R Shiny Server on EC2 (admin analytics) Server\nEC2 (t3.small/t4g.small) with: R 4.4+, Shiny Server (open-source), Nginx reverse proxy, TLS via ACM/ALB or Let’s Encrypt. IAM instance profile (no static keys). Security group allows HTTPS from office/VPN or Cognito-gated admin site. App (packages)\nshiny, shinydashboard/bslib, plotly, DT, dplyr, DBI + RPostgres or duckdb, lubridate. If querying DynamoDB directly for small cards, use paws.dynamodb (optional). Dashboards\nTraffic \u0026amp; Engagement: DAU/MAU, sessions, avg pages, bounce proxy. Funnels: view→add_to_cart→checkout→purchase with stage conversion \u0026amp; drop-off. Product Performance: views, CTR, ATC rate, revenue by product/category. Acquisition: referrer, campaign, device, country. Reliability: Lambda error rate, DLQ depth, ETL lag, data freshness. Caching\nQuery results cached in-process (reactive values) or materialized by ETL; cache keys by date range and filters Security baseline IAM\nIngest Lambda: s3:PutObject to raw bucket (scoped to prefix), s3:ListBucket on needed prefixes. ETL Lambda: s3:GetObject/ListBucket on raw prefixes; permission to fetch secrets from SSM Parameter Store; no broad S3 access. EC2 roles: read/write only to its own DB/volumes; optional read to S3 for backups. Shiny EC2: no write to S3 raw; read-only to Postgres as needed. Network\nPlace EC2 in private subnets; public access through ALB (HTTPS 443). Lambda for ETL joins the VPC to reach Postgres; SG rules least-priv (Postgres port from ETL SG only). No wide 0.0.0.0/0 to DB ports. Data\nEncrypt EBS (KMS), S3 server-side encryption, RDS/PG TLS, secrets in SSM Parameter Store. No PII in events; retention: raw S3 90–365 days (lifecycle), curated Postgres per business policy Observability \u0026amp; alerting CloudWatch metrics/alarms\nAPI Gateway 5xx/latency, Lambda (ingest) errors/throttles, S3 PutObject failures, EventBridge schedule success rate, ETL duration/lag, DLQ depth, Shiny health check SNS topics: on-call email/SMS/Slack webhook.\nStructured logs: JSON logs from Lambda \u0026amp; ETL (request_id, event_type, status, ms, error_code).\nWatermark tracking: custom metric “DW Freshness (minutes since last successful upsert)”.\nCost Controls (stay near Free/low tier) Use HTTP API (cheaper), minimal Lambda memory (256–512MB), compress requests. Batch over realtime: S3 as buffer eliminates DynamoDB write/read costs. S3 lifecycle: Standard → Standard-IA/Intelligent-Tiering → Glacier for older raw; enable GZIP to cut storage/transfer. Tune ETL cadence (e.g., 15–60 min) and process only new objects; compact small files into bigger chunks to reduce read I/O. Single small EC2 for Shiny + DW at start; scale vertically or split later. AWS Budgets with SNS alerts (actual \u0026amp; forecast). Deliverables Analytics SDK (TypeScript) with sessionization + beacon + optional offline queue. API/Lambda (ingest) with validation, enrichment, idempotency hints, DLQ. S3 raw bucket spec (prefixing/partitioning, compression, lifecycle) + optional compactor. ETL Lambda (batch) + EventBridge cron + watermarking + upsert strategy to PostgreSQL. PostgreSQL schema (fact_events, fact_sessions, dims) + indexes + vacuum/maintenance plan. R Shiny dashboard app (5 modules) + Nginx/ALB TLS setup. Runbook: alarms, on-call, backups, disaster recovery, freshness SLO, cost guardrails 5. Timeline \u0026amp; Milestones Project Timeline Month 1 – Learning \u0026amp; Preparation Study a wide range of AWS services including compute, storage, analytics, and security. Understand key concepts of cloud architecture, data pipelines, and serverless computing. Conduct team meetings to align project goals and assign responsibilities.\nMonth 2 – Architecture Design \u0026amp; Prototyping Design the overall project architecture and define data flow between components. Set up initial AWS resources such as S3, Lambda, API Gateway, EventBridge, and EC2. Experiment with open-source tools for visualization and reporting. Test sample codes and validate the data ingestion and processing pipeline.\nMonth 3 – Implementation \u0026amp; Testing Implement the full architecture based on the approved design. Integrate all AWS services and ensure system reliability. Conduct performance and functionality testing. Finalize documentation and prepare the project for presentation.\n6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator. Or you can download the Budget Estimation File.\nInfrastructure Costs AWS Services\nAmazon Cognito(User Pools): 0.10 USD/monthly(1 Number of monthly active users (MAU), 1 Number of monthly active users (MAU) who sign in through SAML or OIDC federation)\nAmazon S3\n3 Standard:0.17 USD/monthly(6 GB, 1,000 PUT requests, 1,000 GET requests, 6 GB Data returned, 6 GB Data scanned) Data Transfer: 0.00 USD/monthly(Outbound: 6 TB, Inbound: 6 TB) Amazon CloudFront(United States): 0.64 USD/monthly(6 GB Data transfer out to internet, 6 GB Data transfer out to origin, 10,000 requests Number of requests (HTTPS))\nAmazon API Gateway(HTTP APIs): 0.01 USD/monthly(10,000 requests for HTTP API requests units)\nAmazon Lambda(Service settings): 0.00 USD/monthly(1,000,000 requests, 512 MB)\nAmazon CloudWatch(APIs): 0.03 USD/monthly(100 metrics GetMetricData, 1,000 metrics GetMetricWidgetImage, 1,000 requests API)\nAmazon SNS(Service settings): 0.02 USD/monthly(1,000,000 requests, 100,000 calls HTTP/HTTPS Notifications, 1,000 calls EMAIL/EMAIL-JSON Notifications, 100,000,000 notifications QS Notifications, 100,000,000 deliveries Amazon Web Services Lambda, 100,000 notifications Amazon Kinesis Data Firehose)\nAmazon EC2(EC2 specifications): 1.68 USD/monthly(1 instances, 730 Compute Savings Plans)\nAmazon EventBridge: 0.00 USD/monthly(1,000,000 events(Number of AWS management events) EventBridge Event Bus - Ingestion)\nTotal: 2.65 USD/month, 31.8 USD/12 months\n7. Risk Assessment Risk Likelihood Impact Mitigation Strategy High costs exceeding the estimated budget Medium High Closely monitor and calculate all potential AWS expenses. Limit the use of high-cost AWS services and replace them with simpler, cost-effective alternatives that provide similar functionality. Potential issues during data transfer or service integration between AWS components Medium Medium Perform step-by-step validation before going live. Conduct early testing, use managed AWS services, and continuously monitor performance through Amazon CloudWatch. Data collection or processing risks (e.g., excessive user interactions, network instability, missing or duplicated events) High Medium Apply data validation, temporary buffering, and schema enforcement to ensure consistency. Use structured logging and alarms to detect and resolve ingestion errors. Low or no user adoption of the analytics dashboard Low High Conduct internal training sessions and leverage existing communication channels to raise awareness. Encourage adoption by showcasing the system’s practical benefits and actionable insights. 8. Expected Outcomes Understanding Customer Behavior and Journey The system records the entire customer journey — including which pages users visit, which products they view, how long they stay, and where they exit the site.\nBy analyzing session duration, bounce rate, and navigation paths, businesses can evaluate user engagement and the overall experience.\nThis provides a reliable data foundation for improving website interface, optimizing page layout, and enhancing overall customer satisfaction.\nIdentifying Popular Products and Consumer Trends Based on clickstream data collected and processed in AWS, the system identifies the most viewed and most purchased products.\nProducts that receive less attention are also tracked, enabling businesses to assess the effectiveness of product listings, adjust pricing or visuals, and plan inventory more effectively.\nFurthermore, the system supports discovering shopping trends across time periods, regions, or device types — allowing for data-driven and timely business decisions.\nOptimizing Marketing and Sales Strategies Customer behavior data is transformed into business insights and presented through R Shiny dashboards.\nWith these analytical results, businesses can:\nAccurately define target customer segments for marketing efforts Customize advertising and promotional campaigns for specific product groups or demographics Evaluate the effectiveness of marketing initiatives through measurable engagement and conversion indicators As a result, marketing and sales strategies become more evidence-based and precise, supporting better decision-making and improved business performance.\nFile TEMPLETE DOCX: DOWLOAD Proposal (DOCX) "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " How TCS\u0026rsquo;s Smart Power Plant Solution on AWS Helps Utilities Optimize Operations and Drive Energy Transition By Alakh Srivastava, Rajesh Natesan, Siva Thangavel, and Yogesh Chaturvedi – March 19, 2025, in the Amazon DocumentDB, Amazon ECS, Amazon S3, AWS IoT Core, AWS Step Functions, Energy (Oil \u0026amp; Gas), Industries section.\nSolution Overview Advanced digital technologies are revolutionizing the energy industry, enabling organizations to achieve sustainability goals while reducing costs and carbon emissions.\nAccording to McKinsey, digital transformation in the energy sector could unlock $1.6 trillion in value by 2035, helping reduce 20–30% of operational costs and 5% of carbon emissions.\nAs the industry moves towards a distributed power generation model integrated with renewable energy, businesses require smart solutions such as digital grids, AI-driven energy coordination, and real-time monitoring platforms.\nThe TCS Smart Power Plant solution was developed to address these needs — delivering a 0.5% increase in performance, an 8% reduction in NOx, and improving 8–10% accuracy in renewable power generation forecasts.\nBuilt on the AWS platform, the solution leverages the power of AI/ML to process real-time data from thousands of energy sensors across multiple locations.\nThis article explains how TCS and AWS collaborate to deliver superior operational efficiency and sustainable business outcomes for the energy industry.\nSolution Architecture and Data Flow The solution architecture is designed with a closed-loop data flow, utilizing AWS services to manage, process, and analyze information comprehensively.\nFigure 1. Overall architecture of the Smart Power Plant solution on AWS.\nData Ingestion: Data is collected from OPC-UA (industrial devices), on-premise historical systems, and Amazon S3 data lakes.\nEach unit can send up to 4,000 sensor values per minute. Ingestion and Orchestration: AWS IoT Core receives the data stream and triggers AWS Step Functions for automated orchestration. Data Processing: AWS Lambda functions perform data cleaning, calculate KPIs, and generate alerts. Storage: Amazon DocumentDB stores structured data (KPIs, alerts), Amazon S3 stores raw sensor data and training results. ML Model Training: Performed in Amazon SageMaker, with models stored in Amazon Elastic Container Registry (ECR). Real-Time Inference: Models are deployed through Amazon ECS for TCS InTwin (online analytics engine). Application Deployment: Front-end/back-end interfaces run containers on Amazon ECS, ensuring flexible scalability. Key Capabilities The TCS Smart Power Plant solution provides four core capabilities that transform how plants operate:\nFigure 2. Four core capabilities of the solution.\nSelf-learning Digital Twin (AI):\nCombines real data and physical AI models to continuously adapt to operational conditions, ensuring accurate predictions and cost savings.\nOpen and Scalable Solution:\nCan integrate with existing plant systems or proprietary AI models, with an open and explainable architecture.\nLow-code Digital Workbench:\nEnables rapid creation and management of AI models, supporting the creation of KPIs, FMEA, and specific use cases.\nPre-built Platform:\nConfigurable modules for each plant, reducing deployment time and enabling easy scaling.\nReal-world Use Cases Solar Power Generation Forecasting Using ML models and advanced analytics to predict renewable energy generation.\nAt an offshore wind farm in the UK, forecasting accuracy improved by 15.1%, leading to a 6% revenue increase.\nCombustion Optimization in Heat Generation At a Japanese plant, AI improved 0.5% performance, reduced 8% NOx, and saved $2.5 million/year.\nPredictive Maintenance of Gas Turbine Components At an Australian plant, models predicted failures 8–12 months in advance, reducing maintenance costs by 20% and downtime.\nBusiness Benefits TCS\u0026rsquo;s solution helps energy enterprises:\nBenefit Impact Reduce Operational Costs Cut maintenance and operational costs by up to 20%. Accurate Failure Prediction Achieve error prediction accuracy up to 85%. Optimize KPIs and Reduce Emissions Improve performance while reducing carbon emissions. Support Workforce AI assists decision-making, reducing dependence on individual experience. Additionally, AWS integration eliminates data silos, boosts productivity, and provides a standardized platform for future power plants.\nConclusion The TCS Smart Power Plant solution on AWS is shaping the sustainable future of the energy industry.\nThrough AI and advanced analytics, this platform helps optimize performance, predictive maintenance, and seamlessly integrate renewable energy.\nTCS — with deep expertise and a team of AWS-certified specialists — has demonstrated its ability to deploy successfully across various types of plants, from traditional thermal plants to large-scale renewable energy.\nTo learn more, please see the original TCS post on the Smart Power Plant solution on AWS.\nAbout the Authors Alakh Srivastava\nGlobal Product Director – Smart Power Plant Practice, TCS.\nOver 20 years of experience in digital transformation in the power industry, specializing in renewable energy, AI, and industrial IoT.\nRajesh Natesan\nChief Technical Group Leader – Smart Power Plant Group, TCS.\n20 years of experience in IoT, AI/ML, and large-scale energy system architecture.\nSiva Thangavel\nPartner Solution Architect at AWS.\nProvides optimal architectural solutions for partners and enterprise customers across multiple industries.\nYogesh Chaturvedi\nPrincipal Solution Architect at AWS – Energy and Utilities.\nFocuses on helping customers solve challenges using cloud technology. Outside work, he enjoys hiking, traveling, and sports.\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.3-week3/",
	"title": "Worklog Week 3",
	"tags": [],
	"description": "",
	"content": " Week 3 Objectives: Focus on Networking in AWS, especially VPC and Route 53. Set up a secure network and configure domain names in AWS. Learn about Security Groups, NACLs, and VPN. Practice labs for networking services and security configurations. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS VPC (Virtual Private Cloud) + Overview of VPC + Create VPC, Subnets, Route Tables 22/09/2025 22/09/2025 Study materials 3 - Learn about Amazon Route 53 + Manage domains with Route 53 + Configure and manage DNS records 23/09/2025 23/09/2025 Study materials 4 - Learn about Security Groups and NACLs (Network ACLs) + Configure Security Groups for EC2 + Configure NACLs for subnets 24/09/2025 24/09/2025 Study materials 5 - Learn about VPN in AWS + Concept of Site-to-Site VPN + Create and configure Site-to-Site VPN, traffic routing 25/09/2025 25/09/2025 Study materials 6 - Combined hands-on labs: + VPC, Route 53, VPN, Security Groups, and NACLs + Review security configurations and network connectivity between resources 26/09/2025 26/09/2025 Study materials Week 3 Achievements: Created and configured a VPC with subnets, route tables, and an internet gateway. Set up Route 53 to manage domains and basic DNS records (A, CNAME, …). Configured Security Groups and NACLs to control inbound/outbound traffic at both instance and subnet levels. Set up a Site-to-Site VPN to enable secure connectivity between a (simulated) on-premises network and the VPC on AWS. Completed hands-on labs on Networking and Security in AWS, strengthening understanding of network architecture and security layers in AWS. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - How iFood built a platform to run hundreds of machine learning models with Amazon SageMaker Inference This blog introduces how iFood uses Amazon SageMaker Inference to operate hundreds of machine learning models at scale. The post explains how iFood automates model training, deployment, and monitoring processes while optimizing cost and performance with features such as zero-scale endpoints and multi-model GPU serving.\nBlog 2 - How Salesforce Business Technology uses AWS Direct Connect SiteLink for reliable global connectivity This blog describes how Salesforce Business Technology implements AWS Direct Connect SiteLink to build a reliable global network architecture. The post shares how SiteLink helps Salesforce unify network connectivity across seven locations, reduce latency, enhance security, and simplify operations by leveraging the AWS global backbone.\nBlog 3 - How TCS Smart Power Plant on AWS helps utilities optimize operations and accelerate energy transformation This blog introduces how Tata Consultancy Services (TCS) deploys the Smart Power Plant on AWS, helping energy companies optimize performance, reduce emissions, and drive sustainable energy transformation. The solution uses AI/ML, IoT, and digital twin technologies to analyze real-time data, predict failures, and optimize power generation. The post also presents real-world use cases such as renewable energy forecasting, combustion optimization, and predictive maintenance, delivering economic efficiency and carbon reduction.\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.4-week4/",
	"title": "Worklog Week 4",
	"tags": [],
	"description": "",
	"content": " Week 4 Objectives Understand database services on AWS (RDS, DynamoDB). Learn how to deploy and manage database instances. Study backup, restore, scaling, and performance optimization for databases. Practice hands-on labs with RDS and DynamoDB. Tasks for this week Day Task Start Date End Date References 2 - Learn about AWS RDS + Create an RDS instance + Backup and maintenance for RDS 29/09/2025 29/09/2025 Learning Materials 3 - Learn about Amazon DynamoDB + Introduction to DynamoDB + Working with tables and data 30/09/2025 30/09/2025 Learning Materials 4 - Study backup and restore mechanisms in RDS 01/10/2025 01/10/2025 Learning Materials 5 - Learn about database performance optimization on AWS (RDS \u0026amp; DynamoDB) 02/10/2025 02/10/2025 Learning Materials 6 - Practice labs for RDS and DynamoDB + Create / update / delete databases, tables, and data + Test backup \u0026amp; restore 03/10/2025 03/10/2025 Learning Materials Week 4 Outcomes Deployed and configured RDS instances (create, modify, stop/delete). Got familiar with DynamoDB and practiced working with tables, items, and querying data. Understood automatic and manual backup mechanisms in RDS, as well as the data restore process. Learned basic principles for optimizing database performance on AWS (choosing instance types, storage, IOPS, indexes, etc.). Completed hands-on labs with RDS and DynamoDB, reinforcing knowledge of databases on AWS. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/4-eventparticipated/",
	"title": "Events Attended",
	"tags": [],
	"description": "",
	"content": "Event 1 Event name: Vietnam Cloud Day 2025\nDate: 18/09/2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole in the event: Attendee\nBrief description of main content and activities:\nVietnam Cloud Day is an annual event organized by AWS, bringing together experts, enterprises, and the tech community to share the latest trends in cloud computing, AI, big data, and security.\nThe program includes strategic discussion sessions, industry-specific tracks (FSI, telecommunications, retail, real estate, etc.), technical sessions on modern architectures (microservices, event-driven, serverless), and real-world case study presentations from Techcombank, Honda, Masterise, TymeX, etc., providing a comprehensive view of the modernization journey on AWS.\nOutcomes / value gained (lessons learned, new skills, contributions to team/project):\n- Gained a clearer understanding of the role of Cloud \u0026amp; GenAI in the national digital transformation strategy and in key industries.\n- Learned about modern architectural models such as DDD, microservices, event-driven, serverless, and how to choose appropriate compute services (EC2, ECS, Fargate, Lambda).\n- Developed deeper awareness of the importance of data strategy, “security by design”, and the “migrate to operate” mindset instead of just “lift-and-shift”.\n- Generated many ideas to apply to academic/personal projects (e.g., designing systems on AWS, integrating Amazon Q Developer into the DevOps workflow) and expanded my network with the cloud engineering community.\nEvent 2 Event name: GenAI-powered App-DB Modernization workshop\nDate: 07/11/2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole in the event: Attendee\nBrief description of main content and activities:\nThe workshop focused on Agentic AI and the Amazon QuickSuite ecosystem, clarifying the difference between traditional Generative AI and Agentic AI that is capable of autonomous action.\nParticipants were introduced for the first time to QuickSuite in Vietnam, including the combination of Amazon QuickSight and QuickSuite Q to build “Analyst Agents” that can read data, answer business questions, and recommend appropriate actions.\nThe program also introduced the AWS LIFT financial support package (credits of up to USD 80,000) to reduce cost barriers when experimenting with AI/Agentic AI solutions.\nA ~90-minute hands-on session allowed participants to work directly with QuickSight + QuickSuite Q under the guidance of AWS experts and Cloud Kinetics, and to network with the cloud, data, and AI community.\nOutcomes / value gained (lessons learned, new skills, contributions to team/project):\n- Gained a clearer understanding of Agentic AI, the shift from “AI that only answers” to “AI that actually works on behalf of humans”, and practical use cases in enterprises.\n- Learned the main components of Amazon QuickSuite, and how to combine QuickSight and QuickSuite Q to build data analysis agents (analyst agents).\n- Understood the role of the AWS LIFT program in financially supporting AI PoCs/R\u0026amp;D, enabling future proposals for related projects.\n- Improved skills in designing Agentic AI use cases (selecting repetitive, multi-step, data-driven processes) and generated more ideas to apply to study/projects, such as automated reporting, metric monitoring, or operational optimization recommendations.\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.5-week5/",
	"title": "Worklog Week 5",
	"tags": [],
	"description": "",
	"content": " Week 5 Objectives Learn about AWS CloudWatch to monitor and manage AWS services. Study AWS CloudTrail and AWS Config to track and manage changes in the AWS environment. Practice configuring and monitoring AWS resources. Tasks for this week Day Task Start Date End Date References 2 - Learn about AWS CloudWatch + Configure and monitor AWS resources with CloudWatch + Create Alarms and Dashboards 06/10/2025 06/10/2025 Learning Materials 3 - Learn about AWS CloudTrail + Monitor user actions and events on AWS + Analyze CloudTrail event logs 07/10/2025 07/10/2025 Learning Materials 4 - Learn about AWS Config + Track configuration changes and resource history on AWS 08/10/2025 08/10/2025 Learning Materials 5 - Hands-on with CloudWatch + Create CloudWatch Alarms + Monitor EC2 and other resources via metrics \u0026amp; logs 09/10/2025 09/10/2025 Learning Materials 6 - Hands-on with CloudTrail and AWS Config + Analyze CloudTrail events + Manage configuration changes using AWS Config 10/10/2025 10/10/2025 Learning Materials Week 5 Outcomes Successfully configured and monitored AWS resources using AWS CloudWatch (e.g., EC2, RDS, …). Created alarms and dashboards to track system performance and health. Understood how to use AWS CloudTrail to track user actions and events in the AWS account. Used AWS Config to track and manage configuration changes of AWS resources over time. Strengthened knowledge of monitoring, logging, and auditing in AWS, supporting safer and more stable system operations. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Batch-Based Clickstream Analytics Platform Figure: Architecture Batch-base Clickstream Analytics Platform.\nOverview This workshop implements a Batch-Based Clickstream Analytics Platform for an e-commerce website selling computer products.\nThe system collects clickstream events from the frontend, stores raw JSON data in Amazon S3, processes events via scheduled ETL (AWS Lambda + EventBridge), and loads analytical data into a dedicated PostgreSQL Data Warehouse on EC2 inside a private subnet.\nAnalytics dashboards are built using R Shiny, running on the same EC2 instance as the Data Warehouse, and accessed via AWS Systems Manager Session Manager.\nThe platform is engineered with:\nClear separation between OLTP vs Analytics workloads Private-only analytical backend (no public DW access) Cost-efficient, scalable AWS serverless components Zero-SSH admin access via SSM Session Manager into the private DW / Shiny EC2 You can run the Shiny app locally at localhost:3838 Key Architecture Components Frontend \u0026amp; OLTP Domain\nNext.js app: ClickSteam.NextJS hosted on AWS Amplify Hosting Amazon CloudFront as global CDN Amazon Cognito User Pool for authentication OLTP PostgreSQL on EC2: SBW_EC2_WebDB (public subnet) DB: clickstream_web (schema public) Port: 5432 Ingestion \u0026amp; Data Lake Domain\nAmazon API Gateway (HTTP API): clickstream-http-api Route: POST /clickstream Lambda Ingest: clickstream-lambda-ingest Validates payload, enriches metadata, writes JSON files to S3 S3 Raw Clickstream Bucket: clickstream-s3-ingest Prefix: events/YYYY/MM/DD/ File pattern: event-\u0026lt;uuid\u0026gt;.json RAW_BUCKET = clickstream-s3-ingest Analytics \u0026amp; Data Warehouse Domain\nPrivate EC2 for DWH + Shiny: SBW_EC2_ShinyDWH (private subnet 10.0.128.0/20)\nDWH DB: clickstream_dw Main table: clickstream_events with fields: event_id, event_timestamp, event_name user_id, user_login_state, identity_source, client_id, session_id, is_first_visit context_product_id, context_product_name, context_product_category, context_product_brand context_product_price, context_product_discount_price, context_product_url_path R Shiny Server on port 3838, web path /sbw_dashboard Lambda ETL: SBW_Lamda_ETL (VPC-enabled)\nReads raw JSON from clickstream-s3-ingest Transforms into SQL-ready rows Inserts into clickstream_dw.public.clickstream_events EventBridge Rule: SBW_ETL_HOURLY_RULE\nSchedule: rate(1 hour) VPC \u0026amp; Networking\nVPC CIDR: 10.0.0.0/16 Public subnet: 10.0.0.0/20 → SBW_Project-subnet-public1-ap-southeast-1a (OLTP EC2) Private subnet: 10.0.128.0/20 → SBW_Project-subnet-private1-ap-southeast-1a (DW, Shiny, ETL Lambda) S3 Gateway VPC Endpoint for private S3 access SSM Interface Endpoints (SSM, SSMMessages, EC2Messages) for Session Manager Admin Access (SSM)\nPort forwarding: localPort = 3838 portNumber = 3838 Shiny URL from local: http://localhost:3838/sbw_dashboard Content Map 5.1. Objectives \u0026amp; Scope 5.2. Architecture Walkthrough 5.3. Implementing Clickstream Ingestion 5.4. Building the Private Analytics Layer 5.5. Visualizing Analytics with Shiny Dashboards 5.6. Summary \u0026amp; Clean up "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.6-week6/",
	"title": "Worklog Week 6",
	"tags": [],
	"description": "",
	"content": " Week 6 Objectives: Learn about AWS Auto Scaling and Elastic Load Balancing (ELB) to ensure the application can scale and distribute traffic efficiently. Study AWS Elastic Container Service (ECS) and AWS Fargate. Practice deploying a containerized application on ECS and using Auto Scaling to automatically adjust resources. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Learn about AWS Auto Scaling + Configure Auto Scaling for EC2 + Set up scale-out and scale-in policies 13/10/2025 13/10/2025 Study materials 3 - Learn about Elastic Load Balancer (ELB) + Configure and use ELB to distribute traffic across EC2 instances / services 14/10/2025 14/10/2025 Study materials 4 - Learn about AWS ECS and AWS Fargate + Introduction to ECS architecture: Cluster, Task, Service + Introduction to Fargate and serverless container model 15/10/2025 15/10/2025 Study materials 5 - Practice deploying an application on ECS \u0026amp; Fargate + Deploy a simple containerized application on ECS + Use Fargate to run the container 16/10/2025 16/10/2025 Study materials 6 - Practice with Auto Scaling and ELB for the ECS application + Configure Auto Scaling for ECS Service + Integrate with ELB for traffic distribution 17/10/2025 17/10/2025 Study materials Week 6 Achievements: Configured AWS Auto Scaling and Elastic Load Balancing for applications running on EC2 / ECS. Became familiar with AWS ECS and AWS Fargate, and understood how Cluster – Task – Service are organized. Practiced deploying a containerized application on ECS, using Fargate to run containers without managing servers. Understood how to combine ELB + Auto Scaling + ECS to build a system that can automatically scale, distribute traffic, and operate stably. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/6-self-evaluation/",
	"title": "Self-Evaluation",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services (AWS) from 08/09/2025 to 05/12/2025, I had the opportunity to learn, practice, and apply the knowledge I acquired at university in a real working environment.\nI participated in the Batch-based Clickstream Analytics Platform project for an e-commerce website selling computer products, with the main responsibilities as follows:\nResearching and working with AWS services such as Amazon S3, AWS Lambda, Amazon EventBridge, Amazon EC2, AWS Amplify, Amazon Cognito to support the collection and processing of clickstream data. Supporting the construction of a batch-based pipeline for collecting, storing, and processing clickstream data, in which user behaviour data on the website is captured, written to S3, and processed periodically by Lambda/EventBridge before being loaded into the analytics system. Taking part in configuring the infrastructure on AWS, connecting to PostgreSQL/Supabase and integrating with the frontend application (Next.js) of the e-commerce website. Writing technical documentation and progress reports, and communicating regularly with my mentor and team to update status and align on implementation directions. Through this, I have clearly improved my skills in programming, system design, working with cloud services, building batch data pipelines, technical writing, teamwork, and communication in a professional environment.\nIn terms of work attitude, I always tried to complete the tasks assigned, followed the team’s working processes, and proactively communicated when facing difficulties to ensure the quality of my work.\nTo objectively reflect my internship, I would like to provide the following self-evaluation based on several criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge and skills Understanding of the field, ability to apply knowledge in practice, tool usage, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge, learn quickly ☐ ✅ ☐ 3 Proactiveness Willingness to research and take on tasks without waiting for detailed instructions ☐ ✅ ☐ 4 Sense of responsibility Completing work on time and ensuring quality ✅ ☐ ☐ 5 Discipline Compliance with working hours, rules, and processes ☐ ✅ ☐ 6 Desire for self-improvement Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in team activities ✅ ☐ ☐ 9 Professional conduct Showing respect toward colleagues, partners, and the workplace ✅ ☐ ☐ 10 Problem-solving mindset Identifying problems, proposing solutions, and being creative ☐ ✅ ☐ 11 Contribution to project/organization Work effectiveness, improvement initiatives, and recognition from the team ☐ ✅ ☐ 12 Overall performance Overall evaluation of the entire internship period ☐ ✅ ☐ Areas for Improvement Strengthen my discipline and time management skills, strictly following rules and deadlines in any working environment. Further develop my problem-solving mindset, especially in analyzing root causes and comparing multiple options before choosing a solution. Improve my communication skills: present more concisely and clearly in meetings, and proactively share difficulties/questions with my mentor and team members. Overall, I consider this internship at AWS a very meaningful experience that helped me grow both in technical capability and work attitude, and also made me more aware of the areas I need to continue improving in the future.\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "General Evaluation 1. Working environment\nThe working environment at AWS is very professional yet still friendly and open. Everyone is always willing to support me when I face difficulties and explain things when I don’t fully understand, so I feel quite comfortable asking questions and discussing issues. The atmosphere is serious but not too stressful, which helps me stay focused and feel more confident when participating in real projects.\n2. Support from mentor / team admin\nThe mentors in the team are very supportive, from explaining AWS-related knowledge to guiding how to work in a professional technical team. When I run into problems, they don’t just give me the answers, but also show me how to think about the problem so that I can find solutions on my own. Thanks to that, I’ve become more proactive and learned a lot of practical experience.\n3. Alignment between work and my major\nI am studying Software Engineering, and my internship work is joining the clickstream project with the team and integrating AWS services into the system, so I find it very aligned with my major. The work allows me to apply my fundamental knowledge in programming, systems, and software architecture, and at the same time gives me the chance to work with a real cloud environment. This helps me both reinforce what I learned at school and better understand which career direction I want to pursue in the future.\n4. Learning opportunities \u0026amp; skill development\nDuring the internship, I’ve had many opportunities to improve both my technical knowledge and how to participate in a real project. Besides that, I’ve also developed my teamwork skills and communication skills in a professional environment, and learned how to receive and give feedback in a more positive and constructive way.\n5. Culture \u0026amp; team spirit\nThe working culture at AWS is very positive: people respect each other and are willing to share and support whenever needed. Within the team, everyone collaborates smoothly, discusses together when there are technical issues, and works towards the common goals of the project. This makes me, even as an intern, feel trusted and truly part of the team.\n6. Policies / benefits for interns\nIn addition to the real working environment and practical tasks, I also had the opportunity to join around 4–5 events organized by AWS, where they shared more about technology, knowledge, and topics related to the industry. These activities are very valuable, helping me broaden my perspective, stay updated with new trends, and connect with many professionals in the field. This makes the internship experience even richer and more meaningful.\nOther questions What are you most satisfied with during your internship?\nI am most satisfied with the friendly working environment, the enthusiastic support from mentors and team members, and the fact that I was assigned real tasks that helped me both learn and work, making me more confident in what I do. What do you think the company should improve for future interns?\nFor me, the internship experience has been very good so far, with clear workflows and support whenever I need it. If possible, the company could organize more sharing sessions, experience-sharing or small team bonding activities so that interns and different teams have more chances to connect and exchange with one another. If you were to recommend this place to your friends, would you encourage them to intern here? Why?\nYes. I would definitely encourage my friends to intern here because the environment is friendly, people are willing to guide and support, mentors are dedicated, and the work allows us to gain real practical experience instead of just going through the motions. This is very helpful for career orientation in the future. Suggestions \u0026amp; expectations Do you have any suggestions to improve the internship experience?\nAt the moment, I feel that the internship experience is already very good. The company has organized 3–4 knowledge-sharing sessions, which have helped me learn a lot. If possible, I just hope the company will continue to maintain these sessions regularly so that everyone has more opportunities to connect and learn from each other. Do you want to continue with this program in the future?\nYes. If I have the chance, I would really like to keep joining other projects or programs at the company to continue learning, improving my skills, and building a longer-term connection. Other comments (free sharing):\nI don’t have any additional comments; I mainly want to say thank you to the company and all the team members for always supporting, encouraging, and creating good conditions for me throughout the internship. This has been a meaningful experience that helped me grow, both in terms of technical skills and working attitude. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.7-week7/",
	"title": "Worklog Week 7",
	"tags": [],
	"description": "",
	"content": "Week 7 Objectives Understand the overall scope of the Batch-based Clickstream Analytics \u0026amp; Sessionization project. Identify the project objectives, problem statement, and approach. Explore AWS services suitable for the clickstream analytics architecture. Draft the system architecture and perform a rough cost estimation. Tasks for this week Day Task Start Date End Date References Mon - Online kick-off meeting with FCJ team members. - Present an overview of the project: sessionization \u0026amp; clickstream for an e-commerce website. - Align on scope, objectives and final deliverables (web application + analytics pipeline). 20/10/2025 20/10/2025 Tue - Study the theory of clickstream and sessionization: concepts, importance and common metrics. - Review several reference architectures for clickstream analytics on AWS. - Note down functional and non-functional requirements related to data, security and performance. 21/10/2025 21/10/2025 Wed - Online team discussion to list AWS services that can be used in the project: Amplify, CloudFront, S3, API Gateway, Lambda, EventBridge, EC2, RDS/EC2 DW, Cognito, IAM, CloudWatch, SNS, … - Analyze the role of each service in the architecture (frontend, ingest, storage, ETL, analytics, monitoring). 22/10/2025 22/10/2025 Thu - Start designing the overall architecture: + Draw the main data flows through the system. - Conduct an online review meeting to get initial feedback on the architecture from all team members. 23/10/2025 23/10/2025 Fri - Finalize Architecture Diagram – Version 1 for the project based on team feedback. 24/10/2025 24/10/2025 Outcomes of Week 7 The whole team has a clear understanding of the project:\nThe problem of collecting and analyzing clickstream data for an e-commerce website selling laptops \u0026amp; electronic devices. The need for sessionization to group user behaviors into meaningful sessions. Clearly defined objectives and problems to solve:\nBuild a batch-based clickstream pipeline from frontend → storage → ETL → analytics. Leverage managed AWS services to reduce operational overhead and improve scalability. Completed Architecture Diagram – Version 1 for the entire system and reached consensus within the team through multiple online discussions.\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.8-week8/",
	"title": "Worklog Week 8",
	"tags": [],
	"description": "",
	"content": "Week 8 Objectives Have the first offline meeting, agree on how the team will work together in person. Finalize the proposal on Git/GitPages, configure its display so it’s easy to access and read. Start learning Amazon Cognito, S3, ORM and begin building the e-commerce website from an existing template. Prepare for the next phase: creating sample data, studying CloudFront, Amplify, LocalStack, and standardizing the working environment. Tasks for this week Day Task Start Date End Date References 2 - Offline meeting: first in-person meetup with the team - Review the proposal, fix minor content issues - Push the proposal to Git/GitPages - Split the work items for the next phase of the project 28/10/2025 28/10/2025 3 - Online meeting: study Amazon Cognito \u0026amp; S3 together - Start building the e-commerce website using the existing frontend template from Git - Decide not to use a separate backend and focus on the clickstream architecture on AWS 29/10/2025 30/10/2025 4 - Self-study ORM in the frontend template: + How to map data and define models + How to run migrations - Take notes for later when connecting to a real database (for Data Warehouse / OLTP parts) 30/10/2025 31/10/2025 5 - Team meeting: confirm that the first version of the web app can run locally - Discuss next steps: creating mock data, researching CloudFront, Amplify, LocalStack - Agree on standardizing the Python environment \u0026amp; libraries for ETL/analytics 05/11/2025 05/11/2025 6 - Individual work: the first build of the web still has many issues - Collect feedback from the team about UI problems and fix the web UI accordingly 06/11/2025 07/11/2025 Week 8 Outcomes The team had the first offline meeting, which made teamwork more comfortable and helped discussions become quicker and more effective. The project proposal has been: Reviewed and refined with some minor content fixes. Uploaded to Git/GitPages, making it easier for everyone to access and refer to while working. E-commerce website: The first local build of the site was completed using the existing template, although there are still UI issues. The team started to understand the code structure and how the ORM works in the frontend project. The whole team has begun to get familiar with and discuss more about: Cognito, S3, CloudFront, Amplify, LocalStack and how they will be used in the next parts of the architecture. This week lays the foundation so that in the following weeks the team can focus on integrating AWS services into the web app and building a complete clickstream pipeline. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.9-week9/",
	"title": "Worklog Week 9",
	"tags": [],
	"description": "",
	"content": "Week 9 Objectives Review the approach for CloudFront, Amplify and agree on how to use LocalStack. Adjust the system architecture to match the new design (Amplify, Supabase/PostgreSQL, LocalStack, …). Continue improving the web UI and fixing remaining issues. Start getting familiar with Docker to containerize the development environment. Prepare the report template (docx) and plan how to publish content on Hugo. Tasks for this week Day Task Start Date End Date References 2 - Individual: fix the web UI based on team feedback (layout, colors, display issues) - Continue learning about S3, Cognito, CloudFront, Amplify, LocalStack to better understand the role of each service in the architecture - Re-test the web after UI changes to ensure no new bugs are introduced 03/11/2025 06/11/2025 3 - Team meeting: review the approach of using CloudFront \u0026amp; Amplify in the architecture - Agree to use LocalStack Base to emulate multiple AWS services (especially Cognito) - Update the architecture flow to reflect using Supabase/PostgreSQL as the web database 09/11/2025 10/11/2025 4 - Meeting: explore how to use Docker (image, container, installing libraries into the image) - Propose the idea of using Docker + LocalStack as the standard development environment for the whole team 11/11/2025 11/11/2025 5 - Team meeting: update and finalize architecture diagram v9 - Individual: complete the docx template (worklog, workshop, architecture description sections) - Break down the content, assign owners, and set clear deadlines for each part 12/11/2025 13/11/2025 Week 9 Outcomes The web UI has been reviewed and improved:\nFixed several display, alignment, and color issues based on team feedback. The web app runs more stably on local, ready for further AWS service integration. The team understands the roles of AWS services in the architecture more clearly:\nContinued to study and align on how to use S3, Cognito, CloudFront, Amplify, LocalStack. Decided to use LocalStack Base to emulate multiple AWS services (especially Cognito) in the dev environment. Confirmed Supabase/PostgreSQL as the database solution for the web app. The system architecture has been updated to version 9:\nData flows were adjusted to better match Amplify, Supabase, and LocalStack. The list of core services in the model has been finalized, giving the team a clear “frame” to implement. First steps in working with Docker:\nUnderstood the concepts of image, container, and how to install libraries into an image. Laid the groundwork for using Docker + LocalStack as a standard development environment for the team. The report template (docx) now has an initial draft and task breakdown:\nA structure has been defined for the worklog, workshop, and architecture description sections. Content has been split into smaller parts with assigned owners and clear deadlines, making future reporting easier and more consistent, and smoother to publish on Hugo. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.10-week10/",
	"title": "Worklog Week 10",
	"tags": [],
	"description": "",
	"content": "Week 10 Objectives: Review the report template (docx) and check the content structure. Run and re-examine the e-commerce web project, verify its stability. Get familiar with and practice using Docker and LocalStack. Deploy the web app to Vercel, fix build errors, and prepare for the migration from Clerk to Cognito. Complete preparation steps for the environment LocalStack Pro + Terraform + Amplify + Cognito + PostgreSQL on EC2 + S3 according to the proposed architecture. Tasks to be carried out this week: Day Tasks Start date Completion date References 2 - Review the docx template (layout, table of contents, worklog/workshop sections) - Rerun and study the structure of the e-commerce web project - Check whether the e-commerce site is stable (page loading, navigation, basic interactions) - Start getting familiar with LocalStack - Activate GitHub Student - Learn how to use Docker + LocalStack at a basic level 21/11/2025 21/11/2025 3 - Test the e-commerce website: + Check basic features (view details, add to cart, proceed to checkout,…) - Try using Clerk for login and fetching user id to the database, but this does not meet the requirements → agree to switch to Cognito - Deploy the web app to Vercel: fix multiple build errors (next.config.ts, package.json, ESLint, env) until deployment succeeds, and ensure user id is saved to the database (user pool) 23/11/2025 24/11/2025 4 - Online meeting: learn and practice LocalStack Pro + Configure API Key for LocalStack Pro + Run the LocalStack docker image + Apply Terraform to LocalStack according to the architecture - Terraform: adjust TF files to match the diagram, enable CloudFront in Amplify - Amplify: research how to bring the NextJS project into Amplify - Cognito: experiment with implementing Cognito in the Amplify local environment - Web data: prepare a plan to deploy PostgreSQL on EC2 (subnet 1) and upload sample image files to S3 24/11/2025 24/11/2025 Week 10 Achievements: Report template (docx):\nReviewed layout and main content sections (worklog, workshop, project description). Helps the team have a clearer document framework to use for the following weeks. E-commerce web project:\nReran and checked stability; core features work correctly (view products, add to cart, proceed to checkout,…). After trying Clerk and realizing it was not suitable, the team agreed to switch to Cognito to align with the AWS architecture. Deploying the web app to Vercel:\nEncountered many build errors (next.config.ts, package.json, duplicated config files, ESLint, env issues) but resolved them step by step. After 4 failed attempts, the 5th deployment was successful; the site runs stably on Vercel and can save user id to the database via the user pool. Docker \u0026amp; LocalStack Pro:\nTeam members became familiar with Docker (images, containers, how to install libraries into images). Successfully configured LocalStack Pro: set API key, ran the docker image, and applied Terraform to LocalStack. This is an important step to standardize the dev environment for the whole team. Infrastructure according to the target architecture:\nTerraform files were adjusted to match the architecture diagram and CloudFront in Amplify was enabled. Started researching how to bring NextJS into Amplify and how to implement Cognito in the Amplify local environment. Prepared a plan for the web data layer: PostgreSQL on EC2 (subnet 1) and uploading image files to S3, as a foundation for the upcoming implementation weeks. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.11-week11/",
	"title": "Worklog Week 11",
	"tags": [],
	"description": "",
	"content": "Week 11 Objectives: Deploy the e-commerce web application to AWS Amplify in the real AWS environment. Integrate Amazon Cognito into the web app to manage logins/users. Set up S3 for image storage and configure the web app to use images from S3. Migrate data from Supabase to EC2 and use EC2 as the main data source. Complete the creation and configuration of the core AWS cloud services according to the planned architecture. Tasks to be carried out this week: Day Tasks Start date Completion date References 2 - Online meeting: deploy the web application to AWS Amplify in the real AWS environment - Integrate Cognito into the web app - Start creating the required AWS Cloud services according to the architecture (Amplify app, Cognito user pool, S3 bucket, EC2, …) 01/12/2025 03/12/2025 3 - Complete Cognito integration into the web app and verify that login works - Successfully deploy the web app to Amplify (stable build \u0026amp; hosting) - Create an S3 bucket, upload product images, and configure the web app to load images directly from S3 03/12/2025 04/12/2025 4 - Migrate data from Supabase to the database on EC2 (import/export, migrate schema \u0026amp; data) - Reconfigure the web application to use EC2 as the main data source instead of Supabase - Quickly test core features after switching the data source 04/12/2025 04/12/2025 Week 11 Achievements: E-commerce web application:\nSuccessfully deployed to AWS Amplify in the real AWS environment. Build \u0026amp; hosting are stable; the app can be accessed and tested directly on Amplify. Cognito:\nIntegrated into the web app to support user login/management in line with the AWS architecture. The basic login flow works and is ready for future enhancements (authorization, security, …). S3 \u0026amp; media:\nCreated an S3 bucket and uploaded product images to S3. Reconfigured the web app to use images from S3, separating media from the source code. Data layer (Supabase → EC2):\nData has been migrated from Supabase to EC2. The web application has been reconfigured to use the EC2-hosted database as the main data source. Quick checks of core features after switching data show that the system still operates correctly. Core AWS cloud services:\nCompleted the initial creation and configuration of key services: Amplify, Cognito, S3, EC2, … according to the architectural direction. Established a solid foundation so that the following weeks can focus on clickstream, ETL, and analytics. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/1-worklog/1.12-week12/",
	"title": "Worklog Week 12",
	"tags": [],
	"description": "",
	"content": "Week 12 Objectives Complete the project and re-test the whole system. Ensure the processing flow runs end-to-end stably (web → data → analytics). Finalize and consolidate Worklogs from previous weeks. Support teammates in reviewing Worklogs and editing presentation slides for the final report. Tasks for this week Day Task Start Date End Date References 2 - Meeting: finish the remaining parts of the project + Re-check all main features on the web app + Re-test the data processing flow and analysis charts - Support teammates: review and refine Worklogs and presentation slides (structure, content, key points) 07/12/2025 08/12/2025 3 - Start writing the Worklog: summarize key activities for each week - Note down important milestones, architecture changes, and AWS services used 08/12/2025 09/12/2025 4 - Finalize the personal Worklog 09/12/2025 09/12/2025 Week 12 Outcomes The project has been completed and re-tested:\nMain features on the web app work stably after testing. The data processing flow has been tested end-to-end, ensuring the system is ready for demo/reporting. Personal Worklog:\nThe Worklog has been consolidated and completed, clearly recording tasks, timelines, and outcomes. Highlight points (architecture changes, AWS services implementation, bug fixing, dashboard completion) have been noted for later use in reporting and self-evaluation. Team support:\nHelped teammates review and adjust their Worklogs, making the content more consistent across members. Gave feedback and refined presentation slides, ensuring the project story is coherent from architecture → implementation → testing → lessons learned. Week 12 marks the completion of the project; all major parts are done and the team is ready for the final presentation/evaluation.\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/5-workshop/5.1-objectives--scope/",
	"title": "Objectives &amp; Scope",
	"tags": [],
	"description": "",
	"content": "Business Context The target system is an e-commerce website selling laptops, monitors, and accessories.\nThe business wants to:\nUnderstand how users interact with the website: Which pages they visit Which products they view Add-to-cart and checkout events Measure the conversion funnel from product view → add to cart → checkout Identify: Top products (best-selling / most viewed) Peak activity windows (by hour, by day) Use these insights to design the most effective marketing strategies to: Increase revenue Optimize and reduce costs To achieve this, the platform:\nCollects a dataset of user behavior via clickstream events, then produces statistics and analytics. Learning Objectives Understand the Architecture Be able to explain the overall architecture of a batch-based clickstream analytics platform using: Amplify, CloudFront, Cognito, EC2 OLTP in the user-facing domain API Gateway, Lambda Ingest, S3 Raw bucket in the ingestion \u0026amp; data lake domain ETL Lambda, PostgreSQL DW on EC2, R Shiny in the analytics \u0026amp; DW domain Be able to explain why OLTP and Analytics are separated: Logical: different schemas, different types of workloads Physical: public subnet vs private subnet, two different EC2 instances Hands-on Skills Send clickstream events from the frontend to API Gateway → Lambda Ingest → S3 Raw (clickstream-s3-ingest). Configure a Gateway VPC Endpoint for S3 and adjust the private route table so private components can access S3. Configure and test an ETL Lambda (SBW_Lamda_ETL) that can: Read raw JSON files from s3://clickstream-s3-ingest/events/YYYY/MM/DD/ Transform events into rows for the table clickstream_dw.public.clickstream_events Connect to the DW (SBW_EC2_ShinyDWH) and run sample SQL queries: Count the number of events Top products Basic funnel Access R Shiny dashboards via SSM port forwarding and interpret: Funnel charts Product engagement charts Time-series charts of activity over time Security \u0026amp; Cost Awareness Understand the importance of protecting user behavior data by placing EC2_ShinyDWH and Lambda_ETL in private subnets: Only allow Lambda_ETL to access S3 through a Gateway VPC Endpoint Only allow admins to access EC2_ShinyDWH through SSM using Interface VPC Endpoints Recognize the main security controls: Separation of public and private subnets Security groups between sg_oltp_webDB, sg_Lambda_ETL, sg_analytics_ShinyDWH Least-privilege IAM permissions for each Lambda Zero-SSH administration using AWS Systems Manager Session Manager (no bastion host, no open SSH port). Workshop Scope The workshop focuses on three main capability areas:\nImplementing the clickstream ingestion layer\nCapture browser interactions in the Next.js frontend Send JSON events to API Gateway (clickstream-http-api) Store raw events over time in clickstream-s3-ingest Building the private analytics layer\nCreate the VPC, subnets, route tables, and VPC endpoints Run SBW_Lamda_ETL inside the VPC Connect the ETL Lambda to the private EC2 Data Warehouse (SBW_EC2_ShinyDWH) Visualizing analytics with Shiny dashboards\nQuery clickstream_dw from R Shiny Display funnels, product performance, and time trends Access Shiny through SSM Session Manager port forwarding Out of Scope To keep the workshop focused and manageable, we do not go deep into:\nReal-time streaming (Kinesis, Kafka, MSK, …) Advanced DW services (Amazon Redshift / Redshift Serverless) Recommendation, segmentation, or anomaly detection using ML Production-grade CI/CD, blue/green deployments, multi-account setups Advanced SQL tuning or detailed index design These are natural follow-up directions once the batch-based clickstream foundation in this workshop is in place.\n"
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/5-workshop/5.2-architecture-walkthrough/",
	"title": "Architecture Walkthrough",
	"tags": [],
	"description": "",
	"content": "\nFigure: Architecture Batch-base Clickstream Analytics Platform.\n5.2.1 User \u0026amp; Frontend Domain (1) User Browser – End User Responsibilities\nAccess the website: browse the catalogue, view product detail pages, cart, checkout, etc. Log in / register an account. Generate clickstream events such as: page_view, product_view, add_to_cart, purchase, … Role in the pipeline\nIs the source of all behaviors that the analytics system analyzes. JavaScript on the frontend will collect these events and send them to the backend through the API (6). (2) Amazon CloudFront (CDN / Edge) Responsibilities\nActs as the CDN layer that delivers website content: Caches HTML/CSS/JS/images to reduce latency. Offloads traffic from the origin (Amplify, S3 assets). Can serve as a single entry point for web traffic: Route/forward requests: Dynamic web content → Amplify. Static images/media → S3 (3). Why it is important\nAn e-commerce website needs good page load speed. CloudFront helps improve user experience across different geographic regions. (3) Amazon S3 – Media Assets Responsibilities\nStore product images and other static assets for the website. Act as the origin when CloudFront (2) serves images/assets. Notes\nThis bucket (for example, clickstream-s3-sbw) is separate from the RAW clickstream bucket (8) in order to: Clearly separate permissions. Make management and cleanup easier. (4) Amazon Amplify – Front-end Hosting Responsibilities\nBuild \u0026amp; deploy the Next.js application (for example, ClickSteam.NextJS). Host the frontend (SSR/ISR, API routes). Manage the deployment pipeline: build, artifact, domain mapping. Integration with other services\nConnects to Cognito (5) for handling authentication. Sends clickstream events from the frontend to API Gateway (6). Connects to EC2 OLTP (20) via Prisma to read/write transactional data. (5) Amazon Cognito – Authentication / Authorization Responsibilities\nManage user identity: Sign up / sign in / reset password. Store the user pool and issue JWT tokens (ID token, access token). Issue tokens for the frontend so that it can call backend APIs with the correct permissions. Role in clickstream\nAllows determining: user_login_state (logged-in / guest). identity_source (Cognito, social login, etc.). This information is attached to events and stored in S3/DWH to analyze user behavior. 5.2.2 Ingestion \u0026amp; Data Lake Domain (6) Amazon API Gateway – Clickstream HTTP API Responsibilities\nProvide a public HTTP endpoint for the frontend: POST /clickstream. Perform basic validation: Method/path, throttling. Optional: integrate a Cognito authorizer. Processing flow\nReceive JSON requests from the browser. Forward the payload to Lambda Clickstream Ingest (7). (7) AWS Lambda – Clickstream Ingest Main responsibilities\nReceive events from API Gateway (6).\nAct as the ingestion layer:\nValidate schema / event type. Perform minimal enrichment: Generate event_id. Normalize event_timestamp. Attach client_id, session_id, user_login_state, identity_source if needed. Write raw events (raw JSON) to the S3 Clickstream Raw bucket (8) using time-based prefixes/partitions.\nWrite logs to CloudWatch (17) for debugging.\n(8) Amazon S3 – Clickstream Raw Data Responsibilities\nStore all raw clickstream data written by Lambda Ingest (7).\nAct as a “mini data lake” for batch ETL:\nData is organized by time-based folders/prefixes: events/YYYY/MM/DD/. ETL (11) reads the data in batches (hourly, daily, etc.). Architectural role\nServes as the source of truth for clickstream data: You can reprocess or rebuild the Data Warehouse if needed. (9) Amazon EventBridge – ETL Trigger / Cron Job Responsibilities\nRun batch schedules, for example: rate(1 hour). On each schedule: Trigger Lambda ETL (11). Why separating it out\nThe ETL “clock” lives in EventBridge: Easy to adjust frequency (1h, 30’, 5’, etc.). Easy to disable/enable. Separates “when to run” from the ETL logic itself. 5.2.3 VPC \u0026amp; Private Analytics Domain (19) VPC + Subnets + Internet Gateway Amazon VPC\nIsolate the network and control routing / security for the entire platform. Public subnet – OLTP\nContains EC2 OLTP (20). Has a route 0.0.0.0/0 → Internet Gateway. Allows: Amplify/Internet to access PostgreSQL OLTP based on security groups. Private subnet – Analytics\nContains: EC2 Shiny + DWH (12). Lambda ETL (11). Has no public IP. Can only go out through VPC Endpoints (10, 15). Internet Gateway\nProvides Internet access for resources in the public subnet. The private subnet does not route to the IGW (unless you configure it differently). (10) Gateway Endpoint – S3 Gateway Endpoint Responsibilities\nAllow resources inside the VPC (Lambda ETL, private EC2) to access S3 (8) without needing NAT/Internet. Traffic to S3 goes over the AWS private network, which is more secure and cheaper than a NAT Gateway. Role\nA key piece to: Keep the decision of “no NAT Gateway”. Still allow ETL / DW to read and write S3. (11) AWS Lambda – ETL Responsibilities\nTriggered by EventBridge (9) on a schedule.\nRuns inside the private subnet of the VPC.\nPerforms batch ETL:\nRead raw events from S3 (8) via the Gateway Endpoint (10). Transform / clean / flatten data according to the Data Warehouse schema (for example, the 15 fields you finalized). Load into PostgreSQL DWH on private EC2 (12): Insert / upsert, possibly partitioned by day/hour. Write logs and metrics to CloudWatch (17); on errors it can send notifications via SNS (18).\n(12) Amazon EC2 – Private (Shiny + DWH) Responsibilities\nServer for Data Warehouse (PostgreSQL) + Shiny Server. Receive cleaned data from Lambda ETL (11) and store it in DWH tables. Provide data to Shiny dashboards: Shiny queries the local/private Postgres instance. Characteristics\nRuns in a private subnet. Has no public IP. Administration access is via SSM Session Manager (14 + 15). (13) R Shiny Server (on Private EC2) Responsibilities\nHost analytics dashboards:\nOverall KPIs. Conversion funnel. Top products. User behavior, session analysis, etc. Connect directly to PostgreSQL DWH on the same EC2 instance.\nAccess\nListens on port 3838.\nTypically accessed via:\nInternal VPN, or SSM port forwarding (14). (14) AWS Systems Manager – Session Manager Responsibilities\nAllow admins to access private EC2 instances without SSH, without public IPs.\nSupport:\nOpening a terminal into EC2 (run command). Port forwarding (for example, forward localhost:3838 → Shiny on EC2). Better auditing of access sessions. Example\nYou can access Shiny from your local machine at:\nhttp://localhost:3838/sbw_dashboard\nafter successfully configuring the session port forwarding. (15) Interface Endpoint – SSM VPC Endpoints Responsibilities\nCreate private connections from EC2/Lambda in the VPC to SSM/Session Manager services:\nssm, ssmmessages, ec2messages, … Ensure that:\nPrivate EC2 instances can still use SSM. Traffic does not have to go through the Internet/NAT. 5.2.4 Cross-cutting Services (16) Amazon IAM Responsibilities\nManage roles/policies for the whole system:\nPermissions for API Gateway, Lambda. Lambda Ingest writes to S3 (8). Lambda ETL reads S3 (8) + connects to DB (12). EC2 role for SSM + CloudWatch agent (if any). Principles\nLeast privilege. Clearly separate permissions by function and by service. (17) Amazon CloudWatch Responsibilities\nCollect logs \u0026amp; metrics from:\nLambda Ingest (7). Lambda ETL (11). EventBridge rule (9). EC2 if the agent is installed. Define alarms: ETL failures, ingest failures, number of retries, duration, etc.\nRole\nThe main place for debugging when the pipeline has problems. (18) Amazon SNS Responsibilities\nActs as the notification channel when there are incidents:\nETL failure. Ingest failure. CloudWatch alarm triggered. Send email / SMS / webhook depending on configuration.\n(20) Amazon EC2 – Public (OLTP) Responsibilities\nRun PostgreSQL OLTP for the e-commerce web system:\nOrder transactions. Product information, inventory. User information. Relation to DWH\nSeparated from the Data Warehouse (12):\nOLTP is optimized for transactions. DWH is optimized for analytics and batch loads. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/5-workshop/5.3-implementing-clickstream-ingestion/",
	"title": "Implementing Clickstream Ingestion",
	"tags": [],
	"description": "",
	"content": "5.3.1 Ingestion Flow Overview High-level data flow:\nUser interacts with the Next.js frontend (ClickSteam.NextJS) hosted on Amplify. Frontend JavaScript bundles clickstream metadata (page/user/session/product) into a JSON payload. Browser sends POST requests to clickstream-http-api at POST /clickstream. API Gateway forwards the request to clickstream-lambda-ingest. Lambda Ingest enriches _ingest metadata and writes one JSON file per event to: s3://clickstream-s3-ingest/events/YYYY/MM/DD/HH/event-\u0026lt;uuid\u0026gt;.json (UTC hour partition) Design stays stateless and append-only, suitable for downstream batch ETL and idempotent inserts.\n5.3.2 S3 Bucket Design Two buckets are relevant:\nAsset Bucket — clickstream-s3-sbw\nStores website assets: product images, static files. Not used for clickstream events. RAW Clickstream Bucket — clickstream-s3-ingest\nStores only raw clickstream JSON events. Partitioned by UTC hour: events/YYYY/MM/DD/HH/ File naming: event-\u0026lt;uuid\u0026gt;.json Hour partitions make batch ETL easier (e.g., process previous hour or a specific day/hour prefix).\n5.3.3 Lambda Ingest Design — clickstream-lambda-ingest Responsibilities The clickstream-lambda-ingest function:\nParses incoming JSON payloads from API Gateway. Adds _ingest metadata only: receivedAt, sourceIp, userAgent, method, path, requestId, apiId, stage, traceId. Writes the event body as provided by the client (no server-side filling of user/session/product fields) to S3. IAM Permissions The execution role should allow:\ns3:PutObject on: arn:aws:s3:::clickstream-s3-ingest/events/* CloudWatch Logs APIs to write logs. No read permissions are needed for this function.\n5.3.4 API Gateway HTTP API — clickstream-http-api The HTTP API provides a public HTTPS endpoint for ingestion:\nRoute: POST /clickstream → Lambda clickstream-lambda-ingest Recommended options:\nEnable CORS so the Amplify frontend can call it from its own domain. Enable access logs to a CloudWatch log group for debugging. (Optional) Attach an API key or Cognito authorizer if you want to restrict ingestion. 5.3.5 Frontend Clickstream Publisher (Logic) Identity \u0026amp; idempotency Generates eventId per event (UUID) for idempotent inserts. Maintains clientId in localStorage (sticky per browser). Maintains sessionId in sessionStorage (30m idle timeout) and isFirstVisit. User, page, and click metadata User/auth (if available): userId, userLoginState, optional identity_source. Page/click: pageUrl, referrer, element metadata for clicks (tag/id/role/text/dataset). Product context Sent as product.{id,name,category,brand,price,discountPrice,urlPath}; ETL maps to DW context_product_* columns. Event coverage Auto: page_view, global click. Custom/product: home_view, category_view, product_view, add_to_cart_click, remove_from_cart_click, wishlist_toggle, share_click, login_open, login_success, logout, checkout_start, checkout_complete. Domain event wiring home_view: home page load (tracker component). category_view: category listing render (slug/params). product_view: product detail render (has product context). add_to_cart_click / remove_from_cart_click: cart add/remove handlers. wishlist_toggle: wishlist button handler. share_click: share button handler. login_open / login_success / logout: auth flows. checkout_start / checkout_complete: checkout entry and success flows. Components \u0026amp; wiring lib/clickstreamClient.ts: identity/session handling, base builders, console logging, fire-and-forget fetch to NEXT_PUBLIC_CLICKSTREAM_ENDPOINT (required env). lib/clickstreamEvents.ts: domain helpers that wrap trackCustom and build product/cart/order context. contexts/ClickstreamProvider.tsx + app/layout.tsx: wire global provider, auto page_view, global click listener. Tracker components: HomeTracker.tsx, CategoryTracker.tsx, ProductViewTracker.tsx skip the auto page_view and emit domain events. Instrumented UI: AddToCartButton.tsx, FavoriteButton.tsx, app/(client)/cart/page.tsx emit add/remove cart, wishlist, checkout events and mark buttons with global-clickstream-ignore-click to avoid duplicate global clicks. Runtime behavior Client-side only (no SSR effects). Logs every event to console; if endpoint missing, runs in dry-run and warns once. Network errors are non-blocking to the UI. 5.3.6 Field projection (frontend -\u0026gt; S3 -\u0026gt; DW) Field / Block Frontend payload Raw S3 (after Ingest) DW (PostgreSQL) Notes event_id eventId generated on client same as payload event_id (maps from eventId; ETL fallback UUID) Primary key, ON CONFLICT DO NOTHING event_timestamp - - (S3 LastModified exists) _ingest.receivedAt \u0026gt; payload \u0026gt; LastModified Derived by ETL event_name eventName eventName event_name page_url pageUrl pageUrl - Not stored in DW referrer referrer referrer - Not stored in DW user_id userId userId user_id user_login_state userLoginState userLoginState user_login_state identity_source optional optional identity_source Needs frontend/auth to populate client_id clientId clientId client_id session_id sessionId sessionId session_id is_first_visit isFirstVisit isFirstVisit is_first_visit product id product.id product.id context_product_id product name product.name product.name context_product_name product category product.category product.category context_product_category product brand product.brandName / brand?.name / brand?.title / brand product.brand or product.brandName context_product_brand Frontend maps brand from DB fields product price product.price product.price context_product_price BIGINT product discount price product.discountPrice product.discountPrice context_product_discount_price BIGINT product url path product.urlPath product.urlPath context_product_url_path element metadata element.{tag,id,role,text,dataset} element... - Not stored (needs schema change) ingest metadata - _ingest.{receivedAt,sourceIp,userAgent,method,path,requestId,apiId,stage,traceId} - Not stored; available during ETL Call pattern (one event per request):\nawait fetch(\u0026#34;https://\u0026lt;api-id\u0026gt;.execute-api.ap-southeast-1.amazonaws.com/clickstream\u0026#34;, { method: \u0026#34;POST\u0026#34;, headers: { \u0026#34;Content-Type\u0026#34;: \u0026#34;application/json\u0026#34; }, body: JSON.stringify(eventPayload), keepalive: true, }); ETL maps eventId -\u0026gt; event_id (PK with ON CONFLICT DO NOTHING), derives event_timestamp from _ingest.receivedAt \u0026gt; payload \u0026gt; S3 LastModified, and inserts into clickstream_dw.public.clickstream_events.\n5.3.6 Testing \u0026amp; Validation To validate ingestion:\nUse the Amplify app UI: Browse a few product pages Add items to cart Check S3 bucket clickstream-s3-ingest: Navigate to events/YYYY/MM/DD/HH/ Confirm new event-\u0026lt;uuid\u0026gt;.json files appear. Inspect one JSON file: Verify that _ingest metadata and product context are present. Review logs: API Gateway access logs Lambda function logs "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/5-workshop/5.4-building-the-private-analytics-layer/",
	"title": "Building the Private Analytics Layer",
	"tags": [],
	"description": "",
	"content": "5.4.1 VPC, Subnets, and Route Tables Configuration VPC CIDR: 10.0.0.0/16 Public subnet: 10.0.0.0/20 → SBW_Project-subnet-public1-ap-southeast-1a Hosts SBW_EC2_WebDB (EC2 OLTP). Private subnet: 10.0.128.0/20 → SBW_Project-subnet-private1-ap-southeast-1a Hosts SBW_EC2_ShinyDWH and SBW_Lamda_ETL. Public Route Table\n10.0.0.0/16 → local 0.0.0.0/0 → Internet Gateway Private Route Table\n10.0.0.0/16 → local S3 prefix list → Gateway VPC Endpoint for S3 No 0.0.0.0/0 route to the IGW or a NAT Gateway 5.4.2 VPC Endpoints (S3 \u0026amp; SSM) S3 Gateway VPC Endpoint Enables private network access to S3 for: SBW_Lamda_ETL No NAT Gateway is required. SSM Interface Endpoints com.amazonaws.ap-southeast-1.ssm com.amazonaws.ap-southeast-1.ssmmessages com.amazonaws.ap-southeast-1.ec2messages These endpoints allow Session Manager to manage and port-forward to SBW_EC2_ShinyDWH without needing a public IP address or SSH port.\n5.4.3 Data Warehouse on EC2 – SBW_EC2_ShinyDWH On this private EC2 instance:\nPostgreSQL database: clickstream_dw Main table: clickstream_events with the following fields: event_id event_timestamp event_name user_id user_login_state identity_source client_id session_id is_first_visit context_product_id context_product_name context_product_category context_product_brand context_product_price context_product_discount_price context_product_url_path The instance allows:\nSBW_Lamda_ETL to connect to the PostgreSQL DB clickstream_dw. Local web access to Shiny via SSM. 5.4.4 ETL Lambda – SBW_Lamda_ETL (running in the Private subnet) The ETL Lambda is where the main batch processing happens.\nVPC configuration:\nSubnet: SBW_Project-subnet-private1-ap-southeast-1a Security group: sg_Lambda_ETL Environment variables:\nDWH_HOST, DWH_PORT=5432, DWH_USER, DWH_PASSWORD, DWH_DATABASE=clickstream_dw RAW_BUCKET=clickstream-s3-ingest AWS_REGION=ap-southeast-1 Responsibilities:\nDetermine the list of files in s3://clickstream-s3-ingest/events/YYYY/MM/DD/ for the batch to be processed. For each JSON file: Extract Transform Load IAM role:\nGrants permissions for the Lambda to access the EC2_ShinyDWH (for example, via the database endpoint in the private subnet). 5.4.5 Scheduling with EventBridge – SBW_ETL_HOURLY_RULE EventBridge helps the platform operate in a batch style:\nRule name: SBW_ETL_HOURLY_RULE Schedule: rate(1 hour) Target: SBW_Lamda_ETL Each time the rule runs:\nThe ETL Lambda runs inside the private subnet. It reads new events from S3 via the Gateway Endpoint. It loads the processed data into clickstream_dw. You can also trigger the ETL Lambda manually (from the Lambda console) for backfill or testing purposes.\n5.4.6 Summary of Security Groups \u0026amp; Connectivity sg_Lambda_ETL:\nOutbound access to the S3 endpoint and to sg_analytics_ShinyDWH:5432. sg_analytics_ShinyDWH:\nInbound 5432/tcp from sg_Lambda_ETL. Inbound 3838/tcp for Shiny (used only via SSM port forwarding). "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/5-workshop/5.5-visualizing-analytics-with-shiny-dashboards/",
	"title": "Visualizing Analytics with Shiny Dashboards",
	"tags": [],
	"description": "",
	"content": "5.5.1 Environment information OS: Ubuntu 22.04 (Jammy) – EC2 in a private subnet PostgreSQL: v18 (installed from the apt.postgresql.org repo) Shiny Server: .deb binary from RStudio (Posit) User running Shiny: shiny App path: /srv/shiny-server/sbw_dashboard/app.R 5.5.2 Install system packages (system libs) Note: you need to enable the NAT Gateway before downloading system packages.\nLog in to EC2 using SSM Session Manager or SSH (temporarily, if available), then run:\n# 1) Update package list sudo apt-get update # 2) Install R (if not installed) sudo apt-get install -y r-base # 3) Install Postgres client \u0026amp; dev headers (for RPostgres) # If your DB is PG 18 then use postgresql-server-dev-18 # (if your version is different, change 18 -\u0026gt; 14, 15, ...) sudo apt-get install -y postgresql-client-18 postgresql-server-dev-18 # 4) Install libpq + libssl (required to build RPostgres) sudo apt-get install -y libpq-dev libssl-dev # 5) (If Shiny Server is not installed yet) # Depending on how you install it, just remember: # - shiny-server service: /etc/systemd/system/shiny-server.service # - app folder: /srv/shiny-server/ # - user: shiny Check that libpq and dev headers are present:\ndpkg -l | grep -E \u0026#39;libpq-dev|postgresql-server-dev\u0026#39; || echo \u0026#34;MISSING_LIBS\u0026#34; ls -l /usr/include/postgresql/libpq-fe.h || echo \u0026#34;NO_LIBPQ_HEADER\u0026#34; If you do not see any error → OK.\n5.5.3 Configure R library folder for user shiny To let Shiny Server load R packages, we install the packages under user shiny and use the folder:\n/home/shiny/R/x86_64-pc-linux-gnu-library/4.1 Run:\nsudo -u shiny R --vanilla \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; # Create library directory for user shiny if it does not exist dir.create(Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), recursive = TRUE, showWarnings = FALSE) # Put R_LIBS_USER at the top of .libPaths() .libPaths(c(Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), .libPaths())) cat(\u0026#34;LIBPATHS: \u0026#34;); print(.libPaths()) q(\u0026#34;no\u0026#34;) EOF You should see LIBPATHS where line 1 is /home/shiny/R/x86_64-pc-linux-gnu-library/4.1.\n5.5.4 Install required R packages Packages needed for the dashboard:\nshiny DBI RPostgres dplyr ggplot2 lubridate pool Install all of them under user shiny:\nsudo -u shiny R --vanilla \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; dir.create(Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), recursive = TRUE, showWarnings = FALSE) .libPaths(c(Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), .libPaths())) cat(\u0026#34;LIBPATHS: \u0026#34;); print(.libPaths()) install.packages( c(\u0026#34;shiny\u0026#34;, \u0026#34;DBI\u0026#34;, \u0026#34;RPostgres\u0026#34;, \u0026#34;dplyr\u0026#34;, \u0026#34;ggplot2\u0026#34;, \u0026#34;lubridate\u0026#34;, \u0026#34;pool\u0026#34;), repos = \u0026#34;https://cloud.r-project.org\u0026#34; ) q(\u0026#34;no\u0026#34;) EOF 💡 If you hit errors related to libpq-fe.h or libpq:\nRe-check that libpq-dev, postgresql-server-dev-XX, libssl-dev are installed. Re-run install.packages(\u0026quot;RPostgres\u0026quot;, ...) after installing all required libs. Verify that packages can be loaded:\nsudo -u shiny R --vanilla \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; .libPaths(c(Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), .libPaths())) cat(\u0026#34;LIBPATHS: \u0026#34;); print(.libPaths()) library(shiny) library(DBI) library(RPostgres) library(dplyr) library(ggplot2) library(lubridate) library(pool) cat(\u0026#34;All packages loaded OK \u0026#34;) q(\u0026#34;no\u0026#34;) EOF If there is no error → the R environment is OK.\n5.5.5 Deploying the Shiny app 5.5.5.1 Create the app folder and copy code sudo mkdir -p /srv/shiny-server/sbw_dashboard sudo chown -R shiny:shiny /srv/shiny-server/sbw_dashboard Create (or replace) the app file:\nsudo nano /srv/shiny-server/sbw_dashboard/app.R # PASTE THE FULL app.R CODE (the full version you are using) # Ctrl+O, Enter, Ctrl+X to save Make sure permissions are correct:\nsudo chown shiny:shiny /srv/shiny-server/sbw_dashboard/app.R sudo chmod 644 /srv/shiny-server/sbw_dashboard/app.R 5.5.5.2 Restart Shiny Server sudo systemctl restart shiny-server sudo systemctl status shiny-server 5.5.6 Check the app from EC2 (local) From an SSM session on EC2 (terminal):\n# Check Shiny welcome page curl -m 5 -sS -o /dev/null -w \u0026#34;WELCOME HTTP %{http_code} \u0026#34; http://127.0.0.1:3838/ # Check SBW dashboard app curl -m 10 -sS -o /dev/null -w \u0026#34;DASHBOARD HTTP %{http_code} \u0026#34; http://127.0.0.1:3838/sbw_dashboard/ If it returns DASHBOARD HTTP 200 → the app is running OK.\nIf it returns 500:\nLATEST=$(ls -1t /var/log/shiny-server/sbw_dashboard-shiny-*.log | head -n 1) echo \u0026#34;LATEST=$LATEST\u0026#34; sudo tail -n 100 \u0026#34;$LATEST\u0026#34; Check the error log for debugging.\n5.5.7 Access the dashboard from your local machine Because the EC2 instance is in a private subnet, you use SSM port forwarding:\n# Example using AWS CLI v2 on your local machine: aws ssm start-session --target \u0026lt;INSTANCE_ID_PRIVATE\u0026gt; --document-name AWS-StartPortForwardingSessionToRemoteHost --parameters \u0026#39;{\u0026#34;host\u0026#34;:[\u0026#34;127.0.0.1\u0026#34;],\u0026#34;portNumber\u0026#34;:[\u0026#34;3838\u0026#34;],\u0026#34;localPortNumber\u0026#34;:[\u0026#34;3838\u0026#34;]}\u0026#39; After that, open your browser on your local machine at:\nhttp://127.0.0.1:3838/sbw_dashboard/ The dashboard will show, for example:\nKPI cards (total events, users, sessions, …) Charts for events over time, event mix, events by login state Products \u0026amp; Raw sample tab (pagination, newest first, auto refresh every 10s – depending on your app code) 5.5.8 Quick summary of important commands # Install system libs sudo apt-get update sudo apt-get install -y r-base postgresql-client-18 postgresql-server-dev-18 libpq-dev libssl-dev # Install R packages for user shiny sudo -u shiny R --vanilla \u0026lt;\u0026lt;\u0026#39;EOF\u0026#39; dir.create(Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), recursive = TRUE, showWarnings = FALSE) .libPaths(c(Sys.getenv(\u0026#34;R_LIBS_USER\u0026#34;), .libPaths())) install.packages( c(\u0026#34;shiny\u0026#34;, \u0026#34;DBI\u0026#34;, \u0026#34;RPostgres\u0026#34;, \u0026#34;dplyr\u0026#34;, \u0026#34;ggplot2\u0026#34;, \u0026#34;lubridate\u0026#34;, \u0026#34;pool\u0026#34;), repos = \u0026#34;https://cloud.r-project.org\u0026#34; ) q(\u0026#34;no\u0026#34;) EOF # Deploy app sudo mkdir -p /srv/shiny-server/sbw_dashboard sudo nano /srv/shiny-server/sbw_dashboard/app.R # paste code sudo chown -R shiny:shiny /srv/shiny-server/sbw_dashboard sudo systemctl restart shiny-server # Check dashboard curl -m 10 -sS -o /dev/null -w \u0026#34;DASHBOARD HTTP %{http_code} \u0026#34; http://127.0.0.1:3838/sbw_dashboard/ "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/5-workshop/5.6-summary--clean-up/",
	"title": "Summary &amp; Clean up",
	"tags": [],
	"description": "",
	"content": "5.6.1 Summary After completing the lab, we have built a fully working Clickstream Analytics Platform:\nUser-Facing Layer\nNext.js application (ClickSteam.NextJS) on Amplify + CloudFront User authentication with Cognito PostgreSQL OLTP database (clickstream_web) on SBW_EC2_WebDB (public subnet) Ingestion \u0026amp; Raw Data Layer\nAPI Gateway HTTP API: clickstream-http-api (route POST /clickstream) Lambda Ingest: clickstream-lambda-ingest S3 Raw bucket: clickstream-s3-ingest/events/YYYY/MM/DD/event-\u0026lt;uuid\u0026gt;.json Private Analytics Layer\nVPC with public \u0026amp; private subnets (SBW_Project_VPC) S3 Gateway Endpoint, SSM Interface Endpoints Data Warehouse on EC2: SBW_EC2_ShinyDWH, database clickstream_dw ETL Lambda inside the VPC: SBW_Lamda_ETL, triggered by SBW_ETL_HOURLY_RULE R Shiny dashboards (sbw_dashboard) accessible only via SSM port forwarding Overall, this architecture demonstrates how to design a batch-based analytics platform that is secure, cost-optimized, and primarily built on serverless components plus two EC2 instances.\n5.6.2 Key Takeaways Separation of concerns:\nOLTP and Analytics are separated on 2 different EC2 instances, belonging to different logical domains. Security:\nThe DW and Shiny run in a private subnet with no public IP. SSM Session Manager replaces traditional SSH. The S3 Gateway Endpoint keeps S3 traffic inside the AWS private network. Cost optimization:\nNo NAT Gateway is used. ETL relies on serverless (Lambda + EventBridge). S3 is used as low-cost storage for raw data. Easy to extend:\nThe current design is batch-based, but can be extended to real-time, more complex analytics, or migrated to other DW technologies. 5.6.3 Resource Clean-up Amplify \u0026amp; CloudFront\nDelete the Amplify app (ClickSteam.NextJS). This action also deletes the associated CloudFront distribution. API Gateway \u0026amp; Lambda\nDelete clickstream-http-api. Delete the Lambda functions: clickstream-lambda-ingest SBW_Lamda_ETL EventBridge\nDelete the rule SBW_ETL_HOURLY_RULE. S3 Buckets\nEmpty and then delete: clickstream-s3-ingest (RAW clickstream) clickstream-s3-sbw (assets), if it is not used for other projects EC2 Instances\nStop or terminate: SBW_EC2_WebDB SBW_EC2_ShinyDWH Release any associated Elastic IP (if attached). VPC \u0026amp; Networking\nDelete VPC endpoints (S3 Gateway, SSM Interface Endpoints). Delete route tables, subnets, and the Internet Gateway. Finally, delete SBW_Project_VPC if it is no longer needed. "
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://tintran04.github.io/tranhoangtin-se185022-AWSFCJ/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]