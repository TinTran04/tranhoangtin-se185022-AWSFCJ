[
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/3-blogstranslated/3.1-blog1/",
	"title": "Blog 1",
	"tags": [],
	"description": "",
	"content": " How iFood Built a Platform to Run Hundreds of Machine Learning Models with Amazon SageMaker Inference By Daniel Vieira, Debora Fanin, Gopi Mudiyala, and Saurabh Trikande – April 8, 2025, in the Advanced section (300), Amazon SageMaker Data \u0026amp; AI Governance, Customer Solutions.\nIntroduction Headquartered in São Paulo, Brazil, iFood is a privately held national company and a leader in food tech in Latin America, processing millions of orders every month. iFood stands out for its strategy of integrating advanced technology into its operations. With the support of AWS, iFood has developed a powerful machine learning (ML) inference infrastructure using services like Amazon SageMaker to create and deploy ML models efficiently. This collaboration has allowed iFood to not only optimize internal processes but also provide innovative solutions for its delivery partners and restaurants.\niFood\u0026rsquo;s ML platform includes a set of tools, processes, and workflows developed with the following main objectives:\nAccelerating the development and training of AI/ML models, making them more reliable and easier to reproduce. Ensuring that the deployment of these models into production is reliable, scalable, and traceable. Enabling transparent, accessible, and standardized testing, monitoring, and evaluation of models in production. Figure 1. Overview — iFood and the application of AI/ML in their product system.\nGoals and Approach To achieve these goals, iFood leverages SageMaker to streamline model training and deployment. By integrating SageMaker features into iFood\u0026rsquo;s infrastructure, key steps are automated — from creating training datasets, training models, deploying them into production, to continuously monitoring their performance.\nThis article outlines how iFood uses SageMaker to enhance the entire ML lifecycle — from training to inference — and describes architectural changes and capabilities developed by the team.\nAI Inference at iFood iFood leverages its AI/ML platform to enhance customer experience across various touchpoints. Some typical use cases include:\nPersonalized Recommendations — Models analyze order history, preferences, and context to suggest appropriate restaurants and dishes, increasing customer satisfaction and order volume. Smart Order Tracking — The system predicts delivery times in real-time by combining traffic data, restaurant preparation times, and the location of the delivery driver, proactively notifying customers. Automated Customer Service — AI chatbots handle thousands of common requests daily, providing fast responses and retrieving relevant data to support personalization. Grocery Shopping Support — The app integrates language models that help customers create shopping lists from voice or text requests. Thanks to these initiatives, iFood can forecast demand, optimize processes, and deliver consistent user experiences.\nSolution Overview (Legacy Architecture) The diagram below illustrates iFood\u0026rsquo;s legacy architecture, where Data Science and Engineering teams had separate workflows — leading to challenges when deploying real-time ML models into production.\nFigure 2. Legacy Architecture — Describing data flow and the barriers between teams.\nPreviously, data scientists developed models in notebooks, fine-tuned, and published artifacts. Engineers then had to integrate these artifacts into the production system, causing delays and integration errors. To address this, iFood developed an internal ML platform to unify the process from development to deployment, creating a seamless experience for both teams.\nUpdated Architecture and ML Go! One of the core capabilities of iFood\u0026rsquo;s ML platform is providing the infrastructure to serve predictions. The internal platform (called ML Go!) is responsible for managing the deployment process, overseeing SageMaker Endpoints and Jobs. ML Go! supports both offline (batch) and real-time (online) predictions, and manages the lifecycle of models (registry, versioning, monitoring).\nFigure 3. Updated Architecture — Including pipelines, model registry, and inference components.\nThe platform provides:\nAutomated ML pipelines (SageMaker Pipelines) for model training and retraining. ML Go! CI/CD to push artifacts, build Docker images, and trigger pipelines. SageMaker Model Registry for versioning and model management. Monitoring mechanisms to detect drift and performance degradation. Final Architecture: Inference Components \u0026amp; ML Go! Gateway A significant improvement is the abstraction concept connecting with SageMaker (Endpoints \u0026amp; Jobs) called ML Go! Gateway, along with separating \u0026ldquo;inference components\u0026rdquo; in the endpoint — helping to divide concerns, accelerate delivery, and manage resources more efficiently. Endpoints now manage multiple inference components, and ML Go! CI/CD only focuses on model version promotion, without deep intervention in infrastructure.\nFigure 4. Final Architecture — Inference components, ML Go! Gateway, and integration with service accounts.\nIn this new structure:\nEndpoints can contain multiple inference components, allowing load distribution by function or load. ML Go! Dispatcher/Gateway forwards requests to the appropriate endpoint or job. CI/CD handles artifacts (Docker images, configs), and SageMaker Pipeline orchestrates training → evaluation → registry → deployment. Using SageMaker Inference Model Serving Containers Standardizing the environment through containers is a crucial element of modern ML platforms. SageMaker provides built-in containers for TensorFlow, PyTorch, XGBoost, and more, as well as the ability to use custom containers.\niFood focuses on using custom containers to:\nStandardize ML code (not directly using notebooks in production). Package dependencies, libraries, and inference logic in an image (e.g., BruceML scaffolding). Easily recreate training and serving environments, monitor results, and debug. BruceML helps standardize the way training and serving code is written, creating a scaffold compatible with SageMaker (autotuning, deployment hooks, monitoring).\nAutomating Deployment and Retraining (ML Pipelines \u0026amp; CI/CD) iFood uses SageMaker Pipelines to build CI/CD for ML: pipelines are responsible for orchestrating the entire data flow — from preprocessing, training, evaluation, to promotion in the Model Registry and deployment. ML Go! CI/CD integrates with the organization’s CI/CD system to:\nPush artifacts (code + container image). Trigger training and evaluation pipelines. Automatically register models into the Model Registry. Deploy or promote models to the appropriate endpoint (online / batch). Depending on SLA:\nBatch inference: Uses SageMaker Transform jobs for large-scale predictions. Real-time inference: Deploys models to SageMaker Endpoint with the appropriate container/instance configuration. SageMaker Pipelines helps automate and coordinate complex workflows, reducing errors and shortening development cycles.\nRunning Inference at Different SLA Formats iFood uses multiple inference methods to meet different requirements:\nReal-time endpoints for low-latency tasks (user-facing). Batch transform jobs for large-scale data processing, periodic recommendations. Asynchronous inference (SageMaker Asynchronous Inference) for time-consuming inference tasks. Multi-model endpoints (GPU) to host multiple models on the same GPU endpoint, optimizing resource use. The improvements in collaboration between iFood and the SageMaker Inference team include:\nOptimizing cost and performance for inference (reducing ~50% of cost for some workloads, lowering ~20% average latency when using inference components). Improving autoscaling to handle spikes more effectively (shortening scaling time, enhancing detection of scale events). Easier deployment of LLM / Foundation Models (FM). Scale-to-zero feature for endpoints helps save costs when there is no traffic. Multi-model GPU endpoints reduce infrastructure costs in multi-model scenarios. Model Optimization and Packaging Some technical points iFood focuses on:\nStandardizing containers for both training and serving. Automating the build/publish of images to the registry (ECR). Packaging LLM / FM for faster deployment. Supporting autoscaling and scale-to-zero for dev/test environments and low-traffic workloads. Achieved Benefits \u0026amp; Impact The benefits iFood has gained:\nReduced time to get models into production (faster time-to-market). Increased pipeline \u0026amp; artifact reuse across teams. Lower operational costs through GPU/multi-model optimization and scale-to-zero. Improved stability and model management at scale. Conclusion By leveraging SageMaker capabilities, iFood has transformed its approach to ML/AI: building a centralized ML platform (ML Go!), automating data flows, standardizing containers, and collaborating with the SageMaker Inference team to optimize efficiency, cost, and scalability. This has helped iFood:\nBridge the gap between Data Science and Engineering. Deploy hundreds of ML models reliably. Create a reference platform for organizations wishing to apply inference at scale. “At iFood, we are leading the way in applying AI and machine learning technologies to transform\u0026hellip; The lessons learned have helped us create our internal platform, which can serve as a blueprint for other organizations\u0026hellip;”\n– Daniel Vieira, Director of ML Platforms at iFood.\nAbout the Authors Daniel Vieira — Director of Machine Learning Engineering at iFood. He has a background in computer science (BSc \u0026amp; MSc, UFMG) and over a decade of experience in software engineering and ML platforms. He enjoys music, philosophy, and coffee.\nDebora Fanin — Senior Customer Solutions Architect at AWS (Brazil). She specializes in managing enterprise customer transformation and designing effective cloud adoption strategies.\nSaurabh Trikande — Senior Product Manager, Amazon Bedrock \u0026amp; SageMaker Inference. Focuses on democratizing AI and inference solutions at scale.\nGopi Mudiyala — Senior Technical Account Manager at AWS. Supports clients in the financial services industry and is passionate about machine learning.\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.1-week1/",
	"title": "Worklog Week 1",
	"tags": [],
	"description": "",
	"content": " Week 1 Goals: Get familiar with the team and learning environment at First Cloud Journey (FCJ). Understand the overview of AWS services and basic service groups. Learn how to create an AWS Free Tier account and activate $200 credit. Get familiar with the AWS Management Console to use basic services. Install and set up AWS CLI to interact with AWS resources via the command line. Practice some basic labs such as creating an account, setting up MFA, budgets, and exploring EC2. Tasks for this week Day Task Start Date End Date Resource 2 - Create AWS Free Tier account - Follow 5 steps to receive $200 credit: + Use the platform model in Amazon Bedrock playground + Create Amazon RDS database + Build a web application using AWS Lambda + Set up cost budget with AWS Budgets + Launch an EC2 instance with Amazon EC2 08/09/2025 08/09/2025 AWS Free Tier 3 - Watch and learn AWS introductory modules (YouTube): + Module 01-01 - What is Cloud Computing? + Module 01-02 - What makes AWS unique? + Module 01-03 - How to get started with the cloud journey + Module 01-04 - AWS Global Infrastructure + Module 01-05 - AWS Services Management Tools + Module 01-06 - Cost Optimization and Working with AWS + Module 01-07 - Hands-on and further research 09/09/2025 09/09/2025 Learning Materials 4 - Practice (YouTube): + Module 01-Lab01-01 - Create AWS Account + Module 01-Lab01-02 - Set up Virtual MFA device + Module 01-Lab01-03 - Create an Admin group and users + Module 01-Lab01-04 - Support Account Authentication + Module 01-Lab07-01 - Create Budget from Template + Module 01-Lab07-02 - Guide to Creating Cost Budgets + Module 01-Lab07-03 - Create Usage Budget in AWS + Module 01-Lab07-04 - Create Reserved Instance Budget (RI) + Module 01-Lab07-05 - Create Savings Plans Budget + Module 01-Lab07-06 - Clean Up Budgets + Module 01-Lab09-01 - AWS Support Plans + Module 01-Lab09-02 - Types of Support Requests + Module 01-Lab09-03 - Change Support Plan + Module 01-Lab09-04 - Manage Support Requests 10/09/2025 10/09/2025 Learning Materials 5 - Learn EC2 basics (hands-on): + What is EC2? + AMI (Amazon Machine Image) + EBS (Elastic Block Store) + Elastic IP + Methods to SSH into EC2 + Create and manage EC2 instances 11/09/2025 11/09/2025 Learning Materials Achievements of Week 1: Successfully created AWS Free Tier account and activated $200 credit. Learned to log in and use AWS Management Console, search and access basic services. Successfully installed and configured AWS CLI on the personal computer, including: Setting Access Key, Secret Key, and default Region. Became familiar with theoretical modules and hands-on labs on YouTube, including: Overview of cloud computing and AWS AWS global infrastructure Managing account and cost budgets Introduction to EC2 and creating an instance Started to understand how to create, launch, and connect to an EC2 instance. Learned how to set up a budget with AWS Budgets to monitor usage costs. Completed basic security configurations (setting up MFA, IAM admin users). "
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/4-eventparticipated/4.1-event1/",
	"title": "Event 1",
	"tags": [],
	"description": "",
	"content": "Reflection Report “Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders” Event Information Event name: AWS Cloud Day Vietnam – AI Edition 2025 (Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders) Date: 18 September 2025 Location: 2 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City Role: Attendee Purpose of the Event AWS Cloud Day Vietnam – AI Edition 2025 is positioned as an important gathering of the technology and business community in Vietnam. Its main goal is to accelerate digital transformation by leveraging the combined power of Cloud Computing and Artificial Intelligence (AI).\nThe objectives can be summarized into the following pillars and meanings:\nConnecting the Vietnamese technology community:\nCreating a space for companies, IT experts, developers, and industry leaders to meet, exchange, and learn together about Cloud \u0026amp; AI.\nHelping businesses understand and practically apply Cloud \u0026amp; AI:\nFocusing on topics such as Gen AI, data analytics, modernizing workloads, and industry cloud tailored for specific sectors.\nShowcasing success stories:\nPresenting case studies such as Xanh SM, Honda Vietnam, Masterise Group, Techcombank, TymeX, F88… to demonstrate how Cloud \u0026amp; AI can transform business operations.\nPromoting digital economy growth and innovation:\nContributing to the realization of the national digital transformation strategy, making Cloud \u0026amp; AI a foundation for economic growth.\nBuilding a platform for long-term networking and collaboration:\nCreating bridges between technology – enterprises – industry leaders, aiming at sustainable cooperation in the digital era.\nFour key strategic pillars emphasized Democratizing Generative AI (GenAI) for businesses\nMoving GenAI beyond initial “hype” into practical applications. Demonstrating how businesses can transform generic AI models into context-aware solutions tied to a clear data strategy. Bridging the gap between Business and IT\nEspecially in the Financial Services (FSI) domain. Cloud is not just infrastructure, but a driver of business value, enabling models such as Ecosystem Banking and Embedded Finance. Accelerating industry-specific modernization\nDesigning customized roadmaps for each sector: Retail, Energy, Telecommunications, Real Estate, Public Sector, etc. Emphasizing that modernization has no “one-size-fits-all formula”; it must be based on the characteristics of each system and each business strategy. Strengthening security and resilience\nPromoting the “security by design” mindset – security embedded from the design phase. Integrating security throughout the entire application lifecycle, from development to production operations. Speakers The event brought together 24 speakers from government agencies, embassies, financial institutions, technology companies, and AWS partners. Some key speakers:\nH.E. Pham Duc Long – Vice Minister, Ministry of Science and Technology, Vietnam H.E. Marc E. Knapper – United States Ambassador to Vietnam Jaime Valles – Vice President, Managing Director for Asia Pacific \u0026amp; Japan, AWS Jeff Johnson – Managing Director ASEAN, AWS Dr Jens Lottner – CEO, Techcombank Dieter Botha – CEO, TymeX Trang Phung – CEO, U2U Network Vu Van – Co-founder \u0026amp; CEO, ELSA Corp Nguyen Hoa Binh – Chairman, Nexttech Group Along with many other leaders and experts from F88, Masterise Group, VTV Digital, Honda Vietnam, Mobifone, Katalon, Renova Cloud, TechX Corp,… Key Content 1. Strategic convergence: Policy and leadership Government endorsement:\nThe opening remarks from the Vice Minister of the Ministry of Science and Technology and the U.S. Ambassador showed strong support for Vietnam’s digital infrastructure and Cloud–AI ecosystem.\nLeadership Panel:\nLeaders such as Jeff Johnson (AWS), Vu Van (ELSA), Nguyen Hoa Binh (Nexttech)… emphasized the role of people and an innovation culture in driving digital transformation, not just technology alone.\n2. Track 1: Financial Services (FSI) – New banking models Innovation in Banking \u0026amp; Insurance:\nTechcombank, Bao Viet Holdings and other financial organizations shared their journey towards Ecosystem Banking and Embedded Finance.\nXGenAI implementation:\nTechX presented the XGenAI platform built on AWS, illustrating how GenAI is used to personalize customer experiences, optimize care, and recommend financial services.\n3. Track 2: Multi-industry modernization Honda Vietnam:\nShared a detailed roadmap for migrating the SAP system to AWS, going beyond “lift-and-shift” towards changing the operating model, optimizing costs, and increasing flexibility.\nVTV Digital \u0026amp; Mobifone:\nPresented their digital transformation journeys “From Vision to Value” in digital media and telecommunications.\nMasterise Group:\nHighlighted the migration of hundreds of VMware workloads to AWS, showing the scale and complexity of modernizing real estate infrastructure.\n4. Tracks 3 \u0026amp; 4: Data, AI and DevOps Data Strategy:\nExperts from Onebyzero and Techcom Securities emphasized that “data is the key differentiator” for GenAI; AI output quality directly depends on input data quality.\nThe DevOps revolution:\nKatalon and Renova Cloud discussed integrating GenAI into the DevOps lifecycle: auto code generation, automated test creation, log analysis, and CI/CD optimization.\n5. Negative impacts of legacy application architectures Slow product release cycles → Lost revenue and missed market opportunities. Inefficient operations → Lost productivity and higher operational costs. Non-compliance with security standards → High security risks and reputational damage. These limitations are the driving force behind modernizing application architectures.\n6. Transition to Microservices Architecture The event emphasized the shift from monolithic systems to Microservices Architecture:\nSystems are modularized, with each function as an independent service. Services communicate via events (event-driven). Three key architectural pillars were highlighted:\nQueue Management: Handling asynchronous tasks, offloading the main system. Caching Strategy: Optimizing performance and reducing query latency. Message Handling: Enabling flexible communication between services, supporting scalability and fault tolerance. 7. Domain-Driven Design (DDD) Four-step approach:\nIdentify domain events → build timelines → identify actors → define bounded contexts.\n“Bookstore” case study:\nUsed to demonstrate how to apply DDD in practice, from domain modeling to service decomposition.\nContext mapping – 7 integration patterns:\nHelping manage relationships between bounded contexts (Partnership, Shared Kernel, Anti-corruption Layer, etc.).\n8. Event-Driven Architecture Three main integration patterns:\nPublish/Subscribe Point-to-Point Streaming Benefits:\nLoose coupling between services Easy scalability Higher resilience in case of localized failures Sync vs Async comparison:\nSpeakers clearly explained the trade-offs: synchronous calls are simpler but prone to bottlenecks; asynchronous patterns are more complex but more flexible and scalable.\n9. Compute Evolution \u0026amp; Serverless Shared Responsibility Model:\nExplained the roles of AWS vs customers across compute models: EC2 → ECS → Fargate → Lambda.\nBenefits of Serverless:\nNo server management Automatic scaling Pay-for-value based on actual usage Functions vs Containers:\nSuggested criteria for choosing between them (runtime duration, statefulness, frequency, deployment complexity, etc.).\n10. Amazon Q Developer SDLC automation:\nSupporting developers across the entire lifecycle from planning → coding → testing → maintenance.\nCode transformation:\nHelping upgrade Java, modernize .NET, and migrate from mainframe/legacy stacks to modern architectures.\nAWS Transform Agents:\nProviding dedicated agents for VMware, Mainframe, .NET… to reduce time and risk during application modernization.\nLessons Learned 1. Design \u0026amp; Strategic Thinking Business-first approach:\nTechnology solutions must start from business problems, rather than following technology trends blindly.\nUbiquitous Language:\nA shared language between business and tech teams helps reduce misunderstandings and accelerates requirements analysis and design.\nResilience as a baseline:\nModern systems need to be designed with resilience from the beginning, instead of patching after incidents occur.\n2. Technical Architecture Event storming:\nAn effective method to model business processes into domain events, leading naturally to DDD and event-driven architecture.\nEvent-driven communication:\nReplacing part of synchronous communication with event-driven mechanisms makes the system more flexible, more scalable, and less tightly coupled.\nIntegration patterns:\nUnderstanding clearly when to use synchronous APIs, when to use pub/sub, and when to use streaming (Kafka/Kinesis).\nCompute spectrum:\nKnowing how to balance between VMs, containers, and serverless to choose the most suitable runtime for each workload.\n3. Modernization Strategy Phased approach:\nInstead of “tearing everything down and rebuilding”, a phased roadmap is needed, prioritizing systems with the highest impact.\n7Rs framework:\nThere are multiple paths to modernization (Rehost, Replatform, Refactor, Rearchitect, Repurchase, Retire, Retain).\nEach application must be assessed individually.\n“Migrate to Operate”:\nThe goal is not just to move to the cloud, but to operate more intelligently after migration, continuously optimizing cost, performance, and security.\nROI measurement:\nIt is important to measure clearly: cost reduction, increased agility, shorter time-to-market, and improved customer experience.\nApplication to My Work Applying DDD to current projects:\nOrganizing event storming sessions with the business team to model domains and split bounded contexts.\nRefactoring Microservices:\nUsing bounded contexts to define service boundaries, avoiding microservices being “over-fragmented” or overlapping responsibilities.\nImplementing event-driven patterns:\nGradually replacing certain synchronous APIs with asynchronous messaging, pub/sub, or streaming.\nServerless Adoption:\nExperimenting with some use cases on AWS Lambda or Fargate to evaluate real-world benefits.\nAssessing data readiness:\nBefore starting any GenAI project, checking data quality, data governance, and the overall data strategy.\nExperimenting with GenAI in DevOps:\nUsing tools like Amazon Q Developer to auto-generate code, tests, and shorten the development cycle.\nImplementing “Security by design”:\nApplying best practices in IAM, encryption, logging, and monitoring right from the system design stage.\nEvent Experience Attending AWS Cloud Day Vietnam – AI Edition 2025 provided me with insights that are both strategic and hands-on:\nLearning from high-profile experts Presentations from banking leaders, cloud experts, and solution architects helped me clearly see the connection between business strategy and technical infrastructure. Real-world case studies (Techcombank, Honda, Masterise, TymeX…) gave me a concrete picture of the transformation journeys in different industries. Practical technical experience Sessions on event storming, DDD, microservices, and event-driven architecture helped me connect theoretical knowledge with realistic deployment scenarios. I was particularly impressed by the “GenAI-powered App-DB Modernization” workshop, which simulated the journey of modernizing a monolithic application into a cloud-native architecture. Using modern tools My first deep dive into Amazon Q Developer in a real SDLC context showed me the potential of using AI to increase developer productivity and reduce repetitive work. Integrating GenAI into DevOps gave me a new perspective on the future of software development. Networking and discussions The event was a great opportunity to talk with experts and professionals working in finance, technology, and cloud consulting. Through short side conversations, I realized that digital transformation is not only about technology, but also about changing mindsets, ways of working, and organizational culture. Key takeaways Data is the key differentiator: GenAI cannot be truly effective if data is not properly governed. Modernization is a continuous journey: There is no fixed “final destination”; it is an ongoing process of optimization and adaptation. Security is everyone’s responsibility: From developers to operators, from leadership to staff, everyone must be conscious about security. "
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/4-eventparticipated/4.2-event2/",
	"title": "Event 2",
	"tags": [],
	"description": "",
	"content": "Reflection Report “Exploring Agentic AI – Amazon QuickSuite” Event Information **Event name: GenAI-powered App-DB Modernization workshop **Date: 7 November 2025 Location: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City Role: Attendee Purpose of the Event Clarify the concept of Agentic AI: the shift from passive Generative AI to Agentic AI (AI agents) capable of autonomous action. Introduce and demonstrate Amazon QuickSuite live for the first time in Vietnam. Encourage businesses to adopt Agentic AI through the AWS LIFT financial support program (credits of up to USD 80,000). Provide a practical hands-on environment in which participants can directly build and experience AI models under the guidance of AWS experts and partners. Speakers Vivien Nguyen – Territory Manager, AWS Tung Cao – Solution Architect, AWS Cloud Kinetics team – AWS strategic implementation partner in Vietnam Key Content Paradigm shift: From Generative AI to Agentic AI Generative AI\nFocuses on generating content (text, images, source code, etc.) based on user prompts. Fundamentally reactive: only responds when queried. Agentic AI (AI agents)\nFocuses on autonomy and the ability to act. Can perceive its environment, plan, reason through multiple steps, and execute tasks automatically without constant human intervention. The goal is to build systems where “AI does the work for us”, not just “chats with us”. The workshop helped me clearly understand that Agentic AI is the evolution from “AI that answers” to “AI that acts”, which is very suitable for complex and repetitive tasks in enterprises.\nIntroduction to Amazon QuickSuite First live demo in Vietnam:\nThe workshop marked the first live demonstration of Amazon QuickSuite for users in Vietnam. Unified ecosystem:\nTightly integrates: Amazon QuickSight – data visualization and analytics. QuickSuite Q – natural language interaction for asking questions and generating insights. Together, they enable “Analyst Agents” that can read data, answer business questions, and propose actions. Speed and agility (“Quick”):\nAllows businesses to move from idea → prototype → real-world testing in a short time. Suitable for teams that want to experiment quickly and iterate continuously. Data-centric by design:\nQuickSuite is built for large data volumes, ensuring that AI agents have enough “fuel” to make accurate decisions grounded in real business context. Partner ecosystem \u0026amp; strategic support AWS provides the underlying infrastructure platform and AI/Analytics services. Cloud Kinetics plays the role of: Architecture advisor, ensuring that solutions fit the specific needs of each business. Supporting the “last mile” of implementation – turning abstract technology into concrete solutions that run in production. This “two-layer support model” (AWS + partner) helps businesses reduce risk, especially when starting out with new technologies such as Agentic AI.\nAWS LIFT program – Financial support for innovation Credits of up to USD 80,000 for eligible customers, particularly new customers and SMBs. Purpose: Lower the cost barrier when experimenting with high-performance compute systems and AI-related R\u0026amp;D projects. Allow companies to focus on ideas and execution, instead of worrying too much about initial infrastructure costs. What I Learned Design Mindset Focus on system autonomy:\nWhen designing Agentic AI, the goal is to build agents that proactively work on behalf of humans, for example: Automatically generating periodic reports. Monitoring metrics and sending alerts. Suggesting operational adjustments (such as in the supply chain). Tying AI to concrete operational bottlenecks:\nAI should not be built just because it is a “trend”, but should start from processes that are: Repetitive. Composed of many manual steps. Error-prone when done entirely by humans. Technical Architecture Ecosystem-based approach:\nAn effective agent must be connected to multiple tools: data sources, BI, APIs, workflows, etc. In this context, QuickSuite is the “connective tissue” between: Data (data sources, QuickSight). Action logic (questions, rules, automated task workflows). AWS infrastructure readiness:\nThe first step is to set up the AWS account, region, access permissions, and required services. This lays the foundation to easily enable/disable and experiment with new services such as QuickSuite and Agentic workflows later on. Modernization \u0026amp; Deployment Strategy Early adopter advantage:\nBusinesses/teams that learn and master QuickSuite early will gain advantages in productivity, decision-making speed, and the ability to test new business models. Smart cost management:\nCombining AWS LIFT credits with a small, clearly scoped PoC approach. Helps accelerate time-to-market while still minimizing financial risk. Application to My Work Deepening my knowledge of QuickSuite:\nExploring how to integrate QuickSight + QuickSuite Q into existing data analytics workflows. Aiming to build “Analyst Agents” that can automatically answer questions about KPIs, trends, and reporting. Leveraging the AWS LIFT program:\nReviewing eligibility and applying to receive AWS credits for upcoming AI/Agentic AI R\u0026amp;D or PoC projects. Identifying internal use cases suitable for Agentic AI:\nReviewing workflows that involve many repetitive steps (reporting, monitoring, request classification, etc.) to see whether they can be transformed into AI agent workflows. Collaborating with partners such as Cloud Kinetics:\nInstead of building everything 100% in-house, partnering with experts to design a robust architecture from the start, and avoid fundamental mistakes for complex systems. Event Experience Attending the “Exploring Agentic AI – Amazon QuickSuite” workshop at Bitexco Financial Tower was a professional and multi-dimensional experience: combining theory, hands-on practice, and community connection.\nLearning from experienced speakers I had the opportunity to hear directly from AWS and Cloud Kinetics about the vision for Agentic AI in enterprises. The content was well balanced between concepts (what Agentic AI is and why it matters) and real-world implementation (how to use QuickSuite and where to start). Hands-on technical experience The ~90-minute hands-on session allowed me to: Work directly with QuickSight + QuickSuite Q. Understand more concretely how an “analyst agent” can read data and answer business questions. The “hands-on guidance” from AWS experts helped me quickly overcome initial difficulties when working with a new tool. Networking and ecosystem Networking time enabled me to talk with: Professionals working in cloud, data, and AI. The Cloud Kinetics team, which helped me better understand the role of partners in turning the AWS platform into concrete solutions for each industry. Key takeaways Agentic AI is the future of enterprise operations:\nThe shift from “chatting with AI” to “AI actually doing the work” will unlock many new optimization models. Speed is a critical factor:\nTools like QuickSuite are designed for rapid deployment, so early adopters will gain a clear advantage in agility. Capital enables innovation:\nThe AWS LIFT program shows that the key question is no longer “Can we afford it?”, but rather “Do we have the courage and readiness to seize the opportunity?”. In summary, the workshop helped me gain a much clearer understanding of Agentic AI, how AWS brings it to life through Amazon QuickSuite, and the resources (tools, partners, funding) that AWS provides so that businesses can start their AI adoption journey in a practical and sustainable way.\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/",
	"title": "Internship Report",
	"tags": [],
	"description": "",
	"content": "Internship Report Student Information: Full Name: Trần Hoàng Tín\nPhone Number: 0936091757\nEmail: tinthse185022@fpt.edu.vn\nUniversity: FPT University Ho Chi Minh\nMajor: Information Technology\nClass: SE185022\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 12/08/2025 to 12/11/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/",
	"title": "Worklog",
	"tags": [],
	"description": "",
	"content": "Week 1: Getting familiar with AWS and basic AWS services\nWeek 2: Doing task A\u0026hellip;\nWeek 3: Doing task B\u0026hellip;\nWeek 4: Doing task C\u0026hellip;\nWeek 5: Doing task D\u0026hellip;\nWeek 6: Doing task E\u0026hellip;\nWeek 7: Doing task G\u0026hellip;\nWeek 8: Doing task H\u0026hellip;\nWeek 9: Doing task I\u0026hellip;\nWeek 10: Doing task L\u0026hellip;\nWeek 11: Doing task M\u0026hellip;\nWeek 12: Doing task N\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/5-workshop/5.1-objectives--scope/",
	"title": "Objectives &amp; Scope",
	"tags": [],
	"description": "",
	"content": "This section defines the purpose, learning objectives, and scope of the workshop that is built around a Batch-based Clickstream Analytics Platform for an e-commerce website selling computer products.\nThe platform is implemented on AWS and uses:\nNext.js on AWS Amplify Hosting (front-end, Server-Side Rendering). Amazon CloudFront (global content delivery). Amazon Cognito (user authentication). Amazon API Gateway and AWS Lambda (clickstream ingestion and ETL). Amazon S3 (raw clickstream data storage). Amazon VPC with public and private subnets (network isolation). PostgreSQL on Amazon EC2 (OLTP and Data Warehouse). R Shiny Server (analytics dashboards). Business Context The target system is an e-commerce website that sells computer-related products (laptops, monitors, accessories, etc.).\nThe business would like to:\nUnderstand how users interact with the website (pages visited, products viewed, add-to-cart events, checkout attempts). Measure conversion funnels (from product view to purchase). Identify top-performing products and periods of high activity. Do this without impacting the operational database and without exposing internal analytics components to the public Internet. The platform therefore separates online transaction processing (OLTP) from analytics, and gathers clickstream data in a dedicated analytics environment.\nLearning Objectives After completing all sections (5.1–5.6), the reader should be able to:\nArchitectural understanding Describe the overall architecture of a batch-based clickstream analytics platform on AWS. Explain the differences between: The user-facing domain (Amplify, CloudFront, Cognito, OLTP EC2). The ingestion \u0026amp; data lake domain (API Gateway, Lambda Ingest, S3 Raw bucket). The analytics \u0026amp; data warehouse domain (ETL Lambda, PostgreSQL DW, Shiny). Justify why OLTP and Analytics are logically and physically separated, and how this reduces risk for the operational workload. Practical skills Trigger and inspect clickstream ingestion from the frontend to API Gateway → Lambda Ingest → S3. Configure a Gateway VPC Endpoint for S3 and update route tables so that analytics components inside private subnets can reach S3 without using a NAT Gateway. Configure and test a VPC-enabled ETL Lambda that: Reads raw JSON files from the S3 Raw Clickstream bucket. Transforms events into SQL-ready analytical tables. Loads data into a PostgreSQL Data Warehouse hosted on EC2 in a private subnet. Connect to the Data Warehouse and run sample SQL queries to validate the pipeline (event counts, top products, etc.). Access R Shiny dashboards running on the same EC2 instance as the Data Warehouse, and interpret the main charts (funnels, top products, time series). Security and cost-awareness Explain how Gateway VPC Endpoints keep S3 traffic on the AWS private network. Compare using a Gateway VPC Endpoint versus a NAT Gateway in terms of security, cost, and operational complexity. List the main security controls used in the architecture: Public vs. private subnets. Security groups between OLTP, ETL Lambda, and Data Warehouse. Limited IAM permissions for Lambda Ingest and ETL Lambda. Scope of the Workshop The workshop focuses on three core capabilities of the platform:\nImplementing clickstream ingestion\nCapturing user interactions in the browser. Sending events as JSON to API Gateway. Persisting raw events as time-partitioned files in an S3 Raw Clickstream bucket. Building the private analytics layer\nConfiguring the VPC, private subnets, and S3 Gateway VPC Endpoint. Running ETL Lambda inside the VPC so that it can: Read from S3 using private connectivity. Write data into the PostgreSQL Data Warehouse on a private EC2 instance. Visualizing analytics with Shiny dashboards\nQuerying analytical tables from R Shiny Server running on the same EC2 instance as the Data Warehouse. Providing interactive dashboards for funnel analysis, product performance, and time-based trends. The workshop assumes that:\nThe base infrastructure (VPC, subnets, EC2 instances, IAM roles, basic Lambda skeletons, and S3 buckets) has already been provisioned, for example via Terraform or CloudFormation. The focus is on understanding and validating the data flow and private connectivity, rather than on writing production-grade ETL code or frontend code from scratch. Out-of-Scope Topics To keep the content focused and achievable within a limited time, the workshop does not cover:\nReal-time streaming pipelines (for example, Amazon Kinesis, Kafka, or managed streaming services). Advanced data warehousing technologies such as Amazon Redshift, Redshift Serverless, or Lakehouse architectures. Complex machine learning or user segmentation models built on top of the clickstream data. Production-grade CI/CD pipelines, blue/green deployments, or multi-account AWS setups. Deep optimization of SQL queries and index design beyond simple examples. These topics are considered future extensions of the platform and may be explored in subsequent work.\nTarget Audience The workshop is primarily intended for:\nStudents or engineers with a basic understanding of AWS who want to see how multiple services fit together into a real analytics solution. Developers who are comfortable with JavaScript/TypeScript, SQL, and Linux-based environments. Anyone interested in learning how to design a secure, cost-conscious analytics architecture using mostly serverless components and a small EC2 footprint. Readers are not expected to be experts in networking or data engineering; the workshop provides concrete, guided steps in later sections (5.2–5.5) to make the architecture tangible and reproducible.\nFigure 5-1: Amplify hosting for the e-commerce frontend\nThe screenshot shows the Amplify application that hosts the Next.js e-commerce frontend, including the main branch, last deployment status, and the CloudFront URL used in this workshop.\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/3-blogstranslated/3.2-blog2/",
	"title": "Blog 2",
	"tags": [],
	"description": "",
	"content": " How Salesforce Business Technology Uses AWS Direct Connect SiteLink for Reliable Global Connectivity By Alexandra Huides and Corey Harris Jr – May 9, 2025, in the AWS Direct Connect SiteLink section.\nIntroduction Salesforce Business Technology has used AWS Direct Connect SiteLink to build a global hybrid network architecture, ensuring flexible, high-performance, and reliable connectivity.\nThis solution helped Salesforce scale its infrastructure, reduce operational costs, and accelerate innovation in its journey toward cloud modernization with AWS.\nThis article is brought to you in collaboration with Georgi Stoev and Ravi Patel – senior technical experts at Salesforce.\nOverview Salesforce is an AWS strategic partner and the world leader in customer relationship management (CRM).\nThe Business Technology team is responsible for building and operating enterprise applications that support areas such as finance, data centers, security, data warehousing, and Salesforce\u0026rsquo;s virtual machines.\nWith a global presence, Salesforce needed a network architecture that was:\nFlexible and highly scalable. Minimized latency and downtime. Ensured high security and reliability. However, traditional internet-based network solutions couldn\u0026rsquo;t meet these stringent requirements.\nThis is why AWS Direct Connect SiteLink was chosen — providing a private, dedicated connection, bypassing the public internet to significantly improve security and latency.\nPrerequisites Before deployment, the Salesforce technical team understood the following AWS network components:\nAmazon Virtual Private Cloud (VPC) AWS Transit Gateway AWS Direct Connect These services form the foundation for the global hybrid network architecture, allowing for private, low-latency connectivity between multiple Direct Connect locations without going through intermediate AWS regions.\nAWS Direct Connect SiteLink AWS Direct Connect provides a private network connection between on-premises infrastructure and AWS, optimizing performance, latency, and reliability.\nSiteLink is an extension feature of Direct Connect that allows direct connections between on-premises networks via the global AWS backbone, enabling:\nData transmission through the shortest path, bypassing AWS regions. Leveraging the global AWS network to transfer data quickly and securely. Pay-per-use pricing, without needing to establish new connections. How it works:\nEstablish an on-premises connection to AWS at one of over 100 Direct Connect locations worldwide. Create a Virtual Interface (VIF) on that connection and enable SiteLink. When VIFs are attached to the same Direct Connect Gateway (DXGW), data is transmitted directly between locations using the AWS backbone. Salesforce Business Technology\u0026rsquo;s Global Footprint Salesforce Business Technology manages 7 strategic locations globally:\n3 in the United States 3 in the Asia-Pacific region 1 in Europe The network is built on private MPLS backbone links combined with AWS Regions, supporting complex data flows between data centers and cloud environments.\nHowever, challenges arose including:\nStatic and hard-to-scale infrastructure. High operational costs and heavy reliance on multiple providers. Routing complexity and outages in some regions. Figure 1. A sample global private data center connection using dedicated circuits.\nSolution: SiteLink To solve these problems, Salesforce Business Technology modernized its network infrastructure by deploying SiteLink.\nThe main objectives were:\nBuild a flexible, scalable network as needed. Reduce operational costs and complexity. Enhance resilience and security. The team:\nDeployed SiteLink on existing Direct Connect connections. Created new dedicated VIFs for production and development environments. Maintained a global segmentation to meet on-premises data storage requirements. Figure 2. Sample global SiteLink deployment for Production and Development.\nAchieved Benefits The SiteLink solution delivered several outstanding benefits for Salesforce:\nBenefit Description Simplified network management Eliminated the complexity of MPLS Layer 3 VPN routing while maintaining traffic isolation. Improved performance Increased stability, reducing global average latency by 15%. Cost optimization Utilized existing connections with pay-per-use billing. Enhanced security Applied MACSec Layer 2 encryption on all Direct Connect connections. Additionally, SiteLink helps:\nReduce single points of failure (SPOF) and improve network reliability. Optimize data path routing between data centers. Comprehensive monitoring via CloudWatch Network Monitor. “With SiteLink, Salesforce Business Technology has streamlined network operations and ensured maximum resilience for global connectivity. We can set up connections between 7 data centers in just a few minutes and expand into new markets in a few days.”\n— Ravi Patel, Senior Technical Director at Salesforce.\nConclusion Adopting AWS Direct Connect SiteLink has helped Salesforce:\nUnify its global network architecture. Modernize infrastructure, reduce costs, and improve performance. Be ready for scaling and rapid innovation. To learn more about AWS Direct Connect SiteLink, you can refer to the official documentation or ask questions on AWS re:Post.\nAbout the Authors Alexandra Huides\nSenior Network Solutions Architect at AWS.\nSpecializes in large-scale network architecture, helping clients adopt IPv6 and build flexible environments. Outside of work, she enjoys kayaking, traveling, and reading books.\nCorey Harris Jr.\nSenior Solutions Architect at AWS.\nA network and serverless expert, helping customers optimize their AWS systems. Outside of work, he enjoys gaming, traveling, and spending time with family.\nGeorgi Stoev\nSenior Technical Architect at Salesforce.\nWith over 20 years of experience in networking, AI, and security, he is passionate about technology, beekeeping, and nature exploration.\nRavi Patel\nSenior Technical Director at Salesforce.\nWith over 15 years of experience in building flexible and high-performance networks, he enjoys surfing, mountaineering, and adventuring around the world.\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.2-week2/",
	"title": "Week 2 Worklog",
	"tags": [],
	"description": "",
	"content": " Week 2 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/5-workshop/5.2-architecture-walkthrough/",
	"title": "Architecture Walkthrough",
	"tags": [],
	"description": "",
	"content": "This section provides a high-level walkthrough of the overall architecture before diving into the hands-on steps.\nUser-facing Domain Frontend: Next.js hosted on AWS Amplify, fronted by Amazon CloudFront. Authentication: end users sign in via an Amazon Cognito User Pool. Operational database: PostgreSQL OLTP running on an EC2 instance in the public subnet. Connectivity: Amplify connects to the OLTP EC2 instance via the Internet Gateway using Prisma, with DATABASE_URL pointing to the public endpoint of the EC2 instance. In this design, the OLTP database is intentionally left in a public subnet to keep the connection from Amplify simple. In a production-grade system, OLTP would typically reside in private subnets behind an internal API layer.\nIngestion \u0026amp; Data Lake Domain The frontend sends HTTP events to Amazon API Gateway (HTTP API), route POST /clickstream.\nA Lambda Ingest function:\nValidates the incoming payload. Enriches metadata such as timestamps, user/session information, and product context. Writes raw JSON into an S3 Raw Clickstream bucket using a partitioned directory structure: s3://\u0026lt;raw-bucket\u0026gt;/events/YYYY/MM/DD/HH/events-\u0026lt;uuid\u0026gt;.json This pattern keeps raw data immutable and time-partitioned, which is convenient for batch ETL jobs.\nAnalytics \u0026amp; Data Warehouse Domain A VPC-enabled ETL Lambda runs inside a private subnet dedicated to analytics/ETL.\nThe ETL Lambda reads data from S3 via an S3 Gateway VPC Endpoint, then:\nCleans and normalizes events. Converts JSON logs into SQL-friendly analytical tables (facts and dimensions). Writes data into a PostgreSQL Data Warehouse hosted on an EC2 instance in a separate private subnet. An R Shiny Server runs on the same EC2 instance as the Data Warehouse and connects through localhost/private IP. It renders interactive dashboards on top of the DW schema.\nFigure 5-2: EC2 instances and basic CloudWatch metrics\nThe screenshot shows the two EC2 instances used in the workshop:\nSBW_EC2_Web_OLTP – the public instance that hosts the OLTP PostgreSQL database. SBW_EC2_Shiny_DW – the private instance that hosts the PostgreSQL Data Warehouse and the Shiny Server. Below the instance list, the EC2 console displays basic CloudWatch metrics such as CPU utilization and network traffic, which can be used to verify that the instances are healthy while the workshop labs are running.\nFigure 5-3: Overall Clickstream Analytics Architecture\nThe diagram illustrates:\nOutside the VPC: Amazon Cognito, Amazon CloudFront, AWS Amplify, Amazon API Gateway, Lambda Ingest, and Amazon EventBridge. Inside the VPC: Public Subnet – OLTP: EC2 PostgreSQL OLTP instance and the Internet Gateway. Private Subnet – Analytics: EC2 PostgreSQL Data Warehouse and R Shiny Server (no public IP). Private Subnet – ETL: VPC-enabled ETL Lambda function and the S3 Gateway VPC Endpoint. Numbered arrows (1)–(13) show the main flows: user login, browsing, clickstream ingestion, batch ETL, loading data into the DW, and Shiny visualization. Figure 5-4: VPC Network Layout (OLTP \u0026amp; Analytics)\nThe diagram illustrates a single VPC with two subnets:\nPublic Subnet – OLTP (10.0.1.0/24), highlighted in yellow, hosting the EC2 PostgreSQL OLTP instance and connected to the Internet Gateway. Private Subnet – Analytics (10.0.2.0/24), in green, hosting the EC2 PostgreSQL Data Warehouse, the R Shiny Server, the VPC-enabled ETL Lambda function, and the S3 Gateway VPC Endpoint (all with no public IP). The public subnet route table includes a default route:\n0.0.0.0/0 → Internet Gateway (IGW) The private subnet route table:\nHas 10.0.0.0/16 → local for internal VPC traffic. Includes a route to the S3 prefix list via the S3 Gateway VPC Endpoint. Does not have any 0.0.0.0/0 route and does not use a NAT Gateway. "
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/2-proposal/",
	"title": "Proposal",
	"tags": [],
	"description": "",
	"content": "Team Super Beast Warrior(SBW) Batch-based Clickstream Analytics Platform 1. Executive Summary This project aims to design and implement a Batch-based Clickstream Analytics Platform for a computer and accessories e-commerce website (The website’s frontend integrates a lightweight JavaScript SDK that sends user activity data (clicks, views, searches) to the backend API) using AWS Cloud Services. The system collects user interaction data (such as clicks, searches, and page visits) from the website and stores it in Amazon S3 as raw logs. Every hour, Amazon EventBridge triggers AWS Lambda functions to process and transform the data before loading it into a data warehouse hosted on Amazon EC2.\nThe processed data is visualized through R Shiny dashboards, providing store owners with business insights such as customer behavior patterns, product popularity, and website engagement trends.\nThis architecture focuses on batch analytics, ETL pipelines, and business intelligence while ensuring security, scalability, and cost efficiency by leveraging AWS managed services.\n2. Problem Statement What’s the Problem? E-commerce websites generate a large volume of clickstream data—including product views, cart actions, and search activities—that contain valuable business insights.\nHowever, small and medium-sized stores often lack the infrastructure and expertise to collect, process, and analyze this data effectively.\nAs a result, they face difficulties in:\nUnderstanding customer purchasing behavior Identifying top-performing products Optimizing marketing campaigns and website performance Making data-driven inventory and pricing decisions The Solution This project introduces an AWS-based batch clickstream analytics system that automatically collects user interaction data from the website every hour, processes it through serverless functions, and stores it in a central data warehouse on Amazon EC2.\nThe results are visualized using R Shiny dashboards, enabling store owners to gain actionable insights into customer behavior and improve overall business performance.\nBenefits and Return on Investment Data-driven decision making: Discover customer preferences, popular products, and shopping trends. Scalable and modular design: Easily extendable to handle more users or additional data sources. Cost-efficient batch processing: Reduces continuous compute costs by operating on a scheduled, hourly basis. Business insight enablement: Empowers store owners to optimize sales strategies and improve revenue using evidence-based analytics. 3. Solution Architecture AWS Services Used Amazon Cognito: Handles user authentication and authorization for both administrators and website customers, ensuring secure access to the e-commerce platform. Amazon S3: Acts as a centralized data storage layer — hosting the static website front-end and storing raw clickstream logs collected from user interactions. It also temporarily holds batch files before they are processed and transferred to the data warehouse. Amazon CloudFront: Distributes static website content globally with low latency, improving user experience and caching resources close to customers. Amazon API Gateway: Serves as the main entry point for incoming API calls from the website, enabling secure data submission (such as clickstream or browsing activity) into AWS. AWS Lambda: Executes serverless functions to preprocess and organize clickstream data uploaded to S3. It also handles scheduled data transformation jobs triggered by EventBridge before loading them into the data warehouse. Amazon EventBridge: Schedules and orchestrates batch workflows — for example, triggering Lambda functions every hour to process and move clickstream data from S3 into the EC2 data warehouse. Amazon EC2 (Data Warehouse): Acts as the data warehouse environment, running PostgreSQL or another relational database for batch analytics, trend analysis, and business reporting. Both instances are deployed inside a VPC private subnet for network isolation and security R Shiny (on EC2): Hosts interactive dashboards that visualize batch-processed insights, helping the business explore customer behavior, popular products, and sales opportunities. AWS IAM: Manages access permissions and policies to ensure that only authorized users and AWS components can interact with data and services. Amazon CloudWatch: Collects and monitors metrics, logs, and scheduled job statuses from Lambda and EC2 to maintain system reliability and performance visibility. Amazon SNS: Sends notifications or alerts when batch jobs complete, fail, or encounter errors, ensuring timely operational awareness. 4. Technical Implementation End-to-end data flow Auth (Cognito). Browser authenticates with Amazon Cognito (Hosted UI or JS SDK). ID token (JWT) is stored in memory; SDK attaches Authorization: Bearer \u0026lt;JWT\u0026gt; for API calls. Static web (CloudFront + S3). SPA/assets hosted on S3; CloudFront in front with OAC, gzip/brotli, HTTP/2, WAF managed rules. The page loads a tiny analytics SDK that collects events and sends to API Gateway (below). Event ingest (API Gateway). POST /v1/events (HTTP API). CORS locked to site origin; JWT authorizer validates Cognito token (or API key for anon flows). Requests forwarded to Lambda. Security \u0026amp; Ops. IAM least-privilege for every component. CloudWatch logs/metrics/alarms on API 5xx, Lambda errors, throttles, Shiny health. SNS notifies on alarms \u0026amp; DLQ growth. Processing \u0026amp; storage (Lambda → S3 batch buffer → EventBridge → Lambda(ETL) → PostgreSQL on EC2 (data warehouse) → Shiny). Ingest Lambda validates/enriches events then append-writes NDJSON objects into S3 (partitioned by date/hour). EventBridge (cron) triggers an ETL Lambda (batch) on a fixed cadence (e.g., every 60 minutes). ETL Lambda reads a slice of S3 partitions, deduplicates, transforms, and upserts into PostgreSQL on EC2 (VPC access). R Shiny Server (on EC2) reads curated tables and renders dashboards for admins. Data Contracts \u0026amp; Governance Event JSON (ingest)\n{ \u0026#34;event_id\u0026#34;: \u0026#34;uuid-v4\u0026#34;, \u0026#34;ts\u0026#34;: \u0026#34;2025-10-18T12:34:56.789Z\u0026#34;, \u0026#34;event_type\u0026#34;: \u0026#34;view|click|search|add_to_cart|checkout|purchase\u0026#34;, \u0026#34;session_id\u0026#34;: \u0026#34;uuid-v4\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;cognito-sub-or-null\u0026#34;, \u0026#34;anonymous_id\u0026#34;: \u0026#34;stable-anon-id\u0026#34;, \u0026#34;page_url\u0026#34;: \u0026#34;https://site/p/123\u0026#34;, \u0026#34;referrer\u0026#34;: \u0026#34;https://google.com\u0026#34;, \u0026#34;device\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;mobile|desktop|tablet\u0026#34; }, \u0026#34;geo\u0026#34;: { \u0026#34;country\u0026#34;: \u0026#34;VN\u0026#34;, \u0026#34;city\u0026#34;: null }, \u0026#34;ecom\u0026#34;: { \u0026#34;product_id\u0026#34;: \u0026#34;sku-123\u0026#34;, \u0026#34;category\u0026#34;: \u0026#34;Shoes\u0026#34;, \u0026#34;currency\u0026#34;: \u0026#34;USD\u0026#34;, \u0026#34;price\u0026#34;: 79.99, \u0026#34;qty\u0026#34;: 1, }, \u0026#34;props\u0026#34;: { \u0026#34;search_query\u0026#34;: \u0026#34;running shoes\u0026#34; }, } PII: never send name/email/phone; any optional identifier is hashed in Lambda. Behaviour: generate anonymous_id once, maintain session_id (roll after 30 min idle), send via navigator.sendBeacon with fetch retry fallback; optional offline buffer via IndexedDB. S3 raw layout \u0026amp; retention\nBucket: s3://clickstream-raw/ Object format: NDJSON, optionally GZIP. Partitioning: year=YYYY/month=MM/day=DD/hour=HH/ → events-\u0026lt;uuid\u0026gt;.ndjson.gz Optional manifest per batch: processed watermark, object list, record counts, hash. Lifecycle: raw → (30 days Standard/IA) → (365+ days Glacier/Flex). Idempotency: maintain a compact staging table in PostgreSQL (or a small S3 key-value manifest) to track last processed object/batch and prevent double-load. Frontend SDK (Static site on S3 + CloudFront) Instrumentation\nTiny JS snippet loaded site-wide (defer). Generates anonymous_id once and keeps session_id in localStorage; session rolls after 30 minutes of inactivity. Sends events via navigator.sendBeacon; fallback to fetch with retry \u0026amp; jitter. Auth context\nIf user signs in with Cognito, include user_id = idToken.sub to enable logged-in funnels. Offline durability\nOptional Service Worker queue: when offline, buffer events in IndexedDB and flush on reconnect. Ingestion API (API Gateway → Lambda) API Gateway (HTTP API)\nRoute: POST /v1/events. JWT authorizer (Cognito user pool). For anonymous pre-login events, use an API key usage-plan with strict rate limits. WAF: AWS Managed Core + Bot Control; block non-site origins via strict CORS. Lambda (Node.js or Python)\nValidate against JSON Schema (ajv/pydantic). Idempotency: recent event_id cache in memory (short TTL) + batch-level dedupe during ETL. Enrichment: derive date/hour, parse UA, infer country from CloudFront-Viewer-Country if present. Persist: PutObject to S3 path .../year=YYYY/month=MM/day=DD/hour=HH/.... Failure path: publish to SQS DLQ; alarm via SNS if DLQ depth \u0026gt; 0. Batch Buffer (S3) Purpose: cheap, durable buffer for batch analytics. Write pattern: small per-request objects or micro-batches (e.g., 1–5 MB each) with GZIP. Optional compactor merges into ≥64MB files for efficient reads. Read pattern: ETL Lambda scans only new partitions/objects since the last watermark. Schema-on-read: ETL applies schema, handles late-arriving data by reprocessing a small sliding window (e.g., last 2 hours) to correct sessions. EC2 “data warehouse” node Purpose: run ETL + host the curated analytical store that Shiny queries. Two choices:\nPostgres on EC2 (recommended if team prefers SQL/window functions) Instance: t3.small/t4g.small; gp3 50–100GB. Schema: fact_events, fact_sessions, dim_date, dim_product. Security: within VPC private subnet; access via ALB/SSM Session Manager; automated daily snapshots to S3 ETL (Lambda, batch via EventBridge cron): Trigger: rate(5 minutes) / cron(\u0026hellip;) depending on cost \u0026amp; freshness. Steps: list new S3 objects → read → validate/dedupe → transform (flatten nested JSON, cast types, add ingest_date, session_window_start/end) → upsert into Postgres using COPY to temp tables + merge, hoặc batched INSERT \u0026hellip; ON CONFLICT. Networking: Lambda attached to VPC private subnets to reach EC2 Postgres security group. R Shiny Server on EC2 (admin analytics) Server\nEC2 (t3.small/t4g.small) with: R 4.4+, Shiny Server (open-source), Nginx reverse proxy, TLS via ACM/ALB or Let’s Encrypt. IAM instance profile (no static keys). Security group allows HTTPS from office/VPN or Cognito-gated admin site. App (packages)\nshiny, shinydashboard/bslib, plotly, DT, dplyr, DBI + RPostgres or duckdb, lubridate. If querying DynamoDB directly for small cards, use paws.dynamodb (optional). Dashboards\nTraffic \u0026amp; Engagement: DAU/MAU, sessions, avg pages, bounce proxy. Funnels: view→add_to_cart→checkout→purchase with stage conversion \u0026amp; drop-off. Product Performance: views, CTR, ATC rate, revenue by product/category. Acquisition: referrer, campaign, device, country. Reliability: Lambda error rate, DLQ depth, ETL lag, data freshness. Caching\nQuery results cached in-process (reactive values) or materialized by ETL; cache keys by date range and filters Security baseline IAM\nIngest Lambda: s3:PutObject to raw bucket (scoped to prefix), s3:ListBucket on needed prefixes. ETL Lambda: s3:GetObject/ListBucket on raw prefixes; permission to fetch secrets from SSM Parameter Store; no broad S3 access. EC2 roles: read/write only to its own DB/volumes; optional read to S3 for backups. Shiny EC2: no write to S3 raw; read-only to Postgres as needed. Network\nPlace EC2 in private subnets; public access through ALB (HTTPS 443). Lambda for ETL joins the VPC to reach Postgres; SG rules least-priv (Postgres port from ETL SG only). No wide 0.0.0.0/0 to DB ports. Data\nEncrypt EBS (KMS), S3 server-side encryption, RDS/PG TLS, secrets in SSM Parameter Store. No PII in events; retention: raw S3 90–365 days (lifecycle), curated Postgres per business policy Observability \u0026amp; alerting CloudWatch metrics/alarms\nAPI Gateway 5xx/latency, Lambda (ingest) errors/throttles, S3 PutObject failures, EventBridge schedule success rate, ETL duration/lag, DLQ depth, Shiny health check SNS topics: on-call email/SMS/Slack webhook.\nStructured logs: JSON logs from Lambda \u0026amp; ETL (request_id, event_type, status, ms, error_code).\nWatermark tracking: custom metric “DW Freshness (minutes since last successful upsert)”.\nCost Controls (stay near Free/low tier) Use HTTP API (cheaper), minimal Lambda memory (256–512MB), compress requests. Batch over realtime: S3 as buffer eliminates DynamoDB write/read costs. S3 lifecycle: Standard → Standard-IA/Intelligent-Tiering → Glacier for older raw; enable GZIP to cut storage/transfer. Tune ETL cadence (e.g., 15–60 min) and process only new objects; compact small files into bigger chunks to reduce read I/O. Single small EC2 for Shiny + DW at start; scale vertically or split later. AWS Budgets with SNS alerts (actual \u0026amp; forecast). Deliverables Analytics SDK (TypeScript) with sessionization + beacon + optional offline queue. API/Lambda (ingest) with validation, enrichment, idempotency hints, DLQ. S3 raw bucket spec (prefixing/partitioning, compression, lifecycle) + optional compactor. ETL Lambda (batch) + EventBridge cron + watermarking + upsert strategy to PostgreSQL. PostgreSQL schema (fact_events, fact_sessions, dims) + indexes + vacuum/maintenance plan. R Shiny dashboard app (5 modules) + Nginx/ALB TLS setup. Runbook: alarms, on-call, backups, disaster recovery, freshness SLO, cost guardrails 5. Timeline \u0026amp; Milestones Project Timeline Month 1 – Learning \u0026amp; Preparation Study a wide range of AWS services including compute, storage, analytics, and security. Understand key concepts of cloud architecture, data pipelines, and serverless computing. Conduct team meetings to align project goals and assign responsibilities.\nMonth 2 – Architecture Design \u0026amp; Prototyping Design the overall project architecture and define data flow between components. Set up initial AWS resources such as S3, Lambda, API Gateway, EventBridge, and EC2. Experiment with open-source tools for visualization and reporting. Test sample codes and validate the data ingestion and processing pipeline.\nMonth 3 – Implementation \u0026amp; Testing Implement the full architecture based on the approved design. Integrate all AWS services and ensure system reliability. Conduct performance and functionality testing. Finalize documentation and prepare the project for presentation.\n6. Budget Estimation You can find the budget estimation on the AWS Pricing Calculator. Or you can download the Budget Estimation File.\nInfrastructure Costs AWS Services\nAmazon Cognito(User Pools): 0.10 USD/monthly(1 Number of monthly active users (MAU), 1 Number of monthly active users (MAU) who sign in through SAML or OIDC federation)\nAmazon S3\n3 Standard:0.17 USD/monthly(6 GB, 1,000 PUT requests, 1,000 GET requests, 6 GB Data returned, 6 GB Data scanned) Data Transfer: 0.00 USD/monthly(Outbound: 6 TB, Inbound: 6 TB) Amazon CloudFront(United States): 0.64 USD/monthly(6 GB Data transfer out to internet, 6 GB Data transfer out to origin, 10,000 requests Number of requests (HTTPS))\nAmazon API Gateway(HTTP APIs): 0.01 USD/monthly(10,000 requests for HTTP API requests units)\nAmazon Lambda(Service settings): 0.00 USD/monthly(1,000,000 requests, 512 MB)\nAmazon CloudWatch(APIs): 0.03 USD/monthly(100 metrics GetMetricData, 1,000 metrics GetMetricWidgetImage, 1,000 requests API)\nAmazon SNS(Service settings): 0.02 USD/monthly(1,000,000 requests, 100,000 calls HTTP/HTTPS Notifications, 1,000 calls EMAIL/EMAIL-JSON Notifications, 100,000,000 notifications QS Notifications, 100,000,000 deliveries Amazon Web Services Lambda, 100,000 notifications Amazon Kinesis Data Firehose)\nAmazon EC2(EC2 specifications): 1.68 USD/monthly(1 instances, 730 Compute Savings Plans)\nAmazon EventBridge: 0.00 USD/monthly(1,000,000 events(Number of AWS management events) EventBridge Event Bus - Ingestion)\nTotal: 2.65 USD/month, 31.8 USD/12 months\n7. Risk Assessment Risk Likelihood Impact Mitigation Strategy High costs exceeding the estimated budget Medium High Closely monitor and calculate all potential AWS expenses. Limit the use of high-cost AWS services and replace them with simpler, cost-effective alternatives that provide similar functionality. Potential issues during data transfer or service integration between AWS components Medium Medium Perform step-by-step validation before going live. Conduct early testing, use managed AWS services, and continuously monitor performance through Amazon CloudWatch. Data collection or processing risks (e.g., excessive user interactions, network instability, missing or duplicated events) High Medium Apply data validation, temporary buffering, and schema enforcement to ensure consistency. Use structured logging and alarms to detect and resolve ingestion errors. Low or no user adoption of the analytics dashboard Low High Conduct internal training sessions and leverage existing communication channels to raise awareness. Encourage adoption by showcasing the system’s practical benefits and actionable insights. 8. Expected Outcomes Understanding Customer Behavior and Journey The system records the entire customer journey — including which pages users visit, which products they view, how long they stay, and where they exit the site.\nBy analyzing session duration, bounce rate, and navigation paths, businesses can evaluate user engagement and the overall experience.\nThis provides a reliable data foundation for improving website interface, optimizing page layout, and enhancing overall customer satisfaction.\nIdentifying Popular Products and Consumer Trends Based on clickstream data collected and processed in AWS, the system identifies the most viewed and most purchased products.\nProducts that receive less attention are also tracked, enabling businesses to assess the effectiveness of product listings, adjust pricing or visuals, and plan inventory more effectively.\nFurthermore, the system supports discovering shopping trends across time periods, regions, or device types — allowing for data-driven and timely business decisions.\nOptimizing Marketing and Sales Strategies Customer behavior data is transformed into business insights and presented through R Shiny dashboards.\nWith these analytical results, businesses can:\nAccurately define target customer segments for marketing efforts Customize advertising and promotional campaigns for specific product groups or demographics Evaluate the effectiveness of marketing initiatives through measurable engagement and conversion indicators As a result, marketing and sales strategies become more evidence-based and precise, supporting better decision-making and improved business performance.\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/3-blogstranslated/3.3-blog3/",
	"title": "Blog 3",
	"tags": [],
	"description": "",
	"content": " How TCS\u0026rsquo;s Smart Power Plant Solution on AWS Helps Utilities Optimize Operations and Drive Energy Transition By Alakh Srivastava, Rajesh Natesan, Siva Thangavel, and Yogesh Chaturvedi – March 19, 2025, in the Amazon DocumentDB, Amazon ECS, Amazon S3, AWS IoT Core, AWS Step Functions, Energy (Oil \u0026amp; Gas), Industries section.\nSolution Overview Advanced digital technologies are revolutionizing the energy industry, enabling organizations to achieve sustainability goals while reducing costs and carbon emissions.\nAccording to McKinsey, digital transformation in the energy sector could unlock $1.6 trillion in value by 2035, helping reduce 20–30% of operational costs and 5% of carbon emissions.\nAs the industry moves towards a distributed power generation model integrated with renewable energy, businesses require smart solutions such as digital grids, AI-driven energy coordination, and real-time monitoring platforms.\nThe TCS Smart Power Plant solution was developed to address these needs — delivering a 0.5% increase in performance, an 8% reduction in NOx, and improving 8–10% accuracy in renewable power generation forecasts.\nBuilt on the AWS platform, the solution leverages the power of AI/ML to process real-time data from thousands of energy sensors across multiple locations.\nThis article explains how TCS and AWS collaborate to deliver superior operational efficiency and sustainable business outcomes for the energy industry.\nSolution Architecture and Data Flow The solution architecture is designed with a closed-loop data flow, utilizing AWS services to manage, process, and analyze information comprehensively.\nFigure 1. Overall architecture of the Smart Power Plant solution on AWS.\nData Ingestion: Data is collected from OPC-UA (industrial devices), on-premise historical systems, and Amazon S3 data lakes.\nEach unit can send up to 4,000 sensor values per minute. Ingestion and Orchestration: AWS IoT Core receives the data stream and triggers AWS Step Functions for automated orchestration. Data Processing: AWS Lambda functions perform data cleaning, calculate KPIs, and generate alerts. Storage: Amazon DocumentDB stores structured data (KPIs, alerts), Amazon S3 stores raw sensor data and training results. ML Model Training: Performed in Amazon SageMaker, with models stored in Amazon Elastic Container Registry (ECR). Real-Time Inference: Models are deployed through Amazon ECS for TCS InTwin (online analytics engine). Application Deployment: Front-end/back-end interfaces run containers on Amazon ECS, ensuring flexible scalability. Key Capabilities The TCS Smart Power Plant solution provides four core capabilities that transform how plants operate:\nFigure 2. Four core capabilities of the solution.\nSelf-learning Digital Twin (AI):\nCombines real data and physical AI models to continuously adapt to operational conditions, ensuring accurate predictions and cost savings.\nOpen and Scalable Solution:\nCan integrate with existing plant systems or proprietary AI models, with an open and explainable architecture.\nLow-code Digital Workbench:\nEnables rapid creation and management of AI models, supporting the creation of KPIs, FMEA, and specific use cases.\nPre-built Platform:\nConfigurable modules for each plant, reducing deployment time and enabling easy scaling.\nReal-world Use Cases Solar Power Generation Forecasting Using ML models and advanced analytics to predict renewable energy generation.\nAt an offshore wind farm in the UK, forecasting accuracy improved by 15.1%, leading to a 6% revenue increase.\nCombustion Optimization in Heat Generation At a Japanese plant, AI improved 0.5% performance, reduced 8% NOx, and saved $2.5 million/year.\nPredictive Maintenance of Gas Turbine Components At an Australian plant, models predicted failures 8–12 months in advance, reducing maintenance costs by 20% and downtime.\nBusiness Benefits TCS\u0026rsquo;s solution helps energy enterprises:\nBenefit Impact Reduce Operational Costs Cut maintenance and operational costs by up to 20%. Accurate Failure Prediction Achieve error prediction accuracy up to 85%. Optimize KPIs and Reduce Emissions Improve performance while reducing carbon emissions. Support Workforce AI assists decision-making, reducing dependence on individual experience. Additionally, AWS integration eliminates data silos, boosts productivity, and provides a standardized platform for future power plants.\nConclusion The TCS Smart Power Plant solution on AWS is shaping the sustainable future of the energy industry.\nThrough AI and advanced analytics, this platform helps optimize performance, predictive maintenance, and seamlessly integrate renewable energy.\nTCS — with deep expertise and a team of AWS-certified specialists — has demonstrated its ability to deploy successfully across various types of plants, from traditional thermal plants to large-scale renewable energy.\nTo learn more, please see the original TCS post on the Smart Power Plant solution on AWS.\nAbout the Authors Alakh Srivastava\nGlobal Product Director – Smart Power Plant Practice, TCS.\nOver 20 years of experience in digital transformation in the power industry, specializing in renewable energy, AI, and industrial IoT.\nRajesh Natesan\nChief Technical Group Leader – Smart Power Plant Group, TCS.\n20 years of experience in IoT, AI/ML, and large-scale energy system architecture.\nSiva Thangavel\nPartner Solution Architect at AWS.\nProvides optimal architectural solutions for partners and enterprise customers across multiple industries.\nYogesh Chaturvedi\nPrincipal Solution Architect at AWS – Energy and Utilities.\nFocuses on helping customers solve challenges using cloud technology. Outside work, he enjoys hiking, traveling, and sports.\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/3-blogstranslated/",
	"title": "Translated Blogs",
	"tags": [],
	"description": "",
	"content": "Blog 1 - How iFood built a platform to run hundreds of machine learning models with Amazon SageMaker Inference This blog introduces how iFood uses Amazon SageMaker Inference to operate hundreds of machine learning models at scale. The post explains how iFood automates model training, deployment, and monitoring processes while optimizing cost and performance with features such as zero-scale endpoints and multi-model GPU serving.\nBlog 2 - How Salesforce Business Technology uses AWS Direct Connect SiteLink for reliable global connectivity This blog describes how Salesforce Business Technology implements AWS Direct Connect SiteLink to build a reliable global network architecture. The post shares how SiteLink helps Salesforce unify network connectivity across seven locations, reduce latency, enhance security, and simplify operations by leveraging the AWS global backbone.\nBlog 3 - How TCS Smart Power Plant on AWS helps utilities optimize operations and accelerate energy transformation This blog introduces how Tata Consultancy Services (TCS) deploys the Smart Power Plant on AWS, helping energy companies optimize performance, reduce emissions, and drive sustainable energy transformation. The solution uses AI/ML, IoT, and digital twin technologies to analyze real-time data, predict failures, and optimize power generation. The post also presents real-world use cases such as renewable energy forecasting, combustion optimization, and predictive maintenance, delivering economic efficiency and carbon reduction.\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.3-week3/",
	"title": "Week 3 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 3 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/5-workshop/5.3-implementing-clickstream-ingestion/",
	"title": "Implementing Clickstream Ingestion",
	"tags": [],
	"description": "",
	"content": "This section describes in detail how clickstream events are generated in the browser, sent to Amazon API Gateway, processed by the Lambda Ingest function, and finally stored as raw JSON files in the S3 Raw Clickstream bucket.\nThe ingestion path is the entry point of the entire analytics platform: if events are not captured or stored correctly at this stage, the downstream ETL and dashboards will not be reliable.\nFrontend event generation The e-commerce frontend is implemented using Next.js and hosted on AWS Amplify. A client-side tracking module is responsible for collecting user actions and sending them as clickstream events.\nTypical actions that should be captured include:\nPage views: landing page, category pages, search results, product detail pages. Product interactions: clicking on a product card, viewing product details, adding or removing items from the cart. Checkout steps: starting the checkout, filling in shipping information, placing an order. Session and identity information: user ID (if logged in), session ID, login state, client ID. The frontend tracking code usually performs the following steps:\nListens for events in the UI (e.g., page load, button click).\nBuilds a JSON payload with at least:\nevent_id – unique identifier for the event. event_name – e.g., \u0026quot;page_view\u0026quot;, \u0026quot;product_view\u0026quot;, \u0026quot;add_to_cart\u0026quot;. event_timestamp – client-side timestamp (ISO 8601 or epoch). user_id / identity_source – if the user is logged in (e.g., Cognito user sub). session_id / client_id – session or browser identifier. product_id, product_name, product_category, etc., for product-related events. page_url, referrer, utm_* fields for traffic attribution (optional). Sends the JSON payload via HTTPS to the API Gateway endpoint (see Section 5.3.2).\nExample of a minimal JSON payload sent from the browser:\n{ \u0026#34;event_name\u0026#34;: \u0026#34;product_view\u0026#34;, \u0026#34;event_timestamp\u0026#34;: \u0026#34;2025-12-04T15:23:45.123Z\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;user_123\u0026#34;, \u0026#34;login_state\u0026#34;: \u0026#34;logged_in\u0026#34;, \u0026#34;session_id\u0026#34;: \u0026#34;session_abc\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;browser_xyz\u0026#34;, \u0026#34;product_id\u0026#34;: \u0026#34;SKU-12345\u0026#34;, \u0026#34;product_name\u0026#34;: \u0026#34;Gaming Laptop 15 inch\u0026#34;, \u0026#34;product_category\u0026#34;: \u0026#34;Laptops\u0026#34;, \u0026#34;product_price\u0026#34;: 1299.0, \u0026#34;page_url\u0026#34;: \u0026#34;/products/sku-12345\u0026#34; } The payload can be extended later as the analytics requirements evolve.\nAPI Gateway and Lambda Ingest flow API Gateway HTTP API The ingestion endpoint is exposed using Amazon API Gateway (HTTP API) with a route such as:\nMethod: POST Path: /clickstream Key characteristics:\nThe endpoint accepts JSON payloads from the browser. CORS is configured to allow the frontend domain (CloudFront / Amplify) to call it. The route is integrated with a Lambda function (Lambda proxy integration). Figure 5-5: API Gateway route for POST /clickstream\nThe screenshot shows the HTTP API configuration for the clickstream ingestion endpoint.\nThe /clickstream resource exposes a single POST route, which is integrated with the clickstream-ingest Lambda function.\nNo authorizer is attached in this lab to keep the workshop focused on data ingestion rather than authentication.\nLambda Ingest function The Lambda Ingest function is responsible for the following:\nParsing the request\nReads the request body from the API Gateway event. Validates that the body contains a valid JSON object or array of objects. Basic validation \u0026amp; enrichment\nEnsures mandatory fields (e.g., event_name, event_timestamp) are present. If the client timestamp is missing, attaches a server-side timestamp. Adds metadata such as: The API Gateway request ID. The source IP or user agent (if needed). Batching and writing to S3\nConstructs a key in the Raw Clickstream bucket using a time-based partition pattern, for example:\ns3://\u0026lt;raw-bucket\u0026gt;/events/YYYY/MM/DD/HH/events-\u0026lt;uuid\u0026gt;.json Writes the incoming events as a JSON array or NDJSON (newline-delimited JSON), depending on the chosen format.\nLogging and error handling\nLogs validation errors or malformed events to CloudWatch Logs. Returns an appropriate HTTP status code back to API Gateway (e.g., 200 on success, 400 for invalid payload). Example S3 object key for an event batch captured on 4 December 2025 at 15:00:\nevents/2025/12/04/15/events-a1b2c3d4.json Figure 5-6: Lambda Ingest function overview\nThe screenshot shows the clickstream-lambda-ingest function in the AWS Lambda console.\nThe diagram highlights that the function is triggered by the HTTP API Gateway and uses a lightweight configuration (128 MB memory, short timeout) that is sufficient for validating JSON payloads, enriching metadata, and writing batched events to the S3 Raw Clickstream bucket.\nManual testing from the frontend and Postman To confirm that ingestion is working as expected, two complementary tests are performed.\nEnd-to-end test from the real frontend Open the CloudFront domain of the e-commerce application, for example:\nhttps://dxxxxxxxx.cloudfront.net Sign in using Amazon Cognito with a test user account.\nPerform a sequence of actions, such as:\nOpen the home page and a category page. View the details of two or three products. Add at least one product to the cart and then remove it. Start the checkout flow and proceed to the final confirmation step (placing a real order is optional). Using the browser’s developer tools (Network tab), verify that:\nRequests are being sent to POST /clickstream. The request body contains the expected JSON fields such as event_name, product_id, session_id, etc. The response status code from API Gateway is 200. Direct test using Postman or Thunder Client For controlled testing, you can also send synthetic events using an API client:\nMethod: POST\nURL:\nhttps://\u0026lt;api-id\u0026gt;.execute-api.\u0026lt;region\u0026gt;.amazonaws.com/clickstream Header:\nContent-Type: application/json Body:\n{ \u0026#34;event_name\u0026#34;: \u0026#34;page_view\u0026#34;, \u0026#34;event_timestamp\u0026#34;: \u0026#34;2025-12-04T16:00:00.000Z\u0026#34;, \u0026#34;user_id\u0026#34;: \u0026#34;test_user_manual\u0026#34;, \u0026#34;login_state\u0026#34;: \u0026#34;anonymous\u0026#34;, \u0026#34;session_id\u0026#34;: \u0026#34;session_manual_001\u0026#34;, \u0026#34;client_id\u0026#34;: \u0026#34;postman_client\u0026#34;, \u0026#34;page_url\u0026#34;: \u0026#34;/manual-test\u0026#34; } This approach is helpful to test error handling, validation rules, or edge cases without depending on the frontend code.\nInspecting data in the S3 Raw Clickstream bucket Once events have been sent, the next step is to verify that they were successfully stored in S3.\nOpen the Amazon S3 Console and select the Raw Clickstream bucket, for example:\nclickstream-raw-\u0026lt;account\u0026gt;-\u0026lt;region\u0026gt; Navigate through the prefix structure:\nevents/YYYY/MM/DD/HH/ For example:\nevents/2025/12/04/15/ Confirm that one or more objects with names similar to:\nevents-a1b2c3d4.json events-f9e8d7c6.json have been created.\nDownload one of these files and open it with a text editor (VS Code, Notepad++, etc.):\nVerify that the JSON structure is valid. Check that fields such as event_name, event_timestamp, user_id, session_id, product_id, etc., match the actions you performed on the website. Ensure that multiple events are grouped logically (for example, as an array of JSON objects). Take note of:\nThe approximate file size (KB/MB). The number of events per file. How frequently new files appear (depending on how the Ingest Lambda batches writes). These observations will help when tuning ETL batch sizes and scheduling in later sections.\nFigure 5-7: Raw clickstream JSON objects in the S3 bucket\nThe screenshot shows the S3 Raw Clickstream bucket at the deepest hour-level prefix.\nMultiple JSON objects have been created by the Lambda Ingest function, each representing a batch of clickstream events captured during that hour.\nThe file names follow a UUID-based pattern (for example, event-a8bdf0c0-...json), and the Size column gives a quick indication of how many events are grouped in each file.\nSummary and relation to downstream components At the end of this section, you should have:\nA working ingestion path from browser → API Gateway → Lambda Ingest → S3 Raw bucket. Verified that events are correctly structured and contain the necessary metadata. Confirmed that the Raw Clickstream bucket uses a time-partitioned layout that is suitable for batch processing. In the next section (5.4), the focus will shift to the private analytics layer, where a VPC-enabled ETL Lambda reads these raw JSON files via an S3 Gateway VPC Endpoint, transforms them, and loads them into the PostgreSQL Data Warehouse.\nFigure 5-8: Ingestion flow from Browser to S3 Raw bucket\nThe diagram illustrates the sequence:\nBrowser (user actions) → CloudFront → Amplify (Next.js app). Frontend tracking sends HTTP requests to Amazon API Gateway (HTTP API) on the POST /clickstream route. API Gateway invokes Lambda Ingest, which validates and enriches events. Lambda Ingest writes batched JSON objects into the S3 Raw Clickstream bucket under a time-partitioned key such as events/YYYY/MM/DD/HH/events-\u0026lt;uuid\u0026gt;.json. "
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/4-eventparticipated/",
	"title": "Events Attended",
	"tags": [],
	"description": "",
	"content": "Event 1 Event name: Vietnam Cloud Day 2025\nDate: 18/09/2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Ben Nghe Ward, District 1, Ho Chi Minh City\nRole in the event: Attendee\nBrief description of main content and activities:\nVietnam Cloud Day is an annual event organized by AWS, bringing together experts, enterprises, and the tech community to share the latest trends in cloud computing, AI, big data, and security.\nThe program includes strategic discussion sessions, industry-specific tracks (FSI, telecommunications, retail, real estate, etc.), technical sessions on modern architectures (microservices, event-driven, serverless), and real-world case study presentations from Techcombank, Honda, Masterise, TymeX, etc., providing a comprehensive view of the modernization journey on AWS.\nOutcomes / value gained (lessons learned, new skills, contributions to team/project):\n- Gained a clearer understanding of the role of Cloud \u0026amp; GenAI in the national digital transformation strategy and in key industries.\n- Learned about modern architectural models such as DDD, microservices, event-driven, serverless, and how to choose appropriate compute services (EC2, ECS, Fargate, Lambda).\n- Developed deeper awareness of the importance of data strategy, “security by design”, and the “migrate to operate” mindset instead of just “lift-and-shift”.\n- Generated many ideas to apply to academic/personal projects (e.g., designing systems on AWS, integrating Amazon Q Developer into the DevOps workflow) and expanded my network with the cloud engineering community.\nEvent 2 Event name: GenAI-powered App-DB Modernization workshop\nDate: 07/11/2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Sai Gon Ward, Ho Chi Minh City\nRole in the event: Attendee\nBrief description of main content and activities:\nThe workshop focused on Agentic AI and the Amazon QuickSuite ecosystem, clarifying the difference between traditional Generative AI and Agentic AI that is capable of autonomous action.\nParticipants were introduced for the first time to QuickSuite in Vietnam, including the combination of Amazon QuickSight and QuickSuite Q to build “Analyst Agents” that can read data, answer business questions, and recommend appropriate actions.\nThe program also introduced the AWS LIFT financial support package (credits of up to USD 80,000) to reduce cost barriers when experimenting with AI/Agentic AI solutions.\nA ~90-minute hands-on session allowed participants to work directly with QuickSight + QuickSuite Q under the guidance of AWS experts and Cloud Kinetics, and to network with the cloud, data, and AI community.\nOutcomes / value gained (lessons learned, new skills, contributions to team/project):\n- Gained a clearer understanding of Agentic AI, the shift from “AI that only answers” to “AI that actually works on behalf of humans”, and practical use cases in enterprises.\n- Learned the main components of Amazon QuickSuite, and how to combine QuickSight and QuickSuite Q to build data analysis agents (analyst agents).\n- Understood the role of the AWS LIFT program in financially supporting AI PoCs/R\u0026amp;D, enabling future proposals for related projects.\n- Improved skills in designing Agentic AI use cases (selecting repetitive, multi-step, data-driven processes) and generated more ideas to apply to study/projects, such as automated reporting, metric monitoring, or operational optimization recommendations.\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.4-week4/",
	"title": "Week 4 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 4 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/5-workshop/5.4-building-the-private-analytics-layer/",
	"title": "Building the Private Analytics Layer",
	"tags": [],
	"description": "",
	"content": "This section explains in detail how the private analytics layer is implemented so that batch ETL can run entirely inside private subnets, without exposing internal components to the public Internet.\nThe private analytics layer consists of:\nA VPC-enabled ETL Lambda function running in a private subnet. An S3 Gateway VPC Endpoint that allows the ETL Lambda to access S3 over the AWS private network. A PostgreSQL Data Warehouse running on an EC2 instance in a separate private subnet. Together, these components form the backbone of the batch-processing pipeline:\nS3 Raw Clickstream bucket → ETL Lambda (in VPC) → PostgreSQL Data Warehouse (EC2, private subnet)\nNetworking design for the analytics layer The VPC is logically divided into subnets with distinct roles:\nPublic Subnet – OLTP (10.0.1.0/24)\nHosts the OLTP PostgreSQL EC2 instance. Has a route 0.0.0.0/0 → Internet Gateway (IGW) for inbound/outbound Internet access. Private Subnet – Analytics (10.0.2.0/24)\nHosts the Data Warehouse EC2 instance and the R Shiny Server. Has no route to the Internet Gateway and no NAT Gateway. Only local VPC routes plus internal connectivity to other private subnets. Private Subnet – ETL (10.0.3.0/24)\nHosts the VPC-enabled ETL Lambda and the S3 Gateway VPC Endpoint. Also has no 0.0.0.0/0 route, and no NAT Gateway. Can reach S3 privately through the Gateway Endpoint. This design ensures that:\nThe Data Warehouse and R Shiny are not directly reachable from the public Internet. The ETL Lambda can access S3 and the Data Warehouse only over private AWS networking. There is no NAT Gateway, which reduces cost and simplifies the network footprint. Creating and configuring the S3 Gateway VPC Endpoint The S3 Gateway VPC Endpoint allows resources in private subnets to reach S3 without using public IPs.\nFigure 5-9: S3 Gateway VPC Endpoint attached to private route tables\nThe screenshot shows the Gateway VPC Endpoint for the Amazon S3 service (com.amazonaws.ap-southeast-1.s3) in the workshop VPC.\nThe endpoint is in the Available state and is associated with the two private route tables, which are in turn attached to the Analytics and ETL subnets.\nThis configuration ensures that traffic from the ETL Lambda and the Data Warehouse instance to S3 stays inside the AWS network and does not require a NAT Gateway.\nStep 1 – Create the Gateway Endpoint Open the VPC Console in the AWS Management Console.\nNavigate to Endpoints → click Create endpoint.\nUnder Service category, select AWS services.\nIn the search box, type s3 and select the entry:\ncom.amazonaws.\u0026lt;region\u0026gt;.s3 For Endpoint type, select Gateway.\nFor VPC, choose the project VPC that contains your analytics and ETL subnets.\nUnder Route tables, select the route table associated with the ETL private subnet (10.0.3.0/24) and, optionally, the Analytics private subnet (10.0.2.0/24) if you also want the DW EC2 instance to access S3 privately.\nIn the Policy section, you can start with Full access for the workshop:\n{ \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Resource\u0026#34;: \u0026#34;*\u0026#34; } ] } Later, this can be restricted to a specific bucket or prefix.\nClick Create endpoint.\nStep 2 – Verify route table entries After the endpoint is created, AWS automatically adds routes to the selected route tables.\nIn the VPC Console, open Route tables.\nSelect the route table used by the ETL private subnet.\nOn the Routes tab, verify that:\nThere is a local route:\n10.0.0.0/16 → local There is a prefix-list route to S3​, pointing to the new endpoint:\npl-xxxxxxxx → vpce-xxxxxxxx (Gateway Endpoint to S3) There is no route:\n0.0.0.0/0 → igw-xxxxxxx There is no NAT Gateway target.\nThis confirms that the private subnets do not send traffic directly to the Internet, but can reach S3 through the Gateway Endpoint.\nConfiguring the ETL Lambda inside the VPC The ETL Lambda function must be placed inside the VPC so it can:\nReach S3 via the Gateway Endpoint. Connect to the PostgreSQL Data Warehouse in the Analytics private subnet. Step 1 – Attach the Lambda to the VPC Open the Lambda Console and choose the ETL function, for example:\nclickstream-etl-lambda Go to the Configuration tab → Network (or Environment → VPC depending on the UI).\nClick Edit and set:\nVPC: your project VPC. Subnets: select the ETL private subnet (10.0.3.0/24) (you can select multiple subnets in the same AZ or different AZs for resilience). Security groups: choose a security group that: Allows outbound traffic to S3 (via the Gateway Endpoint). Allows outbound traffic to the Data Warehouse EC2 instance (port 5432). Save the configuration. Lambda will now create ENIs (elastic network interfaces) in the selected subnet(s) the next time it runs.\nStep 2 – IAM role permissions for ETL Lambda The ETL Lambda execution role must include:\nPermission to read from the Raw Clickstream S3 bucket, e.g.:\n{ \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34;: [ \u0026#34;s3:GetObject\u0026#34;, \u0026#34;s3:ListBucket\u0026#34; ], \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::clickstream-raw-\u0026lt;account\u0026gt;-\u0026lt;region\u0026gt;\u0026#34;, \u0026#34;arn:aws:s3:::clickstream-raw-\u0026lt;account\u0026gt;-\u0026lt;region\u0026gt;/events/*\u0026#34; ] } Permission to write logs to CloudWatch Logs (usually added by default).\nNo direct permission to manage infrastructure (keep the role minimal).\nStep 3 – Database connectivity configuration In the ETL Lambda environment variables, store:\nDW_HOST – the private IP or hostname of the Data Warehouse EC2 instance. DW_PORT – typically 5432. DW_USER / DW_PASSWORD – credentials with sufficient privileges to insert into DW tables. DW_DATABASE – the name of the analytics database. The Lambda code will use these environment variables to establish a connection (for example, using a PostgreSQL client library).\nImplementing the ETL logic The ETL Lambda performs the following high-level steps each time it runs (either on a schedule via EventBridge or triggered manually):\nDetermine the time window / S3 prefix to process\nFor example, all objects under events/YYYY/MM/DD/HH/ for the last hour. List relevant S3 objects\nUse ListObjectsV2 on the Raw Clickstream bucket with the chosen prefix. Read and parse events\nFor each object, call GetObject and parse the JSON content. Validate important fields (event name, timestamp, user/session IDs, product IDs). Transform to analytical schema\nMap raw JSON to relational tables, for example:\ndim_users – user-level attributes. dim_products – product attributes. fact_events – individual events with foreign keys to users and products. fact_sessions – aggregated session-level metrics. Apply simple business rules, such as:\nDerive a date and hour dimension from timestamps. Normalize event names. Filter out test or malformed events. Insert into the Data Warehouse\nOpen a transaction against the PostgreSQL Data Warehouse. Use batched INSERT statements or COPY-like bulk inserts (depending on the implementation). Commit if everything is successful; roll back on errors. Mark progress (optional)\nStore a processing marker (e.g., last processed hour) in a control table or a metadata S3 prefix to avoid re-processing. Example of a simple target fact table for events:\nCREATE TABLE IF NOT EXISTS fact_events ( event_id UUID PRIMARY KEY, event_timestamp TIMESTAMPTZ NOT NULL, event_name TEXT NOT NULL, user_id TEXT, session_id TEXT, client_id TEXT, product_id TEXT, page_url TEXT, traffic_source TEXT, created_at TIMESTAMPTZ DEFAULT NOW() ); The ETL Lambda will insert data into fact_events based on the parsed S3 JSON files.\nFigure 5-10: VPC configuration of the ETL Lambda function\nThe screenshot shows the SBW_Lamda_ETL function attached to the SBW_Project-vpc.\nThe Lambda uses two private subnets (one in each Availability Zone) and a dedicated security group (sg_Lamda_ETL) that controls all network access.\nThis configuration allows the function to reach the S3 Gateway VPC Endpoint and the Data Warehouse instance without exposing the Lambda to the public internet.\nSecurity groups and connectivity To ensure that only the necessary traffic is allowed:\nSecurity Group for the Data Warehouse EC2 (SG-DW)\nInbound rules: Allow 5432/tcp from the ETL Lambda security group (not from 0.0.0.0/0). Outbound rules: Allow all outbound traffic (or restrict as needed for updates/monitoring). Security Group for the ETL Lambda (SG-ETL)\nInbound rules: Not needed (Lambda does not accept inbound connections). Outbound rules: Allow traffic to: The Data Warehouse EC2 private IP on port 5432. The S3 Gateway VPC Endpoint (this is usually covered by allowing all outbound traffic within the VPC). This setup ensures that:\nOnly the ETL Lambda can talk to the Data Warehouse (no direct access from the public Internet). The Data Warehouse EC2 does not accept arbitrary incoming connections. Testing the ETL pipeline from end to end To validate that the private analytics layer is working correctly:\nGenerate fresh events\nFollow Section 5.3 to generate new clickstream events (page views, product views, add-to-cart, checkout). Trigger the ETL Lambda\nOption A: Wait for the scheduled EventBridge rule (e.g., every 30 minutes). Option B: In the EventBridge console, select the ETL rule (for example clickstream-etl-schedule) and choose Run now. Option C: In the Lambda console, use the Test button with a minimal test event to manually invoke the ETL. Check CloudWatch Logs\nOpen the ETL Lambda log group in CloudWatch Logs. Verify that: The Lambda successfully listed and read S3 objects. It processed the expected number of events. It connected to the Data Warehouse and executed inserts without errors. Verify data in the Data Warehouse\nConnect to the Data Warehouse EC2 instance using Session Manager or SSH.\nFrom there, use psql or a SQL client to run queries such as:\nSELECT event_name, COUNT(*) AS total_events FROM fact_events GROUP BY event_name ORDER BY total_events DESC; SELECT product_id, COUNT(*) AS view_count FROM fact_events WHERE event_name = \u0026#39;product_view\u0026#39; GROUP BY product_id ORDER BY view_count DESC LIMIT 10; Confirm that the counts and top products roughly match the behaviour you generated on the website.\nSummary By the end of this section, you will have:\nConfigured an S3 Gateway VPC Endpoint so that private subnets can access S3 without a NAT Gateway. Attached the ETL Lambda to the VPC and ensured it can reach both S3 and the Data Warehouse over private networking. Implemented ETL logic that reads raw JSON clickstream files, transforms them, and loads them into a PostgreSQL Data Warehouse in a private subnet. Verified end-to-end connectivity and data flow by inspecting logs and running SQL queries. In the next section (5.5), you will use R Shiny dashboards running on the Data Warehouse EC2 instance to visualize the processed data and build interactive analytics views.\nFigure 5-11: Private analytics layer with ETL Lambda, Data Warehouse, and S3 Gateway Endpoint\nThe diagram shows how the batch ETL pipeline runs entirely inside the Analytics private subnet:\nEventBridge (bottom right) triggers the ETL Lambda on a schedule. The ETL Lambda reads raw clickstream files from the S3 Raw Clickstream bucket and reaches S3 through the S3 Gateway VPC Endpoint (no Internet/NAT required). The ETL Lambda writes transformed data into the PostgreSQL Data Warehouse EC2 instance, and the R Shiny Server running on the same instance reads from the Data Warehouse to power analytics dashboards. "
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.5-week5/",
	"title": "Week 5 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 5 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/5-workshop/5.5-visualizing-analytics-with-shiny-dashboards/",
	"title": "Visualizing Analytics with Shiny Dashboards",
	"tags": [],
	"description": "",
	"content": "In the previous sections, you ingested clickstream events into the S3 Raw Clickstream bucket and used a VPC-enabled ETL Lambda to load curated data into the PostgreSQL Data Warehouse in a private subnet.\nThis section focuses on the final mile of the pipeline:\nValidating that the ETL job has populated the Data Warehouse with meaningful data. Using R Shiny dashboards running on the same EC2 instance as the Data Warehouse to explore user behaviour, funnels, and product performance. The goal is not only to “see some charts”, but to connect each visualization back to the underlying data model and business questions.\nTriggering batch ETL and verifying data in the Data Warehouse Before looking at dashboards, make sure that the Data Warehouse contains fresh data.\nStep 1 – Generate new clickstream events Open the CloudFront domain of the e-commerce application, for example:\nhttps://dxxxxxxxx.cloudfront.net Sign in with a test user via Amazon Cognito.\nPerform a realistic browsing session, such as:\nVisit the home page and at least one category page. Search for a product or filter by brand/category. Open three or more product detail pages. Add one or two items to the cart. Remove an item, change quantity, and proceed to the checkout flow. Optionally, complete an order so that purchase events are generated. Wait 1–2 minutes to ensure all events have been sent to API Gateway → Lambda Ingest → S3 Raw Clickstream bucket.\nStep 2 – Run the ETL job You have three main options to run the ETL Lambda:\nA. Wait for the EventBridge schedule\nIf the ETL rule is configured to run every 15 or 30 minutes, you can simply wait for the next trigger. B. Manually trigger via EventBridge\nOpen Amazon EventBridge console.\nGo to Rules and select the ETL rule, for example:\nclickstream-etl-schedule Choose Actions → Run now to execute the rule immediately.\nC. Manually trigger via Lambda console\nOpen the Lambda Console and select the ETL function:\nclickstream-etl-lambda On the Test tab, create or reuse a test event (payload {} is sufficient if your code ignores the input).\nClick Test to invoke the ETL Lambda.\nStep 3 – Check ETL execution in CloudWatch Logs Open CloudWatch Logs in the AWS console.\nNavigate to the log group for the ETL Lambda function.\nOpen the most recent log stream and check for entries such as:\n“Listing S3 objects under prefix events/YYYY/MM/DD/HH/ …” “Read N files, parsed M events.” “Inserted K rows into fact_events, L rows into dim_products, etc.” Any error messages or stack traces (if present). If there are errors, review:\nIAM permissions for S3 and the database. VPC configuration (subnets, route tables, Gateway Endpoint). Database connectivity (host, port, credentials). Figure 5-12: CloudWatch logs for the latest ETL run\nThe screenshot shows the CloudWatch log stream for a recent execution of the ETL Lambda function.\nThe INIT_START, START, END, and REPORT entries confirm that the function executed successfully, and the reported duration and memory usage are within the expected range.\nChecking these logs helps ensure that the Data Warehouse contains fresh data before exploring the Shiny dashboards.\nStep 4 – Validate data with SQL queries Once the ETL reports success, verify that the Data Warehouse has been updated.\nConnect to the Data Warehouse EC2 instance using Session Manager or SSH (via a bastion host or VPN). From within the instance, connect to PostgreSQL DW using psql or a graphical client. Run basic checks like:\n-- 1. How many events are in the fact table? SELECT COUNT(*) AS total_events FROM fact_events; -- 2. Events by type (page views, product views, add-to-cart, etc.) SELECT event_name, COUNT(*) AS total_events FROM fact_events GROUP BY event_name ORDER BY total_events DESC; -- 3. Top 10 most viewed products SELECT product_id, product_name, COUNT(*) AS view_count FROM fact_events WHERE event_name = \u0026#39;product_view\u0026#39; GROUP BY product_id, product_name ORDER BY view_count DESC LIMIT 10; Optional deeper checks:\n-- 4. Funnel-like view: from product view to add-to-cart SELECT product_id, SUM(CASE WHEN event_name = \u0026#39;product_view\u0026#39; THEN 1 ELSE 0 END) AS product_views, SUM(CASE WHEN event_name = \u0026#39;add_to_cart\u0026#39; THEN 1 ELSE 0 END) AS add_to_cart_events FROM fact_events GROUP BY product_id ORDER BY product_views DESC LIMIT 10; Compare these numbers with your test session:\nDid you view at least as many products as the query shows? Do the products you interacted with appear near the top of the list? If you completed a purchase, can you see related events in the data? Accessing the Shiny dashboards (from a private EC2 instance) The R Shiny Server runs on the same private EC2 instance as the Data Warehouse, without a public IP. To access it securely, you typically use port forwarding.\nOption A – Port forwarding via SSH Ensure you have SSH access to the EC2 instance (either directly from a bastion host or using an SSH client that can reach the VPC).\nOn your local machine, run an SSH command to forward a local port (for example, 3838) to the Shiny Server on the EC2 instance:\nssh -i /path/to/your-key.pem \\ -L 3838:localhost:3838 \\ ec2-user@\u0026lt;bastion-or-dw-ec2-host\u0026gt; Keep this SSH session open.\nOpen a browser on your local machine and navigate to:\nhttp://localhost:3838/ or to the specific app, for example:\nhttp://localhost:3838/clickstream-analytics Option B – AWS Systems Manager Session Manager (port forwarding) If you prefer not to open SSH from the public Internet, you can use Session Manager with port forwarding:\nInstall and configure the Session Manager plugin for the AWS CLI.\nUse a command similar to:\naws ssm start-session \\ --target \u0026lt;instance-id\u0026gt; \\ --document-name AWS-StartPortForwardingSession \\ --parameters \u0026#39;{\u0026#34;portNumber\u0026#34;:[\u0026#34;3838\u0026#34;],\u0026#34;localPortNumber\u0026#34;:[\u0026#34;3838\u0026#34;]}\u0026#39; As with SSH, open your browser and go to:\nhttp://localhost:3838/ Consult the AWS documentation for exact steps if needed.\nExploring the dashboards Once you have access to the Shiny homepage or specific app, you should see one or more dashboards built on top of the Data Warehouse.\nTypical views might include:\nFunnel / User Journey Dashboard Shows how users flow through key steps such as:\npage_view → 2. product_view → 3. add_to_cart → 4. checkout_start → 5. purchase Common visualizations:\nA funnel chart with counts at each step. Drop-off percentages between one step and the next. Filters for date ranges, device type, or traffic source. Questions you can answer:\nHow many users start at a product view and reach the checkout step? Where do most users abandon the journey (e.g., before add-to-cart, or during checkout)? Does the funnel performance change over time or by traffic source? Product Performance Dashboard Focuses on product-level metrics:\nMost viewed products (product_view events). Products with the highest add-to-cart or purchase events. Conversion rate per product (add-to-cart / product views, purchases / product views). Possible visualizations:\nBar charts ranking products by views or purchases. Tables showing product name, category, and key engagement metrics. Filters by category, brand, or price range. Questions you can answer:\nWhich products attract the most attention but have low conversion? Which categories or brands perform best overall? Are there products that rarely get views and might need promotion? Time Series / Activity Over Time Shows how user activity changes over time:\nNumber of events per hour or per day. Separate lines for page views, product views, add-to-cart, purchases. Optional breakdown by device type or traffic source. Questions you can answer:\nWhat times of day see the highest browsing activity? Are there specific days of the week with better conversion? Do special campaigns or promotions (if recorded) align with spikes in activity? Relating dashboards back to the Data Warehouse schema Each dashboard is powered by SQL queries against the Data Warehouse tables, such as:\nfact_events – granular event-level data. dim_products – product attributes (name, category, brand, etc.). dim_users – user attributes (registered vs. guest, segment, etc.). fact_sessions or fact_funnels – pre-aggregated session or funnel metrics (if modeled). When you interact with filters in Shiny (e.g., selecting a date range, category, or event type), the Shiny app typically:\nBuilds a SQL query using the selected parameters. Sends the query to PostgreSQL. Receives the aggregated results. Renders them as charts or tables in the browser. As an exercise, you can:\nOpen the Shiny app source code (R scripts) on the EC2 instance. Locate the SQL queries used for each widget. Compare those queries with the manual SQL you ran earlier in this section. This helps build confidence that:\nThe dashboards are consistent with the underlying data. You can reproduce key numbers directly using SQL if needed. Summary By completing this section, you have:\nEnsured that the ETL batch job has successfully populated the PostgreSQL Data Warehouse with fresh clickstream data. Validated the data using direct SQL queries (event counts, product views, basic funnel metrics). Accessed R Shiny dashboards running on the private EC2 instance via secure port forwarding. Explored key dashboards for user journeys, product performance, and activity over time. Connected Shiny visualizations back to the underlying Data Warehouse schema and queries. In the next section (5.6 Summary \u0026amp; Clean up), you will briefly recap the main learnings from the workshop and review which AWS resources should be stopped or deleted to avoid unnecessary costs.\nFigure 5-13: Shiny dashboard for clickstream analytics\nThe screenshot shows a Shiny dashboard built on top of the PostgreSQL Data Warehouse:\nA user journey funnel from product views to purchases. A time-series chart of events per day (page views, product views, add-to-cart, purchases). A top products table listing the items with the highest engagement and conversion. Filters at the top (date range, event type, device, traffic source) to slice and dice the analysis. "
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/5-workshop/",
	"title": "Workshop",
	"tags": [],
	"description": "",
	"content": "Clickstream Analytics Platform for E-Commerce Overview This workshop walks through a Batch-based Clickstream Analytics Platform for an e-commerce website that sells computer products.\nYou will see how clickstream events:\nAre captured by a Next.js frontend hosted on AWS Amplify and delivered through Amazon CloudFront. Are ingested via Amazon API Gateway and a Lambda Ingest function into a Raw Clickstream bucket on Amazon S3. Are processed in batch by a VPC-enabled ETL Lambda that reads from S3 through an S3 Gateway VPC Endpoint and loads curated data into a PostgreSQL Data Warehouse on EC2 in a private subnet. Are visualized via R Shiny dashboards running on the same EC2 instance as the Data Warehouse. The workshop emphasises:\nSeparation of OLTP and Analytics workloads. A fully private analytics backend (ETL Lambda, Data Warehouse, Shiny) with no public IPs. Cost‑efficient private access to S3 using a Gateway VPC Endpoint instead of a NAT Gateway. Prerequisites Before starting the detailed sections (5.1–5.6), the reader is expected to have:\nBasic familiarity with AWS services such as EC2, S3, Lambda, API Gateway, VPC, and IAM. Working knowledge of SQL and PostgreSQL. A general understanding of web applications (HTTP, JSON, REST APIs). An AWS account with sufficient permissions to create VPC endpoints, Lambda functions, EC2 instances, S3 buckets, and EventBridge rules. This workshop assumes that the core infrastructure (VPC, subnets, EC2 instances, Lambda functions, API Gateway, and S3 buckets) is already provisioned, for example via Infrastructure‑as‑Code such as Terraform or CloudFormation.\nContent Map The workshop content is split into six sections:\nObjectives \u0026amp; Scope Architecture Walkthrough Implementing Clickstream Ingestion Building the Private Analytics Layer Visualizing Analytics with Shiny Dashboards Summary \u0026amp; Clean up "
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/6-self-evaluation/",
	"title": "Self-Evaluation",
	"tags": [],
	"description": "",
	"content": "During my internship at Amazon Web Services (AWS) from 08/09/2025 to 05/12/2025, I had the opportunity to learn, practice, and apply the knowledge I acquired at university in a real working environment.\nI participated in the Batch-based Clickstream Analytics Platform project for an e-commerce website selling computer products, with the main responsibilities as follows:\nResearching and working with AWS services such as Amazon S3, AWS Lambda, Amazon EventBridge, Amazon EC2, AWS Amplify, Amazon Cognito to support the collection and processing of clickstream data. Supporting the construction of a batch-based pipeline for collecting, storing, and processing clickstream data, in which user behaviour data on the website is captured, written to S3, and processed periodically by Lambda/EventBridge before being loaded into the analytics system. Taking part in configuring the infrastructure on AWS, connecting to PostgreSQL/Supabase and integrating with the frontend application (Next.js) of the e-commerce website. Writing technical documentation and progress reports, and communicating regularly with my mentor and team to update status and align on implementation directions. Through this, I have clearly improved my skills in programming, system design, working with cloud services, building batch data pipelines, technical writing, teamwork, and communication in a professional environment.\nIn terms of work attitude, I always tried to complete the tasks assigned, followed the team’s working processes, and proactively communicated when facing difficulties to ensure the quality of my work.\nTo objectively reflect my internship, I would like to provide the following self-evaluation based on several criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge and skills Understanding of the field, ability to apply knowledge in practice, tool usage, work quality ✅ ☐ ☐ 2 Ability to learn Ability to absorb new knowledge, learn quickly ☐ ✅ ☐ 3 Proactiveness Willingness to research and take on tasks without waiting for detailed instructions ☐ ✅ ☐ 4 Sense of responsibility Completing work on time and ensuring quality ✅ ☐ ☐ 5 Discipline Compliance with working hours, rules, and processes ☐ ✅ ☐ 6 Desire for self-improvement Willingness to receive feedback and improve oneself ☐ ✅ ☐ 7 Communication Presenting ideas and reporting work clearly ☐ ✅ ☐ 8 Teamwork Working effectively with colleagues and participating in team activities ✅ ☐ ☐ 9 Professional conduct Showing respect toward colleagues, partners, and the workplace ✅ ☐ ☐ 10 Problem-solving mindset Identifying problems, proposing solutions, and being creative ☐ ✅ ☐ 11 Contribution to project/organization Work effectiveness, improvement initiatives, and recognition from the team ☐ ✅ ☐ 12 Overall performance Overall evaluation of the entire internship period ☐ ✅ ☐ Areas for Improvement Strengthen my discipline and time management skills, strictly following rules and deadlines in any working environment. Further develop my problem-solving mindset, especially in analyzing root causes and comparing multiple options before choosing a solution. Improve my communication skills: present more concisely and clearly in meetings, and proactively share difficulties/questions with my mentor and team members. Overall, I consider this internship at AWS a very meaningful experience that helped me grow both in technical capability and work attitude, and also made me more aware of the areas I need to continue improving in the future.\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.6-week6/",
	"title": "Week 6 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 6 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/5-workshop/5.6-summary--clean-up/",
	"title": "Summary &amp; Clean up",
	"tags": [],
	"description": "",
	"content": "This final section:\nRecaps the key concepts and components you worked with throughout the workshop. Highlights how they fit together into a cohesive Batch-based Clickstream Analytics Platform. Provides a clean-up checklist so you can safely remove or stop AWS resources and avoid unnecessary costs once you finish experimenting. Summary of key learnings Across Sections 5.1–5.5, you have built and validated a complete end-to-end analytics pipeline for an e-commerce website selling computer products.\nArchitecture and design From 5.1 Objectives \u0026amp; Scope and 5.2 Architecture Walkthrough, you learned to:\nDescribe the core building blocks of the platform:\nFrontend \u0026amp; user-facing domain:\nNext.js application on AWS Amplify Hosting. Delivered via Amazon CloudFront. User authentication through Amazon Cognito. Ingestion \u0026amp; data lake domain:\nAmazon API Gateway (HTTP API) as the clickstream ingestion endpoint. Lambda Ingest function validating and enriching events. S3 Raw Clickstream bucket storing time-partitioned JSON logs. Analytics \u0026amp; Data Warehouse domain:\nVPC with a public OLTP subnet, and private Analytics + ETL subnets. PostgreSQL Data Warehouse on EC2 in the Analytics private subnet. R Shiny Server co-located on the same EC2 instance for analytics dashboards. VPC-enabled ETL Lambda in the ETL private subnet. Explain why the platform separates:\nOLTP (Operational) workloads (live orders, inventory, users) and Analytics (Read-heavy) workloads (funnel analysis, product performance, time series). This separation improves performance, resilience, and security.\nIngestion path: Frontend → API Gateway → S3 From 5.3 Implementing Clickstream Ingestion, you:\nImplemented and/or understood a tracking flow where:\nThe browser captures page views, product views, add-to-cart, and checkout events. Events are serialized into JSON payloads containing user, session, and product metadata. The frontend calls POST /clickstream on API Gateway. Saw how Lambda Ingest:\nParses incoming events.\nEnforces minimal validation and adds server-side metadata.\nWrites batched JSON objects into the S3 Raw Clickstream bucket using a partitioned prefix such as:\nevents/YYYY/MM/DD/HH/events-\u0026lt;uuid\u0026gt;.json Practiced manual testing using both:\nThe real frontend (via the browser developer tools), and API clients such as Postman or Thunder Client. Private analytics layer: Gateway Endpoint, ETL Lambda, Data Warehouse From 5.4 Building the Private Analytics Layer, you:\nConfigured a Gateway VPC Endpoint for S3, ensuring that:\nPrivate subnets do not require public IPs or a NAT Gateway to reach S3. Traffic between ETL Lambda and S3 stays on the AWS private network. Attached the ETL Lambda to the VPC by:\nSelecting the ETL private subnet and appropriate security groups. Giving the Lambda execution role minimal IAM permissions to read from the Raw bucket and write to CloudWatch Logs. Implemented ETL logic that:\nLists relevant S3 objects for a given time window. Parses raw JSON clickstream events. Transforms them into relational structures (e.g., fact_events, dim_products). Loads them into a PostgreSQL Data Warehouse on an EC2 instance in the Analytics private subnet. Verified connectivity and security through:\nSecurity groups allowing only the ETL Lambda to connect to the DW on port 5432. Route tables with local + prefix-list routes, and no 0.0.0.0/0 in private subnets. Analytics and visualization with Shiny From 5.5 Visualizing Analytics with Shiny Dashboards, you:\nTriggered the ETL job (via EventBridge or manually) and validated data freshness using SQL queries:\nEvent counts. Distribution of event types. Top viewed products. Simple funnel-like metrics. Accessed R Shiny dashboards running on the same private EC2 instance as the Data Warehouse using secure port forwarding:\nSSH tunnels, or AWS Systems Manager Session Manager. Explored dashboards that represent:\nFunnels and user journeys (page view → product view → add-to-cart → checkout → purchase). Product performance (top products by views and purchases). Time-series trends (events per hour/day). Connected Shiny visualizations back to underlying SQL queries and DW tables, confirming that:\nThe dashboards are consistent with the data. Key business metrics can be verified manually when needed. Clean-up checklist (AWS resources) After completing the workshop, it is recommended to clean up resources to avoid charges. The exact actions depend on whether this environment is:\nA short-lived lab or demo environment, or A persistent development/staging environment. Important: Before deleting anything, confirm that no one else is using the same AWS account or resources for ongoing work.\nEC2 instances OLTP EC2 instance (Public Subnet)\nIf you no longer need the operational database: Stop the instance to pause compute charges, or Terminate it to permanently delete it. Before termination, consider: Taking a final backup or snapshot of the volume. Exporting a logical dump (e.g., pg_dump) of important data. Data Warehouse + Shiny EC2 instance (Private Subnet)\nIf you only needed this instance for the workshop: Stop or terminate it after confirming that you have exported any necessary analytical results or schema definitions. If you plan to extend the analytics layer later: You may keep the instance, but ensure that: Unused services (e.g., experimental Shiny apps) are removed. Security groups remain restrictive (no broad inbound rules). Lambda functions and EventBridge rules Lambda Ingest function\nIf you will not send more clickstream events: Consider deleting the function to avoid confusion in the console. Otherwise, you can keep it for future experimentation. ETL Lambda function\nIf the Data Warehouse EC2 instance is stopped or terminated: Disable or delete the ETL Lambda (and its EventBridge rule) to avoid failing invocations. EventBridge ETL schedule\nNavigate to Amazon EventBridge → Rules. Disable or delete the scheduling rule (for example clickstream-etl-schedule) so it no longer triggers ETL runs. S3 buckets and data Raw Clickstream S3 bucket\nDecide whether the raw clickstream data should be retained:\nFor short-lived labs, you can safely delete the events/YYYY/MM/DD/HH/ prefixes created during testing. For ongoing projects, keep the bucket but: Enable S3 lifecycle policies if needed (e.g., transition older data to cheaper storage classes, or delete after N days). Other project-specific buckets\nIf you created additional buckets specifically for this workshop (for logs, artifacts, or screenshots), review and delete them if they are no longer required. Note: S3 storage costs can accumulate over time; periodically review bucket contents and lifecycle rules.\nVPC Endpoints and networking components S3 Gateway VPC Endpoint\nIf the VPC will no longer be used for analytics: You may delete the Gateway Endpoint. If the VPC remains active and other workloads may benefit from private S3 access: It can be left in place. Route tables, subnets, and VPC\nFor a dedicated lab VPC: Consider deleting the entire VPC (which will also remove associated subnets, route tables, and endpoints) only after ensuring no active EC2, RDS, or other critical resources depend on it. For a shared VPC: Clean up only the subnets and route tables that were created exclusively for this workshop, taking care not to disrupt other environments. CloudWatch Logs and monitoring CloudWatch Logs for Lambda and API Gateway\nLog groups can grow in size over time. Set an appropriate retention policy (for example, 7, 30, or 90 days) for: Lambda Ingest logs. ETL Lambda logs. API Gateway access logs. CloudWatch Alarms and metrics\nIf you created alarms specific to this environment (e.g., error count alarms for ETL or ingestion): Disable or delete them if the environment is being decommissioned. Figure 5-14: CloudWatch alarms for the clickstream platform\nThe screenshot shows a set of CloudWatch alarms for the ingestion path and the Data Warehouse, including:\nAPI latency and 4xx/5xx rate alarms. Lambda ingest duration and throttling alarms. EC2 Data Warehouse status check and CPU utilization alarms. Most alarms are in OK or Insufficient data state, indicating that the system is currently healthy and ready to be stopped or scaled down after the workshop.\nIAM roles and policies Lambda execution roles\nReview IAM roles created for:\nLambda Ingest. ETL Lambda. If they are no longer needed:\nDetach any inline or attached policies. Delete the roles to avoid accumulating unused IAM identities. Test users and permissions\nIf you created dedicated IAM users or test roles for the workshop, consider removing them or tightening their permissions. Amplify, Cognito, and API Gateway Amplify app\nIf the Next.js frontend was deployed only for this workshop and will not be used again: Remove the Amplify app and associated branches/environments. Cognito User Pool\nRemove test users or delete the entire User Pool if it was dedicated to this environment. API Gateway HTTP API\nDelete the clickstream ingestion API if you no longer need it, especially if it was created only for the lab. What to keep for future work If you plan to extend this project beyond the workshop, consider keeping:\nThe code repositories:\nNext.js frontend (including tracking logic). Lambda functions (Ingest and ETL). Shiny app code. Infrastructure-as-Code (Terraform or CloudFormation templates). The Data Warehouse schema and a small sample dataset:\nA minimal EC2 instance and database with a sample of events can be enough to continue developing dashboards and queries without ingesting large volumes of new data. Architecture diagrams and notes:\nDiagrams used as Figures 5-1 to 5-6. Notes on design decisions (e.g., using Gateway Endpoint instead of NAT Gateway). These assets make it easier to resume the project later or adapt it into a more advanced solution (for example, migrating to Amazon Redshift Serverless or adding real-time streaming).\nClosing remarks This workshop demonstrated how to:\nDesign and implement a secure, cost-conscious, and extensible analytics architecture on AWS. Collect clickstream events from a real e-commerce frontend. Build a private ETL process that reads from S3 and loads into a dedicated Data Warehouse. Visualize user behaviour and product performance through R Shiny dashboards, without exposing the analytics backend to the public Internet. You now have a working reference for building similar analytics platforms and a strong foundation for future enhancements such as:\nReal-time streams (Kinesis / Kafka). More advanced attribution and user segmentation models. Migrating the Data Warehouse to managed services like Amazon Redshift. Figure 5-15: Summary of resources to check after the workshop\nThe diagram or table for this figure should list, for each AWS service:\nThe resource type (EC2, S3, Lambda, EventBridge, API Gateway, VPC Endpoint, etc.). Example resource names. Recommended action after the workshop (Keep / Stop / Delete / Set retention). "
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/7-feedback/",
	"title": "Sharing and Feedback",
	"tags": [],
	"description": "",
	"content": "Overall Evaluation 1. Working Environment\nThe working environment is very friendly and open. FCJ members are always willing to help whenever I encounter difficulties, even outside working hours. The workspace is tidy and comfortable, helping me focus better. However, I think it would be nice to have more social gatherings or team bonding activities to strengthen relationships.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don’t understand, and always encourages me to ask questions. The admin team supports administrative tasks, provides necessary documents, and creates favorable conditions for me to work effectively. I especially appreciate that the mentor allows me to try and solve problems myself instead of just giving the answer.\n3. Relevance of Work to Academic Major\nThe tasks I was assigned align well with the knowledge I learned at university, while also introducing me to new areas I had never encountered before. This allowed me to both strengthen my foundational knowledge and gain practical skills.\n4. Learning \u0026amp; Skill Development Opportunities\nDuring the internship, I learned many new skills such as using project management tools, teamwork skills, and professional communication in a corporate environment. The mentor also shared valuable real-world experiences that helped me better plan my career path.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but still keeps things enjoyable. When there are urgent projects, everyone works together and supports one another regardless of their position. This made me feel like a real part of the team, even as an intern.\n6. Internship Policies / Benefits\nThe company provides an internship allowance and offers flexible working hours when needed. In addition, having the opportunity to join internal training sessions is a big plus.\nAdditional Questions What did you find most satisfying during your internship? What do you think the company should improve for future interns? If recommending to a friend, would you suggest they intern here? Why or why not? Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience? Would you like to continue this program in the future? Any other comments (free sharing): "
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.7-week7/",
	"title": "Week 7 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 7 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.8-week8/",
	"title": "Week 8 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 8 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.9-week9/",
	"title": "Week 9 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 9 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.10-week10/",
	"title": "Week 10 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 10 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.11-week11/",
	"title": "Week 11 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 11 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/1-worklog/1.12-week12/",
	"title": "Week 12 Worklog",
	"tags": [],
	"description": "",
	"content": "\r⚠️ Note: The following information is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nWeek 12 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand basic AWS services, how to use the console \u0026amp; CLI. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations 08/11/2025 08/11/2025 3 - Learn about AWS and its types of services + Compute + Storage + Networking + Database + \u0026hellip; 08/12/2025 08/12/2025 https://cloudjourney.awsstudygroup.com/ 4 - Create AWS Free Tier account - Learn about AWS Console \u0026amp; AWS CLI - Practice: + Create AWS account + Install \u0026amp; configure AWS CLI + How to use AWS CLI 08/13/2025 08/13/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn basic EC2: + Instance types + AMI + EBS + \u0026hellip; - SSH connection methods to EC2 - Learn about Elastic IP 08/14/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ 6 - Practice: + Launch an EC2 instance + Connect via SSH + Attach an EBS volume 08/15/2025 08/15/2025 https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Understood what AWS is and mastered the basic service groups:\nCompute Storage Networking Database \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nInstalled and configured AWS CLI on the computer, including:\nAccess Key Secret Key Default Region \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "http://localhost:1313/tranhoangtin-se185022-AWSFCJ/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]